[
  {
    "question": "What is the logistic function and how did it originate in the 19th century?",
    "ground_truth": "The logistic function has the form P(t) = e^(α+βt) / (1 + e^(α+βt)), where α is a constant from integration and values are between 0 and 1. It originated from Pierre-François Verhulst's work (1804-1849) on modeling population growth. Verhulst and his teacher Adolphe Quetelet wanted a more realistic model than exponential growth, so they chose dP(t)/dt = βP(t)(Ω - P(t)), where Ω is the saturation limit. Writing P(t) = W(t)/Ω as the proportion of saturation limit gives dP(t)/dt = βP(t)(1 - P(t)), which leads to the logistic function. Verhulst published papers between 1838 and 1847 demonstrating the curve fit population growth in France, Belgium, Essex, and Russia."
  },
  {
    "question": "In logistic regression, why do we model the log odds ratio as a linear combination of features?",
    "ground_truth": "We want to make a linear combination of feature values, like in regular regression, which can take any real values. However, we need to turn it into a model for P(Y = 1|x, λ), which takes values only between 0 and 1. The odds ratio P(Y=1|x,λ)/P(Y=0|x,λ) turns those probabilities into positive real numbers, and then the log of the odds ratio turns it into any real numbers. This gives us ln(P(Y=1|x,λ)/P(Y=0|x,λ)) = λ^T x. Solving for P(Y=1|x,λ), we get the logistic function: P(Y=1|x,λ) = e^(λ^T x)/(1 + e^(λ^T x))."
  },
  {
    "question": "How does maximum likelihood estimation work for logistic regression?",
    "ground_truth": "Maximum likelihood estimation chooses parameters λ to maximize the likelihood of the data given the model. The likelihood is L(λ) = P(Y₁=y₁,...,Yₘ=yₘ|λ,x₁,...,xₘ) = ∏ᵢ₌₁ᵐ P(Yᵢ=yᵢ|λ,xᵢ), where examples are iid. We choose λ* ∈ argmax L(λ) = argmax log L(λ), taking the log for convenience. For both yᵢ=1 and yᵢ=-1, we can write P(Y=yᵢ|λ,xᵢ) = 1/(1 + e^(-yᵢλ^T xᵢ)). This leads to λ* ∈ argmin ∑ᵢ₌₁ᵐ log(1 + e^(-yᵢλ^T xᵢ)), which is convex in λ and can be minimized by gradient descent."
  },
  {
    "question": "What are the key advantages of decision trees mentioned in the course notes?",
    "ground_truth": "Decision trees have several key advantages: (1) They are interpretable and intuitive, which is why they are popular in medical applications as they mimic the way a doctor thinks. (2) They model discrete outcomes nicely. (3) They can be very powerful and as complex as needed. (4) C4.5 and CART are among the top 10 data mining algorithms, showing their popularity and effectiveness."
  },
  {
    "question": "Explain how Information Gain is calculated for decision tree splitting and what the downward closure property means.",
    "ground_truth": "Information Gain measures the expected reduction in entropy due to branching on attribute A. It is calculated as: Gain(S,A) = H([#pos/(#pos+#neg), #neg/(#pos+#neg)]) - ∑ⱼ ((#posⱼ+#negⱼ)/(#pos+#neg)) * H([#posⱼ/(#posⱼ+#negⱼ), #negⱼ/(#posⱼ+#negⱼ)]), where H(p) is the entropy function. The first term is the original entropy before branching, and the second term is the weighted average entropy after branching. For decision trees to choose the feature A that maximizes Gain(S,A), we want high information gain. One problem is that Gain favors numerous splits. The downward closure property, referenced in the context of the Apriori algorithm, states that if an itemset is frequent, then all its subsets must also be frequent: if Supp(a ∪ b) ≥ θ then Supp(a) ≥ θ and Supp(b) ≥ θ."
  },
  {
    "question": "What is the Gain Ratio and why is it used instead of Information Gain alone?",
    "ground_truth": "The Gain Ratio is an alternative to Information Gain that addresses the problem that Gain favors too many partitions and numerous splits. For example, if each branch contains only 1 example, the entropy terms become zero, making that attribute appear optimal. The Gain Ratio is calculated as Gain(S,A)/SplitInfo(S,A), where we want large Gain and small SplitInfo. SplitInfo(S,A) = -∑ⱼ (|Sⱼ|/|S|) log(|Sⱼ|/|S|), where |Sⱼ| is the number of examples in branch j. We want each term in the sum to be large, meaning we want lots of examples in each branch, which penalizes splits that create many small branches."
  },
  {
    "question": "How does C4.5 perform pruning and what are the three options considered at each node?",
    "ground_truth": "C4.5 uses a pruning method that recursively makes choices about whether to prune on an attribute by considering three options: (1) Leave the tree as is, (2) Replace that part of the tree with a leaf corresponding to the most frequent label in the data S going to that part of the tree, or (3) Replace that part of the tree with one of its subtrees, corresponding to the most common branch in the split. To decide, C4.5 computes upper bounds on the probability of error for each option and chooses the option with the lowest upper bound. This ensures that with high probability the error rate is fairly low."
  },
  {
    "question": "Compare the splitting criteria used by C4.5 and CART decision tree algorithms.",
    "ground_truth": "C4.5 uses Information Gain for splitting, which is based on entropy: H(p) = -∑ⱼ pⱼ log₂(pⱼ). Information Gain measures the expected reduction in entropy. CART (Classification and Regression Trees) uses the Gini index for splitting, which is 2p(1-p) for binary classification. The Gini index represents the variance of a Bernoulli distribution. Additionally, CART only performs binary splits, while C4.5 can perform multiway splits. Both are measures of node impurity, but they quantify it differently."
  },
  {
    "question": "What is the key assumption made by Naïve Bayes and why does it help with high-dimensional problems?",
    "ground_truth": "Naïve Bayes assumes that features x⁽ʲ⁾ are conditionally independent given the class label y. This means P(X⁽¹⁾=x⁽¹⁾,...,X⁽ⁿ⁾=x⁽ⁿ⁾|Y=y) = ∏ⱼ₌₁ⁿ P(X⁽ʲ⁾=x⁽ʲ⁾|Y=y). This strong assumption makes Naïve Bayes great for very high dimensional problems because such problems suffer from the curse of dimensionality—it's difficult to understand what's going on in a high dimensional space without tons of data. The independence assumption greatly reduces the number of parameters that need to be estimated, making the algorithm tractable even with limited data in high dimensions. For example, in a spam filter with a 50,000-dimensional vocabulary, the assumption allows the algorithm to work effectively."
  },
  {
    "question": "Explain why Laplace smoothing is used in Naïve Bayes and how it works.",
    "ground_truth": "Laplace smoothing is used to avoid the problem where most conditional probabilities are 0 in high dimensional data. If even one P̂(X⁽ʲ⁾=xₜₑₛₜ⁽ʲ⁾|Y=ỹ) is zero, then the entire product becomes zero. For example, if no training examples from class 'spam' have the word 'tomato,' we'd never classify a test example containing 'tomato' as spam. To avoid this, we add hallucinated examples using a Bayesian shrinkage estimate. There are K hallucinated examples spread evenly over the possible values of X⁽ʲ⁾, where K is the number of distinct values. The probabilities are pulled toward 1/K. The formula becomes: P̂(X⁽ʲ⁾=xₜₑₛₜ⁽ʲ⁾|Y=ỹ) = (∑ᵢ 1[xᵢ⁽ʲ⁾=xₜₑₛₜ⁽ʲ⁾,yᵢ=ỹ] + 1) / (∑ᵢ 1[yᵢ=ỹ] + K), which sets probabilities to a small positive value when there are no data."
  },
  {
    "question": "What is the difference between generative and discriminative models in classification?",
    "ground_truth": "In classification, we want to predict the label y given x, that is, we want P(Y=y|X=x). A generative model, like Naïve Bayes, estimates P(X=x|Y=y) and P(Y=y), then uses Bayes rule to get P(Y=y|X=x). A discriminative model directly estimates P(Y=y|X=x) without modeling the data generation process. Most of the top 10 classification algorithms are discriminative, including K-NN, CART, C4.5, SVM, and AdaBoost. The generative approach models how the data is generated given the class, while the discriminative approach focuses directly on the decision boundary between classes."
  },
  {
    "question": "How does K-Nearest Neighbors classification work and what are its main advantages and disadvantages?",
    "ground_truth": "K-NN classifies using the majority vote of the k closest training points to the test point. For regression, the value becomes the weighted average of the values of the K neighbors. Main advantages: (1) Simple and powerful with no need for tuning complex parameters, (2) No explicit training involved (it's 'lazy'), (3) New training examples can be added easily. Main disadvantages: (1) Expensive and slow with O(md) complexity where m is the number of examples and d is the number of dimensions—to determine the nearest neighbor of a new point, must compute the distance to all m training examples, (2) The distance measure has to be meaningful and attributes should be scaled properly. Runtime can be improved by pre-sorting training examples into fast data structures, computing only approximate distances, or removing redundant data through condensing."
  },
  {
    "question": "Describe the K-Means clustering algorithm and explain why the cost monotonically decreases during iterations.",
    "ground_truth": "K-Means is a clustering algorithm that minimizes cost(z₁,...,zₖ) = ∑ᵢ minₖ ||xᵢ - zₖ||₂². The algorithm: (1) Initialize centers z₁,...,zₖ and clusters C₁,...,Cₖ, (2) Repeat until no change in cost: for each k, assign Cₖ = {xᵢ: the closest representative is zₖ}, then for each k, set zₖ = mean(Cₖ). The cost monotonically decreases because: In the first step, each data point is assigned to its closest center, so cost(C₁⁽ᵗ⁺¹⁾,...,Cₖ⁽ᵗ⁺¹⁾,z₁⁽ᵗ⁾,...,zₖ⁽ᵗ⁾) ≤ cost(C₁⁽ᵗ⁾,...,Cₖ⁽ᵗ⁾,z₁⁽ᵗ⁾,...,zₖ⁽ᵗ⁾). In the second step, each cluster is re-centered at its mean, which minimizes the cost for that cluster by Lemma 1, so cost(C₁⁽ᵗ⁺¹⁾,...,Cₖ⁽ᵗ⁺¹⁾,z₁⁽ᵗ⁺¹⁾,...,zₖ⁽ᵗ⁺¹⁾) ≤ cost(C₁⁽ᵗ⁺¹⁾,...,Cₖ⁽ᵗ⁺¹⁾,z₁⁽ᵗ⁾,...,zₖ⁽ᵗ⁾). Therefore, the cost decreases at each iteration."
  },
  {
    "question": "What is the bias-variance tradeoff and how does it relate to model complexity?",
    "ground_truth": "The bias-variance decomposition shows that for each fixed x, the expected squared error can be written as: E_{y,S}[(y - f_S(x))²] = var_{y|x}(y) + var_S(f_S(x)) + bias(f_S(x))². The first term is the inherent variance of y that we cannot control. The second term is the variance of our estimator around its mean—it controls how our predictions vary around the average prediction. The third term is the squared bias, the difference between the average prediction and the true conditional mean. The tradeoff means we need to balance reducing variance (second term) and bias (third term) to minimize mean squared error. We can't just minimize one or the other. If f_S fits the data perfectly every time (overfitting), variance will be high. If f_S is a flat line every time, bias will be high. Sometimes injecting some bias can substantially reduce variance, such as by modeling with lower degree polynomials rather than higher degree ones. Simple models have low variance but potentially high bias, while complex models have low bias but potentially high variance."
  },
  {
    "question": "Compare the loss functions used in different machine learning algorithms as shown in the regularized learning expression.",
    "ground_truth": "The regularized learning expression ∑ᵢ R(f(xᵢ),yᵢ) + C·R_reg(f) captures many algorithms with different loss functions R(f(xᵢ),yᵢ): (1) Least squares loss: (f(xᵢ) - yᵢ)² (used in regression), (2) Misclassification error: 1[yᵢ ≠ sign(f(xᵢ))] = 1[yᵢf(xᵢ)≤0] (computationally hard to minimize), (3) Logistic loss: log₂(1 + e^(-yᵢf(xᵢ))) (used in logistic regression), (4) Hinge loss: max(0, 1 - yᵢf(xᵢ)) (used in SVM), (5) Exponential loss: e^(-yᵢf(xᵢ)) (used in AdaBoost). For regularization R_reg(f), when f is linear f(x) = ∑ⱼ λⱼx⁽ʲ⁾, common choices are: ||λ||₂² = ∑ⱼ λⱼ² (ridge regression, SVM) or ||λ||₁ = ∑ⱼ |λⱼ| (LASSO, approximately AdaBoost)."
  },
  {
    "question": "What is the Apriori algorithm and how does it use the downward closure property to efficiently find frequent itemsets?",
    "ground_truth": "The Apriori algorithm finds all frequent itemsets (itemsets a such that Supp(a) ≥ θ, where θ is the minimum support threshold). It uses a breadth-first search approach, generating all k-itemsets from (k-1)-itemsets. The key to its efficiency is the downward closure property: if Supp(a ∪ b) ≥ θ, then Supp(a) ≥ θ and Supp(b) ≥ θ. This means if an itemset is frequent, all its subsets must be frequent. The algorithm starts with L₁ = {frequent 1-itemsets}, then for k=2 and while L_{k-1} ≠ ∅: (1) Generate candidate k-itemsets Cₖ using apriori_gen(L_{k-1}) which joins pairs in L_{k-1} with identical first k-2 items and prunes candidates whose (k-1)-subsets are not in L_{k-1}, (2) Scan the database to find Lₖ = {c: c ∈ Cₖ, Supp(c) ≥ θ}. The output is the union of all Lₖ. This avoids checking all possible itemsets by using the monotonicity property."
  },
  {
    "question": "Explain how CART performs regression tree splitting and what value is assigned to each leaf.",
    "ground_truth": "For CART regression trees, in each leaf we assign f(x) to be a constant fⱼ. To find the optimal value for fⱼ, we minimize the training error R_train(f) = ∑_{leaves j} ∑_{i∈leaf j} (yᵢ - fⱼ)². Taking the derivative with respect to fⱼ and setting to 0 gives fⱼ = (1/|Sⱼ|)∑_{i∈leaf j} yᵢ = ȳ_{Sⱼ}, which is the sample average of the labels for leaf j's examples. For splitting, CART greedily chooses attribute A and split point s that solve: min_{A,s} [min_{C₁} ∑_{i∈{leaf|x^(A)≤s}} (yᵢ - C₁)² + min_{C₂} ∑_{i∈{leaf|x^(A)>s}} (yᵢ - C₂)²]. Since the optimal C₁ = ȳ_{leaf|x^(A)≤s} and C₂ = ȳ_{leaf|x^(A)>s}, we do a line search over s for each attribute A. For pruning, CART uses minimal cost complexity: cost = ∑_{leaves j} ∑_{i∈Sⱼ} (yᵢ - ȳ_{Sⱼ})² + C[# leaves in tree]."
  },
  {
    "question": "What is overfitting and how does learning theory address it through the concepts of training error and test error?",
    "ground_truth": "Overfitting occurs when a model fits the training data perfectly but doesn't generalize well to new data. The model 'memorized' the examples and modeled the noise rather than the underlying pattern. We measure model quality using a loss function. Training error R_train(f) = (1/m)∑ᵢ R(f(xᵢ),yᵢ) measures average performance on training data, while test error R_test(f) = E_{(x,y)~D} R(f(x),y) measures expected performance on new data from the same distribution. If R_train(f) is small but R_test(f) is large, the model has overfitted. Learning theory, particularly Structural Risk Minimization (SRM), addresses generalization by quantifying the class of 'simple models' and providing probabilistic guarantees on true risk. The key insight is that R_train is close to R_test when: (1) m is large (lots of data), and (2) f is 'simple' (low complexity). Computational learning theory provides bounds on how close training and test error are based on model complexity and sample size."
  },
  {
    "question": "How does information theory define 'information' and what is entropy?",
    "ground_truth": "Information theory defines 'information' from observing an event as the number of bits needed to encode the probability of the event: I(p) = -log₂(p). For example, a fair coin flip contains 1 bit of information. If the event has probability 1, we get no information from its occurrence. This definition satisfies three properties: (1) I(p) ≥ 0 and I(1) = 0 (non-negative information, no info from certain events), (2) I(p₁·p₂) = I(p₁) + I(p₂) (information from independent events sums), (3) I(p) is continuous (slight probability changes mean slight information changes). These properties lead to I(p) = -log_b(p) for some base b, with b=2 for 'bits'. Entropy H(p) is the mean information when we have many events v₁,...,vⱼ with probabilities p₁,...,pⱼ: H(p) = E_p I(p) = -∑ⱼ pⱼ log₂(pⱼ). For binary events with probabilities [p, 1-p], H(p) = -p log₂(p) - (1-p) log₂(1-p). As one probability approaches 1, H(p) → 0."
  },
  {
    "question": "Describe the fundamental difference between supervised and unsupervised learning, providing examples from the course materials.",
    "ground_truth": "Supervised learning methods have ground truth labels in the training data and learn to predict those labels. Examples include classification (predicting discrete labels like spam/not spam), regression (predicting continuous values like house prices), ranking (ordering items), and density estimation (predicting probabilities). Classification algorithms from the materials include logistic regression, decision trees (C4.5, CART), Naïve Bayes, K-NN, SVM, and AdaBoost. Regression examples include linear regression and CART regression trees. Unsupervised learning methods have no ground truth labels and find patterns or structure in data without supervision. Examples include: (1) Clustering—grouping data into clusters where objects within a cluster are more similar to each other than to objects in other clusters, such as K-Means and hierarchical clustering, and (2) Rule mining/association rule discovery—finding patterns and correlations in large datasets, such as the Apriori algorithm for finding frequent itemsets and association rules like (Diapers → Beer)."
  },
  {
    "question": "What does the conditional expectation f*(x) = E[y|x] minimize and why is this important in regression?",
    "ground_truth": "The conditional expectation f*(x) = E_y[y|x] minimizes the least squares error E_{x,y~D}[(y - f(x))²]. This is proven by considering each x separately and looking at E_y[(y - f(x))²|x] for each x. For fixed x, define ȳ = E_y[y|x]. Then E_y[(y - f(x))²|x] = E_y[(y - ȳ)²|x] + (ȳ - f(x))² + 2(ȳ - f(x))E_y[(y - ȳ)|x]. The last term is zero by definition of ȳ. The first term doesn't depend on f(x), so to minimize the expression, we set f(x) = ȳ = E_y[y|x]. This is important because it tells us the best possible predictor if we knew the true distribution D—it's the target we're trying to approximate. For absolute loss E_{x,y}[|y - f(x)|], the best predictor is the conditional median f(x) = median[y|x]. In practice, we don't know D, so we estimate f from training data, which leads to the bias-variance tradeoff."
  },
  {
    "question": "Compare how C4.5 and CART handle pruning decisions differently.",
    "ground_truth": "C4.5 pruning recursively considers three options at each node: (1) leave the tree as is, (2) replace with a leaf using the most frequent label, or (3) replace with one of its subtrees using the most common branch. C4.5 computes upper bounds on the probability of error for each option using confidence intervals on proportions with α=0.25 by default. It calculates p_α such that P(M~Bin(N,p_α))(M or fewer errors) ≈ α, where M is misclassified examples and N is total examples. The average upper bound is computed across branches weighted by their frequencies. C4.5 chooses the option with the lowest upper bound. CART uses 'minimal cost complexity' pruning with a cost function: cost(subtree) = ∑_{leaves j} ∑_{x_i∈leaf j} 1[y_i≠leaf's class] + C[# leaves in subtree]. By varying C, CART creates a sequence of nested subtrees. To choose C, typically: (1) for each C, use holdout data to split and prune, (2) select C that performs best on holdout, (3) use all data with chosen C to produce final tree. Cross-validation can also be used."
  },
  {
    "question": "How would you implement and evaluate a machine learning model using the R programming language based on the course materials?",
    "ground_truth": "Based on the R tutorial, implementing a machine learning model involves several steps: (1) Data preparation: Use read.table() or read.csv() to load data from files, with parameters for header and separator. Data can be matrices or data frames (use as.data.frame() to convert). (2) Split data: Use sample() to randomly split into training and test sets. (3) Install and load required packages: Use install.packages() once, then library() each session. For example, e1071 for SVM and Naïve Bayes, class for k-NN, rpart for CART, ada for AdaBoost. (4) Train models: Each algorithm has specific syntax, e.g., svm(x=X, y=as.factor(labels), kernel='radial', cost=C), naiveBayes(y ~ x1 + x2, data=...), rpart(y ~ x1 + x2, data=..., method='class'). (5) Make predictions: Use predict(model, newdata=test_data). (6) Evaluate: Compare predictions to true labels to compute accuracy, confusion matrix, etc. The tutorial notes that for most algorithms, you first generate the model using training data, then predict values for test data using the predict function."
  },
  {
    "question": "Explain how Lemma 1 in the clustering notes helps justify why K-Means assigns cluster centers to the mean of their assigned points.",
    "ground_truth": "Lemma 1 states: For any set C ⊂ R^n and any z ∈ R^n, cost(C; z) = cost(C, mean(C)) + |C| · ||z - mean(C)||₂². This decomposes the cost of placing a representative at z into two parts: the irreducible cost of placing it at the mean, plus an additional penalty proportional to the squared distance from z to the mean. This lemma is proven using a bias-variance decomposition (Lemma 2) applied to a uniform random draw X from points in C, showing E_X||X - z||₂² = E_X||X - x̄||₂² + ||z - x̄||₂² where x̄ = E_X[X] = mean(C). The lemma justifies the K-Means update step: when we have fixed cluster assignments C_k, the best choice for center z_k that minimizes the cost is z_k = mean(C_k). This is because the second term in Lemma 1 is zero only when z = mean(C), and positive otherwise. This is why K-Means alternates between assigning points to nearest centers and updating centers to cluster means—each step provably reduces the cost."
  },
  {
    "question": "How do the origins of logistic regression relate to population modeling, and how was this later applied to classification problems?",
    "ground_truth": "Logistic regression originated in 19th century population modeling. Verhulst and Quetelet developed it to model population growth with a saturation limit, replacing exponential growth (W(t) = Ae^(βt)) with the differential equation dW(t)/dt = βW(t)(Ω - W(t)), where Ω is the saturation limit. This produced the logistic function P(t) = e^(α+βt)/(1 + e^(α+βt)) with values between 0 and 1, fitting population data from France, Belgium, Essex, and Russia. The work was forgotten but reinvented by Pearl and Reed in 1920 for US population growth. The name 'logistic' was revived by Yule in 1925. There was debate over whether the logistic function could replace the CDF of the normal distribution. Joseph Berkson at Mayo Clinic showed it could, but controversy over maximum likelihood delayed acceptance until the 1960s. For classification, the logistic function was adapted by modeling the log odds ratio ln(P(Y=1|x)/P(Y=0|x)) = λ^T x, which transforms the [0,1] probability space into the real line suitable for linear modeling, giving P(Y=1|x,λ) = e^(λ^T x)/(1 + e^(λ^T x))."
  },
  {
    "question": "What are the different node impurity measures mentioned for decision trees and how do they compare?",
    "ground_truth": "Three node impurity measures are discussed for two-class classification: (1) Entropy (used by C4.5): H([p, 1-p]) = -p log₂(p) - (1-p) log₂(1-p), which measures information content and forms the basis of Information Gain. (2) Gini index (used by CART): 2p(1-p), which equals the variance of Bin(n,p)/n = variance of Bernoulli(p). (3) Misclassification error: 1 - max(p, 1-p), which is the proportion of incorrect guesses if you classify the event to happen when p > 1/2 and not happen when p ≤ 1/2. The course notes include a figure showing these three measures plotted as functions of p (the proportion in class 2), where cross-entropy is scaled to pass through (0.5, 0.5). All three measures achieve their maximum at p=0.5 (maximum impurity/uncertainty) and their minimum at p=0 or p=1 (pure nodes). The choice of impurity measure can affect which splits are chosen during tree construction."
  },
  {
    "question": "How does the choice of K affect K-Nearest Neighbors performance and how would you select an appropriate value?",
    "ground_truth": "The choice of K significantly affects K-NN performance through a bias-variance tradeoff: If K is too small, the model will fit noise in the training data (high variance, low bias, overfitting). For K=1, the decision boundary follows training points exactly. If K is too large, neighbors include too many points from other classes, making predictions too smooth (high bias, low variance, underfitting). The course notes suggest determining a good value for K by considering a range of K values and evaluating performance. While not explicitly stated in the K-NN document, from the fundamentals of learning notes, we understand this would typically involve: (1) Using validation data or cross-validation to test different K values, (2) Plotting performance metrics (like accuracy or error rate) against K, (3) Looking for the K that minimizes test error, which represents the best bias-variance tradeoff. The notes also mention that distance-based voting schemes can help, where closer neighbors have more influence, which can mitigate some sensitivity to K choice."
  },
  {
    "question": "Describe the complete workflow of the Apriori algorithm from market basket data to actionable association rules.",
    "ground_truth": "The Apriori workflow: (1) Input: Market basket transaction matrix M where M_{i,j}=1 if transaction i contains item j. (2) Find frequent itemsets: Initialize L₁ = {i: Supp(i) ≥ θ} (frequent 1-itemsets). For k=2 onwards while L_{k-1} ≠ ∅: Generate candidate k-itemsets C_k using apriori_gen(L_{k-1}), which joins pairs in L_{k-1} with identical first k-2 items and prunes candidates whose (k-1)-subsets aren't in L_{k-1}. Scan database to find L_k = {c ∈ C_k: Supp(c) ≥ θ}. Output ⋃_k L_k. (3) Generate rules: For each frequent itemset ℓ: find all nonempty subsets a, output rule a → {ℓ\\a} whenever Supp(ℓ)/Supp(a) ≥ minconf. (4) Handle information overload: Order rules by 'interestingness' measures like confidence (P̂(b|a)), lift (P̂(b|a)/P̂(b)), or interest (P̂(b|a) - P̂(b)). The algorithm leverages downward closure: if a ∪ b is frequent, so are a and b. This allows efficient pruning. Limitations include: huge candidate sets, multiple database scans (at most as many as the size of the largest frequent itemset), and hundreds of possible rules requiring ranking strategies."
  },
  {
    "question": "Explain the relationship between structural risk minimization, regularization, and the bias-variance tradeoff across the course materials.",
    "ground_truth": "Structural Risk Minimization (SRM), regularization, and bias-variance tradeoff are interconnected concepts for controlling model complexity: SRM from computational learning theory says we need bias to learn/generalize and avoid overfitting. It quantifies 'simple models' and provides probabilistic guarantees on true risk R_test based on training risk R_train, sample size m, and model complexity. The regularized learning expression ∑ᵢ R(f(xᵢ),yᵢ) + C·R_reg(f) implements SRM by balancing: (1) empirical risk (training error), and (2) regularization term R_reg(f) that penalizes complexity. For linear models f(x) = ∑ⱼ λⱼx^(j), regularization choices include ||λ||₂² (ridge, SVM) or ||λ||₁ (LASSO, AdaBoost). The bias-variance decomposition E_{x,y,S}[(y-f_S(x))²] = E_x[var_{y|x}(y) + var_S(f_S(x)) + bias(f_S(x))²] explains why this works: simple models (high regularization C) have low variance but potentially high bias, while complex models (low C) have low bias but high variance. Regularization provides the 'structure' or 'bias' that reduces variance, helping R_train approximate R_test. The choice of C controls this tradeoff—large C gives sparser models (more bias, less variance), small C gives better training accuracy (less bias, more variance)."
  },
  {
    "question": "How does hierarchical clustering address the limitations of K-Means clustering?",
    "ground_truth": "Hierarchical clustering addresses a major issue with K-Means: as K changes in K-Means, cluster membership can change arbitrarily—there's no relationship between the K=3 solution and the K=4 solution. Hierarchical clustering creates a nested hierarchy where clusters at one level are created by merging clusters at the next lowest level. This provides a complete clustering structure: at the lowest level, each cluster contains 1 example; at the highest level, there's only 1 cluster containing all data; at intermediate levels, you have different numbers of clusters. The hierarchy shows relationships between clusters at different granularities. This is particularly useful for gene clustering in microarray data, where you want to understand relationships at multiple levels of similarity. The dendrograms produced by hierarchical clustering show how clusters merge, allowing you to cut the tree at different heights to get different numbers of clusters while maintaining the relationships between them. This addresses K-Means' arbitrary cluster reassignment problem and removes the need to pre-specify K."
  }]