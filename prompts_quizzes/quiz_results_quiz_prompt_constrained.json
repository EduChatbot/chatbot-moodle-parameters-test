[
  {
    "config_label": "quiz_prompt_constrained",
    "test_type": "materials_5_6",
    "materialIds": [
      5,
      6
    ],
    "quiz_data": {
      "quizId": 32,
      "courseId": 11,
      "materialIds": [
        5,
        6
      ],
      "section": null,
      "quizTitle": "K-Means Clustering and K-Nearest Neighbors Quiz",
      "questions": [
        {
          "id": 266,
          "question": "In K-Means clustering, what is the primary goal when segmenting a collection of examples into distinct clusters?",
          "options": [
            "Maximizing inter-cluster distances",
            "Minimizing intra-cluster distances",
            "Ensuring equal cluster sizes",
            "Maximizing computational complexity"
          ],
          "correctAnswer": 0,
          "explanation": "The goal of clustering is to group similar examples together, maximizing the distance between different clusters. Minimizing intra-cluster distances is also a goal, but maximizing inter-cluster distance is primary.",
          "source": {
            "chunkId": 38,
            "fileName": "Clustering.pdf",
            "chunkText": "Clustering\n\nMIT 15.097 Course Notes Cynthia Rudin and S¸eyda Ertekin\n\nCredit: Dasgupta, Hastie, Tibshirani, Friedman\n\nClustering (a.k.a. data segmentation) Let’s segment a collection of examples into “clusters” so that objects within a cluster are more closely related to one another than objects assigned to different clusters. We want to assign each ex­ ample xi to a cluster k ∈{1, ...., K}.\n\nThe K-Means algorithm is a very popular way to do this. It assumes points lie\nin Euclidean space.\n\nn Input: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, ..., zK ) :=\nmin Ixi − zkI2\n\n2. k i The choice of the squared norm is fortuitous, it really helps simplify the math!\n\nn If we’re given points {zk}k, they can induce a Voronoi p artition of R : they break the space into cells where each cell corresponds to one of the zk’s. That is, each cell contains the region of space whose nearest representative is zk.\n\nDraw a picture\n\nWe can look at the examples in each of these regions of space, which are the clusters. Specifically,\n\nCk := {xi : the closest representative to xi is zk}.\n\nLet’s compute the cost another way. Before, we summed over examples, and then picked the right representative zk for each example. This time, we’ll sum over clusters, and look at all the examples in that cluster:\n\ncost(z1, ..., zK ) =\nIxi − zkI2\n\n2.\n\nk {i:xi∈Ck}\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 267,
          "question": "What key assumption does the K-Means algorithm make about the data points it is intended to cluster effectively?",
          "options": [
            "Points lie in a non-Euclidean space",
            "Points lie in Euclidean space",
            "Data is normally distributed",
            "Data contains categorical variables"
          ],
          "correctAnswer": 1,
          "explanation": "The K-Means algorithm assumes that the data points lie in Euclidean space, which allows for the calculation of distances between points. This assumption is crucial for the algorithm's operation.",
          "source": {
            "chunkId": 38,
            "fileName": "Clustering.pdf",
            "chunkText": "Clustering\n\nMIT 15.097 Course Notes Cynthia Rudin and S¸eyda Ertekin\n\nCredit: Dasgupta, Hastie, Tibshirani, Friedman\n\nClustering (a.k.a. data segmentation) Let’s segment a collection of examples into “clusters” so that objects within a cluster are more closely related to one another than objects assigned to different clusters. We want to assign each ex­ ample xi to a cluster k ∈{1, ...., K}.\n\nThe K-Means algorithm is a very popular way to do this. It assumes points lie\nin Euclidean space.\n\nn Input: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, ..., zK ) :=\nmin Ixi − zkI2\n\n2. k i The choice of the squared norm is fortuitous, it really helps simplify the math!\n\nn If we’re given points {zk}k, they can induce a Voronoi p artition of R : they break the space into cells where each cell corresponds to one of the zk’s. That is, each cell contains the region of space whose nearest representative is zk.\n\nDraw a picture\n\nWe can look at the examples in each of these regions of space, which are the clusters. Specifically,\n\nCk := {xi : the closest representative to xi is zk}.\n\nLet’s compute the cost another way. Before, we summed over examples, and then picked the right representative zk for each example. This time, we’ll sum over clusters, and look at all the examples in that cluster:\n\ncost(z1, ..., zK ) =\nIxi − zkI2\n\n2.\n\nk {i:xi∈Ck}\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 268,
          "question": "What is the significance of the Voronoi partition in the context of the K-Means clustering algorithm?",
          "options": [
            "It visualizes cluster density",
            "It defines cluster boundaries",
            "It optimizes cluster size",
            "It minimizes computational cost"
          ],
          "correctAnswer": 1,
          "explanation": "The Voronoi partition divides the space into cells, where each cell corresponds to one of the cluster centers (zk). Each cell contains the region of space whose nearest representative is zk, effectively defining cluster boundaries.",
          "source": {
            "chunkId": 38,
            "fileName": "Clustering.pdf",
            "chunkText": "Clustering\n\nMIT 15.097 Course Notes Cynthia Rudin and S¸eyda Ertekin\n\nCredit: Dasgupta, Hastie, Tibshirani, Friedman\n\nClustering (a.k.a. data segmentation) Let’s segment a collection of examples into “clusters” so that objects within a cluster are more closely related to one another than objects assigned to different clusters. We want to assign each ex­ ample xi to a cluster k ∈{1, ...., K}.\n\nThe K-Means algorithm is a very popular way to do this. It assumes points lie\nin Euclidean space.\n\nn Input: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, ..., zK ) :=\nmin Ixi − zkI2\n\n2. k i The choice of the squared norm is fortuitous, it really helps simplify the math!\n\nn If we’re given points {zk}k, they can induce a Voronoi p artition of R : they break the space into cells where each cell corresponds to one of the zk’s. That is, each cell contains the region of space whose nearest representative is zk.\n\nDraw a picture\n\nWe can look at the examples in each of these regions of space, which are the clusters. Specifically,\n\nCk := {xi : the closest representative to xi is zk}.\n\nLet’s compute the cost another way. Before, we summed over examples, and then picked the right representative zk for each example. This time, we’ll sum over clusters, and look at all the examples in that cluster:\n\ncost(z1, ..., zK ) =\nIxi − zkI2\n\n2.\n\nk {i:xi∈Ck}\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 269,
          "question": "According to Lemma 1, where should the representative 'z' be placed to minimize the cost within a single cluster C?",
          "options": [
            "At the median of the examples in C",
            "At the mode of the examples in C",
            "At the mean of the examples in C",
            "At a random point within C"
          ],
          "correctAnswer": 2,
          "explanation": "Lemma 1 states that the representative 'z' should be placed at the mean of the examples in cluster C to minimize the cost. This is because the additional cost incurred by picking z = mean(C) is minimized.",
          "source": {
            "chunkId": 39,
            "fileName": "Clustering.pdf",
            "chunkText": "While we’re analyzing, we’ll need to consider suboptimal partitions of the data, where an example might not be assigned to the nearest representative. So we redefine the cost:\n\nX X\n\nk ∥xi −zk∥2 2. (1)\n\ncost(C1, ..., CK; z1, ..., zK) =\n\n{i:xi∈Ck}\n\nLet’s say we only have one cluster to deal with. Call it C. The representative is z. The cost is then:\ncost(C; z) =\n∥xi −z∥2 2.\n\nX\n\n{ ∈} Where should we place z?\n\ni:xi C\n\nAs you probably guessed, we would put it at the mean of the examples in C. But\nalso, the additional cost incurred by picking z = mean(C) can be characterized\nvery simply:\n\n̸\n\nLemma 1. For any set C ⊂Rn and any z ∈Rn,\n\ncost(C; z) = cost(C, mean(C)) + C\nz mean(C) 2 2. | | · ∥− ∥\n\nLet’s go ahead and prove it. In order to do that, we need to do another biasvariance decomposition (this one’s pretty much identical to one of the ones we did before).\n\nLemma 2. Let X Rn be any random variable. For any z Rn, we have: ∈ ∈\n\nEX∥X −z∥2\n2 = EX∥X −EXX∥2\n2 + ∥z −EXX∥2\n2.\n\nProof. Let x¯ := EXX.\n\nj −z(j))2 X\n\nEX∥X −z∥2\n2 = EX\n(X(j)\n\nj\n−x¯(j) + x¯(j) −zj)2\nX\n\n= EX\n(X(j)\n\nj\n−x¯(j))2 + EX\n(x¯(j)\n\nj −zj)2 X X\n\n= EX\n(X(j)\n\nj −x¯(j))(x¯(j) −z(j)) X\n\n+2EX\n(X(j)\n\n= EX∥X −x¯∥2\n2 + EX∥x¯ −z∥2\n2 + 0. ■\n\n2"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 270,
          "question": "What is the primary reason that trying all possible assignments of data points to clusters in K-Means is computationally infeasible?",
          "options": [
            "The cost function is non-convex",
            "The number of assignments grows exponentially",
            "The algorithm requires extensive memory",
            "The data is often high-dimensional"
          ],
          "correctAnswer": 1,
          "explanation": "The number of distinct assignments of m data points to K clusters grows exponentially, making it computationally infeasible to try all possible assignments. The formula S(m, K) illustrates this exponential growth.",
          "source": {
            "chunkId": 40,
            "fileName": "Clustering.pdf",
            "chunkText": "To prove Lemma 1, pick a specific choice for X, namely X is a uniform random draw from the points xi in set C. So X has a discrete distribution. What will happen with this choice of X is that the expectation will reduce to the cost we already defined above.\n\nX\n\nEX∥X −z∥2\n2 =\n(prob. that point i is chosen)∥xi −z∥2 2 i:xi C\n\n{ ∈} X 1\n=\n\n{i:xi∈C} |C ∥x | i −z∥2 1\n2 =\ncost(C, z) |C| (2)\n\nand if we use Lemma 2 substituting z to be x¯ (a.k.a., EXX, or mean(C)) and simplify as in (2):\n\n1 EX∥X −x¯∥2\n2 =\ncost(C, mean(C)). C | | (3)\n\nWe had already defined cost earlier, and the choice of X was nice because its expectation is just the cost. Let’s recopy Lemma 2’s statement here, using the x¯ notation:\n\nEX∥X −z∥2\n2 = EX∥X −x¯∥2\n2 + ∥z −x¯∥2\n2.\n\nPlugging in (2) and (3),\n\n1 1\ncost(C, z) =\ncost(C, mean(C)) +\n|C| |C ∥z −x¯∥2 | 2. (4)\n\nMultiplying through,\n\ncost(C; z) = cost(C, mean(C)) + |C| · ∥z −mean(C)∥2\n2.\n\nAnd that’s the statement of Lemma 1. ■\n\nTo really minimize the cost (1), you’d need to try all possible assignments of the m data points to K clusters. Uck! The number of distinct assignments is (Jain and Dubes 1988):\n\nK 1\nS(m, K) =\n(\nK! k=1\n−1)K−k K km k\n\nX\n\nS(10, 4) = 34K, S(19, 4)\n1010, ... so not doable. ≈\n\nLet’s try some heuristic gradient-descent-ish method instead.\n\n3"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 271,
          "question": "During the K-Means algorithm, what happens to the cost function with each iteration, assuming the algorithm is functioning correctly?",
          "options": [
            "It increases monotonically",
            "It decreases monotonically",
            "It oscillates randomly",
            "It remains constant"
          ],
          "correctAnswer": 1,
          "explanation": "Lemma 3 states that during the course of the K-Means algorithm, the cost monotonically decreases. This is because each step either assigns data points to closer centers or re-centers clusters at their mean.",
          "source": {
            "chunkId": 41,
            "fileName": "Clustering.pdf",
            "chunkText": "The K-Means Algorithm\n\nChoose the value of K before you start.\n\nn Initialize centers z1, ..., zK ∈ R and clusters C1, ..., CK in any way. Repeat until there is no further change in cost: for each k: Ck ←{xi : the closest representative is zk}\nfor each k: zk = mean(Ck)\n\nThis is simple enough, and takes O(Km) time per iteration.\n\nPPT demo\n\nOf course, it doesn’t always converge to the optimal solution.\n\nBut does the cost converge?\n\nLemma 3. During the course of the K-Means algorithm, the cost monotonically\ndecreases.\n\n(t) (t) (t) (t) Proof. Let z1 , ..., zK , C1 , ..., CK denote the centers and clusters at the start of\nthe tth iterate of K-Means. The first step of the iteration assigns each data point\nto its closest center, therefore, the cluster assignment is better:\n\n(t+1)\n(t+1)\n(t) (t) (t) (t) (t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\nOn the second step, each cluster is re-centered at its mean, so the representatives\nare better. By Lemma 1,\n\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\n•\n\nSo does the cost converge?\n\n4"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 272,
          "question": "What is a key limitation of K-Means clustering regarding changes in the number of clusters (K)?",
          "options": [
            "Cluster membership remains stable",
            "Computational cost decreases linearly",
            "Cluster membership can change arbitrarily",
            "The algorithm always converges optimally"
          ],
          "correctAnswer": 2,
          "explanation": "A major issue with K-means is that as K changes, cluster membership can change arbitrarily. This means that adding or removing a cluster can significantly alter the composition of existing clusters.",
          "source": {
            "chunkId": 44,
            "fileName": "Clustering.pdf",
            "chunkText": "It’s pretty good at keeping the same cancers in the same cluster. The two breast cancers in the 2nd cluster were actually melanomas that metastasized.\n\nGenerally we cluster genes, not patients. Would really like to get something like this in practice:\n\nCourtesy of the Rockefeller University Press. Used with permission.\n\nFigure 7 from Rumfelt, Lynn, et al. \"Lineage Specification and Plasticity in CD19- Early B\ncell Precursors.\" Journal of Experimental Medicine 203 (2006): 675-87.\n\nwhere each row is a gene, and the columns are different immune cell types.\n\nA major issue with K-means: as K changes, cluster membership can change\narbitrarily. A solution is Hierarchical Clustering.\n\n• clusters at the next level of the hierarchy are created by merging clusters at the next lowest level.\n\n– lowest level: each cluster has 1 example\n\n– highest level: there’s only 1 cluster, containing all of the data.\n\n7\n\n[IMAGE_OCR 1] MLP CLP Fr.A Fr. B&C A uma B pw EBFI c D ot"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 273,
          "question": "What is the fundamental principle behind the K-Nearest Neighbors (K-NN) algorithm for classification tasks?",
          "options": [
            "Assigning based on majority vote",
            "Explicitly computing decision boundaries",
            "Minimizing computational complexity",
            "Maximizing inter-cluster distances"
          ],
          "correctAnswer": 0,
          "explanation": "The K-NN algorithm classifies a data point based on the majority vote of its K-Nearest Neighbors. The algorithm uses the labels of the nearest neighbors to predict the label of the new data point.",
          "source": {
            "chunkId": 47,
            "fileName": "k-nearest neighbpurs.pdf",
            "chunkText": "K-NN\n\n15.097 MIT, Spring 2012, Cynthia Rudin Credit: Seyda Ertekin\n\nK-Nearest Neighbors\n\n• Amongst the simplest of all machine learning algorithms. No eXplicit training or model. • Can be used both for classifcaton and regression.\n• Use XIs K-Nearest Neighbors to vote on what\nXIs label should be.\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 274,
          "question": "How does the K-Nearest Neighbors (K-NN) algorithm perform regression, differing from its approach to classification?",
          "options": [
            "By using a weighted average",
            "By computing cluster centroids",
            "By finding decision boundaries",
            "By minimizing intra-cluster variance"
          ],
          "correctAnswer": 0,
          "explanation": "For regression, the K-NN algorithm predicts the value for a test example by calculating the (weighted) average of the values of its K nearest neighbors. This contrasts with classification, which uses majority voting.",
          "source": {
            "chunkId": 49,
            "fileName": "k-nearest neighbpurs.pdf",
            "chunkText": "K-Nearest Neighbors\n\n• For regression: the value for the test eXample becomes the (weighted) average of the values of the K neighbors.\n\nMaking K-NN More Powerful\n\n• A good value for K can be determined by considering a range of K values.\n\n– K too small: we’ll model the noise\n\n– K too large: neighbors include too many points from other classes\n\n• There are problems when there is a spread of distances among the K­\nNN. Use a distance-based voting scheme, where closer neighbors have\nmore influence.\n\n• The distance measure has to be meaningful – attributes should be scaled\n\n– Eg. Income varies 10,000-1,000,000 while height varies 1.5-1.8 meters\n\n3"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 275,
          "question": "In K-NN, what is the likely consequence of selecting a value for K that is excessively large relative to the dataset?",
          "options": [
            "The algorithm models the noise",
            "The algorithm becomes more efficient",
            "Too many points from other classes",
            "The decision boundaries become sharper"
          ],
          "correctAnswer": 2,
          "explanation": "If K is too large, the neighbors considered will include too many points from other classes, which can lead to inaccurate predictions. A balance is needed to avoid modeling noise (small K) and including irrelevant points (large K).",
          "source": {
            "chunkId": 49,
            "fileName": "k-nearest neighbpurs.pdf",
            "chunkText": "K-Nearest Neighbors\n\n• For regression: the value for the test eXample becomes the (weighted) average of the values of the K neighbors.\n\nMaking K-NN More Powerful\n\n• A good value for K can be determined by considering a range of K values.\n\n– K too small: we’ll model the noise\n\n– K too large: neighbors include too many points from other classes\n\n• There are problems when there is a spread of distances among the K­\nNN. Use a distance-based voting scheme, where closer neighbors have\nmore influence.\n\n• The distance measure has to be meaningful – attributes should be scaled\n\n– Eg. Income varies 10,000-1,000,000 while height varies 1.5-1.8 meters\n\n3"
          },
          "grade": null,
          "feedback": null
        }
      ]
    },
    "num_questions_requested": 10,
    "num_questions_generated": 10,
    "latency": 26.27,
    "error": null
  }
]