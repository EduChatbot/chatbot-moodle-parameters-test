[
  {
    "config_label": "quiz_prompt_quiz_scenario",
    "test_type": "materials_5_6",
    "materialIds": [
      5,
      6
    ],
    "quiz_data": {
      "quizId": 36,
      "courseId": 11,
      "materialIds": [
        5,
        6
      ],
      "section": null,
      "quizTitle": "K-Means Clustering and K-Nearest Neighbors Quiz",
      "questions": [
        {
          "id": 306,
          "question": "A data scientist is using K-Means to segment customers. They want to minimize the cost function. Which of the following best describes the cost function K-Means aims to minimize?",
          "options": [
            "The sum of squared distances between each data point and its assigned cluster center.",
            "The sum of distances between each data point and all other data points.",
            "The average distance between all cluster centers.",
            "The maximum distance between any two data points in the dataset."
          ],
          "correctAnswer": 0,
          "explanation": "The K-Means algorithm aims to minimize the sum of the squared Euclidean distances between each data point and the centroid of the cluster it belongs to. This is explicitly stated in the material as the goal of the algorithm.",
          "source": {
            "chunkId": 38,
            "fileName": "Clustering.pdf",
            "chunkText": "Clustering\n\nMIT 15.097 Course Notes Cynthia Rudin and S¸eyda Ertekin\n\nCredit: Dasgupta, Hastie, Tibshirani, Friedman\n\nClustering (a.k.a. data segmentation) Let’s segment a collection of examples into “clusters” so that objects within a cluster are more closely related to one another than objects assigned to different clusters. We want to assign each ex­ ample xi to a cluster k ∈{1, ...., K}.\n\nThe K-Means algorithm is a very popular way to do this. It assumes points lie\nin Euclidean space.\n\nn Input: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, ..., zK ) :=\nmin Ixi − zkI2\n\n2. k i The choice of the squared norm is fortuitous, it really helps simplify the math!\n\nn If we’re given points {zk}k, they can induce a Voronoi p artition of R : they break the space into cells where each cell corresponds to one of the zk’s. That is, each cell contains the region of space whose nearest representative is zk.\n\nDraw a picture\n\nWe can look at the examples in each of these regions of space, which are the clusters. Specifically,\n\nCk := {xi : the closest representative to xi is zk}.\n\nLet’s compute the cost another way. Before, we summed over examples, and then picked the right representative zk for each example. This time, we’ll sum over clusters, and look at all the examples in that cluster:\n\ncost(z1, ..., zK ) =\nIxi − zkI2\n\n2.\n\nk {i:xi∈Ck}\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 307,
          "question": "An analyst is running K-Means and observes that some data points are not assigned to their nearest cluster center. What effect does this have on the cost function?",
          "options": [
            "It decreases the cost function because the algorithm is optimizing globally.",
            "It increases the cost function because the assignment is suboptimal.",
            "It has no effect on the cost function as K-Means is robust to such variations.",
            "It makes the cost function converge faster."
          ],
          "correctAnswer": 1,
          "explanation": "The material explicitly states that suboptimal partitions, where an example might not be assigned to the nearest representative, will lead to a redefinition of the cost function that considers these non-optimal assignments, implying an increase in cost compared to optimal assignments.",
          "source": {
            "chunkId": 39,
            "fileName": "Clustering.pdf",
            "chunkText": "While we’re analyzing, we’ll need to consider suboptimal partitions of the data, where an example might not be assigned to the nearest representative. So we redefine the cost:\n\nX X\n\nk ∥xi −zk∥2 2. (1)\n\ncost(C1, ..., CK; z1, ..., zK) =\n\n{i:xi∈Ck}\n\nLet’s say we only have one cluster to deal with. Call it C. The representative is z. The cost is then:\ncost(C; z) =\n∥xi −z∥2 2.\n\nX\n\n{ ∈} Where should we place z?\n\ni:xi C\n\nAs you probably guessed, we would put it at the mean of the examples in C. But\nalso, the additional cost incurred by picking z = mean(C) can be characterized\nvery simply:\n\n̸\n\nLemma 1. For any set C ⊂Rn and any z ∈Rn,\n\ncost(C; z) = cost(C, mean(C)) + C\nz mean(C) 2 2. | | · ∥− ∥\n\nLet’s go ahead and prove it. In order to do that, we need to do another biasvariance decomposition (this one’s pretty much identical to one of the ones we did before).\n\nLemma 2. Let X Rn be any random variable. For any z Rn, we have: ∈ ∈\n\nEX∥X −z∥2\n2 = EX∥X −EXX∥2\n2 + ∥z −EXX∥2\n2.\n\nProof. Let x¯ := EXX.\n\nj −z(j))2 X\n\nEX∥X −z∥2\n2 = EX\n(X(j)\n\nj\n−x¯(j) + x¯(j) −zj)2\nX\n\n= EX\n(X(j)\n\nj\n−x¯(j))2 + EX\n(x¯(j)\n\nj −zj)2 X X\n\n= EX\n(X(j)\n\nj −x¯(j))(x¯(j) −z(j)) X\n\n+2EX\n(X(j)\n\n= EX∥X −x¯∥2\n2 + EX∥x¯ −z∥2\n2 + 0. ■\n\n2"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 308,
          "question": "A researcher is working with a single cluster 'C' and wants to find the optimal location 'z' to minimize the cost function. According to Lemma 1, where should they place 'z'?",
          "options": [
            "At a randomly chosen point within the cluster.",
            "At the median of the data points in cluster C.",
            "At the mean of the data points in cluster C.",
            "At the point farthest from all other points in cluster C."
          ],
          "correctAnswer": 2,
          "explanation": "Lemma 1 states that the optimal location to minimize the cost function for a single cluster is at the mean of the examples in that cluster. Placing 'z' at the mean minimizes the cost.",
          "source": {
            "chunkId": 39,
            "fileName": "Clustering.pdf",
            "chunkText": "While we’re analyzing, we’ll need to consider suboptimal partitions of the data, where an example might not be assigned to the nearest representative. So we redefine the cost:\n\nX X\n\nk ∥xi −zk∥2 2. (1)\n\ncost(C1, ..., CK; z1, ..., zK) =\n\n{i:xi∈Ck}\n\nLet’s say we only have one cluster to deal with. Call it C. The representative is z. The cost is then:\ncost(C; z) =\n∥xi −z∥2 2.\n\nX\n\n{ ∈} Where should we place z?\n\ni:xi C\n\nAs you probably guessed, we would put it at the mean of the examples in C. But\nalso, the additional cost incurred by picking z = mean(C) can be characterized\nvery simply:\n\n̸\n\nLemma 1. For any set C ⊂Rn and any z ∈Rn,\n\ncost(C; z) = cost(C, mean(C)) + C\nz mean(C) 2 2. | | · ∥− ∥\n\nLet’s go ahead and prove it. In order to do that, we need to do another biasvariance decomposition (this one’s pretty much identical to one of the ones we did before).\n\nLemma 2. Let X Rn be any random variable. For any z Rn, we have: ∈ ∈\n\nEX∥X −z∥2\n2 = EX∥X −EXX∥2\n2 + ∥z −EXX∥2\n2.\n\nProof. Let x¯ := EXX.\n\nj −z(j))2 X\n\nEX∥X −z∥2\n2 = EX\n(X(j)\n\nj\n−x¯(j) + x¯(j) −zj)2\nX\n\n= EX\n(X(j)\n\nj\n−x¯(j))2 + EX\n(x¯(j)\n\nj −zj)2 X X\n\n= EX\n(X(j)\n\nj −x¯(j))(x¯(j) −z(j)) X\n\n+2EX\n(X(j)\n\n= EX∥X −x¯∥2\n2 + EX∥x¯ −z∥2\n2 + 0. ■\n\n2"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 309,
          "question": "A data scientist is trying to determine the absolute best clustering of 20 data points into 4 clusters. What is the primary challenge they will face?",
          "options": [
            "The K-Means algorithm is not guaranteed to converge.",
            "Calculating the mean of each cluster is computationally expensive.",
            "The number of possible assignments grows extremely rapidly, making an exhaustive search infeasible.",
            "The cost function is non-convex, making optimization difficult."
          ],
          "correctAnswer": 2,
          "explanation": "The material explicitly mentions that the number of distinct assignments of m data points to K clusters grows factorially, making it computationally infeasible to try all possible assignments, even for relatively small values of m and K.",
          "source": {
            "chunkId": 40,
            "fileName": "Clustering.pdf",
            "chunkText": "To prove Lemma 1, pick a specific choice for X, namely X is a uniform random draw from the points xi in set C. So X has a discrete distribution. What will happen with this choice of X is that the expectation will reduce to the cost we already defined above.\n\nX\n\nEX∥X −z∥2\n2 =\n(prob. that point i is chosen)∥xi −z∥2 2 i:xi C\n\n{ ∈} X 1\n=\n\n{i:xi∈C} |C ∥x | i −z∥2 1\n2 =\ncost(C, z) |C| (2)\n\nand if we use Lemma 2 substituting z to be x¯ (a.k.a., EXX, or mean(C)) and simplify as in (2):\n\n1 EX∥X −x¯∥2\n2 =\ncost(C, mean(C)). C | | (3)\n\nWe had already defined cost earlier, and the choice of X was nice because its expectation is just the cost. Let’s recopy Lemma 2’s statement here, using the x¯ notation:\n\nEX∥X −z∥2\n2 = EX∥X −x¯∥2\n2 + ∥z −x¯∥2\n2.\n\nPlugging in (2) and (3),\n\n1 1\ncost(C, z) =\ncost(C, mean(C)) +\n|C| |C ∥z −x¯∥2 | 2. (4)\n\nMultiplying through,\n\ncost(C; z) = cost(C, mean(C)) + |C| · ∥z −mean(C)∥2\n2.\n\nAnd that’s the statement of Lemma 1. ■\n\nTo really minimize the cost (1), you’d need to try all possible assignments of the m data points to K clusters. Uck! The number of distinct assignments is (Jain and Dubes 1988):\n\nK 1\nS(m, K) =\n(\nK! k=1\n−1)K−k K km k\n\nX\n\nS(10, 4) = 34K, S(19, 4)\n1010, ... so not doable. ≈\n\nLet’s try some heuristic gradient-descent-ish method instead.\n\n3"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 310,
          "question": "During the K-Means algorithm, what happens to the cost function with each iteration?",
          "options": [
            "It oscillates randomly.",
            "It monotonically increases.",
            "It remains constant.",
            "It monotonically decreases."
          ],
          "correctAnswer": 3,
          "explanation": "Lemma 3 states that during the course of the K-Means algorithm, the cost monotonically decreases. This is because each step either assigns points to closer centers or re-centers the clusters at their means, both of which reduce the cost.",
          "source": {
            "chunkId": 41,
            "fileName": "Clustering.pdf",
            "chunkText": "The K-Means Algorithm\n\nChoose the value of K before you start.\n\nn Initialize centers z1, ..., zK ∈ R and clusters C1, ..., CK in any way. Repeat until there is no further change in cost: for each k: Ck ←{xi : the closest representative is zk}\nfor each k: zk = mean(Ck)\n\nThis is simple enough, and takes O(Km) time per iteration.\n\nPPT demo\n\nOf course, it doesn’t always converge to the optimal solution.\n\nBut does the cost converge?\n\nLemma 3. During the course of the K-Means algorithm, the cost monotonically\ndecreases.\n\n(t) (t) (t) (t) Proof. Let z1 , ..., zK , C1 , ..., CK denote the centers and clusters at the start of\nthe tth iterate of K-Means. The first step of the iteration assigns each data point\nto its closest center, therefore, the cluster assignment is better:\n\n(t+1)\n(t+1)\n(t) (t) (t) (t) (t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\nOn the second step, each cluster is re-centered at its mean, so the representatives\nare better. By Lemma 1,\n\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\n•\n\nSo does the cost converge?\n\n4"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 311,
          "question": "An engineer is using K-Means for a project. After running the algorithm multiple times with different initializations, they notice that the results vary significantly. What is the most likely reason for this?",
          "options": [
            "The dataset is too small.",
            "The K-Means algorithm always converges to the global optimum.",
            "The K-Means algorithm is sensitive to initial centroid positions and can converge to a local optimum.",
            "The learning rate is too high."
          ],
          "correctAnswer": 2,
          "explanation": "The material mentions that K-Means doesn't always converge to the optimal solution. This implies that it can get stuck in local optima, and the final result depends on the initial starting positions of the centroids.",
          "source": {
            "chunkId": 41,
            "fileName": "Clustering.pdf",
            "chunkText": "The K-Means Algorithm\n\nChoose the value of K before you start.\n\nn Initialize centers z1, ..., zK ∈ R and clusters C1, ..., CK in any way. Repeat until there is no further change in cost: for each k: Ck ←{xi : the closest representative is zk}\nfor each k: zk = mean(Ck)\n\nThis is simple enough, and takes O(Km) time per iteration.\n\nPPT demo\n\nOf course, it doesn’t always converge to the optimal solution.\n\nBut does the cost converge?\n\nLemma 3. During the course of the K-Means algorithm, the cost monotonically\ndecreases.\n\n(t) (t) (t) (t) Proof. Let z1 , ..., zK , C1 , ..., CK denote the centers and clusters at the start of\nthe tth iterate of K-Means. The first step of the iteration assigns each data point\nto its closest center, therefore, the cluster assignment is better:\n\n(t+1)\n(t+1)\n(t) (t) (t) (t) (t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\nOn the second step, each cluster is re-centered at its mean, so the representatives\nare better. By Lemma 1,\n\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\n•\n\nSo does the cost converge?\n\n4"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 312,
          "question": "A biologist is using K-Means to cluster microarray data of patients with different types of cancer. After obtaining the clusters, how can they validate the quality of the clusters?",
          "options": [
            "By measuring the test error of a predictive model trained on the clusters.",
            "By using cluster validity measures like the Davies-Baldwin Index or Dunn Index.",
            "By visually inspecting the cluster centers.",
            "By increasing the number of clusters until the cost function reaches zero."
          ],
          "correctAnswer": 1,
          "explanation": "The material suggests using cluster validity measures like the Davies-Baldwin Index and Dunn Index as alternatives to the cost function for evaluating clusters. These indices assess the quality of the clusters based on intracluster and intercluster distances.",
          "source": {
            "chunkId": 42,
            "fileName": "Clustering.pdf",
            "chunkText": "Example of how K-Means could converge to the wrong thing\n\nHow might you make K-Means more likely to converge to the optimal?\n\nHow might you choose K? (Why can’t you measure test error?)\n\nOther ways to evaluate clusters (“cluster validation”)\n\nThere are loads of cluster validity measures, alternatives to the cost. Draw a picture\n\n• Davies-Baldwin Index - looks at average intracluster distance (within-cluster\ndistance) to the centroid (want it to be small), and intercluster distances between centroids (want it to be large).\n\n• Dunn Index - looks pairwise at minimal intercluster distance (want it to be\nlarge) and maximal intracluster distance (want it to be small).\n\nExample: Microarray data. Have 6830 genes (rows) and 64 patients (columns). The color of each box is a measurement of the expression level of a gene. The expression level of a gene is basically how much of its special protein it is pro­ ducing. The physical chip itself doesn’t actually measure protein levels, but a proxy for them (which is RNA, which sticks to the DNA on the chip). If the color is green, it means low expression levels, if the color is red, it means higher expression levels. Each patient is represented by a vector, which is the expression level of their genes. It’s a column vector with values given in color:\n\n5"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 313,
          "question": "A software developer is implementing K-NN for a classification task. What is a key difference between K-NN and K-Means?",
          "options": [
            "K-NN requires explicit training, while K-Means does not.",
            "K-Means is used for classification, while K-NN is used for clustering.",
            "K-NN does not explicitly compute decision boundaries, while K-Means defines cluster centers.",
            "K-Means can only be used for regression, while K-NN can only be used for classification."
          ],
          "correctAnswer": 2,
          "explanation": "K-NN is a lazy learning algorithm that doesn't explicitly compute decision boundaries. Instead, it classifies new instances based on the majority class among its k nearest neighbors in the training data. K-Means, on the other hand, defines cluster centers and assigns data points to the nearest cluster.",
          "source": {
            "chunkId": 48,
            "fileName": "k-nearest neighbpurs.pdf",
            "chunkText": "K-Nearest Neighbors\n\n• Classify using the majority vote of the k closest training points\n\nX X X\n\n(a) 1-nearest neighbor\n(b) 2-nearest neighbor\n(c) 3-nearest neighbor\n\nK-Nearest Neighbors\n\n• K-NN algorithm does not explicitly compute decision\nboundaries. The boundaries between distinct classes form a subset of the Voronoi diagram of the training data.\n\nEach line segment is equidistant to neighboring points.\n\n2"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 314,
          "question": "An engineer is using K-NN for a regression problem. How does K-NN determine the value for a new test example?",
          "options": [
            "By assigning the test example to the closest cluster center.",
            "By taking the median of the values of the K nearest neighbors.",
            "By taking the (weighted) average of the values of the K nearest neighbors.",
            "By randomly selecting a value from one of the K nearest neighbors."
          ],
          "correctAnswer": 2,
          "explanation": "For regression, K-NN predicts the value for a new test example by calculating the (weighted) average of the values of its K nearest neighbors in the training data. This averaging smooths out the predictions and reduces the impact of individual noisy neighbors.",
          "source": {
            "chunkId": 49,
            "fileName": "k-nearest neighbpurs.pdf",
            "chunkText": "K-Nearest Neighbors\n\n• For regression: the value for the test eXample becomes the (weighted) average of the values of the K neighbors.\n\nMaking K-NN More Powerful\n\n• A good value for K can be determined by considering a range of K values.\n\n– K too small: we’ll model the noise\n\n– K too large: neighbors include too many points from other classes\n\n• There are problems when there is a spread of distances among the K­\nNN. Use a distance-based voting scheme, where closer neighbors have\nmore influence.\n\n• The distance measure has to be meaningful – attributes should be scaled\n\n– Eg. Income varies 10,000-1,000,000 while height varies 1.5-1.8 meters\n\n3"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 315,
          "question": "A data scientist is tuning the 'K' parameter in K-NN. What is a potential consequence of choosing a very small value for 'K'?",
          "options": [
            "The model will underfit the data.",
            "The model will be more robust to noise.",
            "The model will model the noise in the data.",
            "The model will include too many points from other classes."
          ],
          "correctAnswer": 2,
          "explanation": "If K is too small, the K-NN model will be highly sensitive to the noise in the training data. Each prediction will be based on very few neighbors, and any noise or outliers among those neighbors will have a disproportionate impact on the result.",
          "source": {
            "chunkId": 49,
            "fileName": "k-nearest neighbpurs.pdf",
            "chunkText": "K-Nearest Neighbors\n\n• For regression: the value for the test eXample becomes the (weighted) average of the values of the K neighbors.\n\nMaking K-NN More Powerful\n\n• A good value for K can be determined by considering a range of K values.\n\n– K too small: we’ll model the noise\n\n– K too large: neighbors include too many points from other classes\n\n• There are problems when there is a spread of distances among the K­\nNN. Use a distance-based voting scheme, where closer neighbors have\nmore influence.\n\n• The distance measure has to be meaningful – attributes should be scaled\n\n– Eg. Income varies 10,000-1,000,000 while height varies 1.5-1.8 meters\n\n3"
          },
          "grade": null,
          "feedback": null
        }
      ]
    },
    "num_questions_requested": 10,
    "num_questions_generated": 10,
    "latency": 35.27,
    "error": null
  }
]