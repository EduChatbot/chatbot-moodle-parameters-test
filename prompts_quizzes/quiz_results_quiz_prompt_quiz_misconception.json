[
  {
    "config_label": "quiz_prompt_quiz_misconception",
    "test_type": "materials_5_6",
    "materialIds": [
      5,
      6
    ],
    "quiz_data": {
      "quizId": 37,
      "courseId": 11,
      "materialIds": [
        5,
        6
      ],
      "section": null,
      "quizTitle": "K-Means Clustering and K-Nearest Neighbors Quiz",
      "questions": [
        {
          "id": 316,
          "question": "What is the primary goal of clustering algorithms like K-Means?",
          "options": [
            "To predict a continuous value for a new data point.",
            "To classify data points into predefined categories.",
            "To segment a collection of examples into clusters based on similarity.",
            "To reduce the dimensionality of the dataset."
          ],
          "correctAnswer": 2,
          "explanation": "The goal of clustering, as stated in the material, is to segment data into clusters where objects within a cluster are more closely related to one another than objects assigned to different clusters. Many students think clustering is about prediction (like regression) or classification, but it's about discovering inherent groupings in the data.",
          "source": {
            "chunkId": 38,
            "fileName": "Clustering.pdf",
            "chunkText": "Clustering\n\nMIT 15.097 Course Notes Cynthia Rudin and S¸eyda Ertekin\n\nCredit: Dasgupta, Hastie, Tibshirani, Friedman\n\nClustering (a.k.a. data segmentation) Let’s segment a collection of examples into “clusters” so that objects within a cluster are more closely related to one another than objects assigned to different clusters. We want to assign each ex­ ample xi to a cluster k ∈{1, ...., K}.\n\nThe K-Means algorithm is a very popular way to do this. It assumes points lie\nin Euclidean space.\n\nn Input: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, ..., zK ) :=\nmin Ixi − zkI2\n\n2. k i The choice of the squared norm is fortuitous, it really helps simplify the math!\n\nn If we’re given points {zk}k, they can induce a Voronoi p artition of R : they break the space into cells where each cell corresponds to one of the zk’s. That is, each cell contains the region of space whose nearest representative is zk.\n\nDraw a picture\n\nWe can look at the examples in each of these regions of space, which are the clusters. Specifically,\n\nCk := {xi : the closest representative to xi is zk}.\n\nLet’s compute the cost another way. Before, we summed over examples, and then picked the right representative zk for each example. This time, we’ll sum over clusters, and look at all the examples in that cluster:\n\ncost(z1, ..., zK ) =\nIxi − zkI2\n\n2.\n\nk {i:xi∈Ck}\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 317,
          "question": "In the K-Means algorithm, what does the cost function aim to minimize?",
          "options": [
            "The distance between cluster centers.",
            "The number of data points in each cluster.",
            "The sum of squared distances between each data point and its cluster center.",
            "The variance within each cluster."
          ],
          "correctAnswer": 2,
          "explanation": "The cost function in K-Means minimizes the sum of squared distances between each data point and its assigned cluster center. Many students might think it minimizes the distance *between* cluster centers, but the algorithm focuses on the distance from points to their respective centers.",
          "source": {
            "chunkId": 38,
            "fileName": "Clustering.pdf",
            "chunkText": "Clustering\n\nMIT 15.097 Course Notes Cynthia Rudin and S¸eyda Ertekin\n\nCredit: Dasgupta, Hastie, Tibshirani, Friedman\n\nClustering (a.k.a. data segmentation) Let’s segment a collection of examples into “clusters” so that objects within a cluster are more closely related to one another than objects assigned to different clusters. We want to assign each ex­ ample xi to a cluster k ∈{1, ...., K}.\n\nThe K-Means algorithm is a very popular way to do this. It assumes points lie\nin Euclidean space.\n\nn Input: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, ..., zK ) :=\nmin Ixi − zkI2\n\n2. k i The choice of the squared norm is fortuitous, it really helps simplify the math!\n\nn If we’re given points {zk}k, they can induce a Voronoi p artition of R : they break the space into cells where each cell corresponds to one of the zk’s. That is, each cell contains the region of space whose nearest representative is zk.\n\nDraw a picture\n\nWe can look at the examples in each of these regions of space, which are the clusters. Specifically,\n\nCk := {xi : the closest representative to xi is zk}.\n\nLet’s compute the cost another way. Before, we summed over examples, and then picked the right representative zk for each example. This time, we’ll sum over clusters, and look at all the examples in that cluster:\n\ncost(z1, ..., zK ) =\nIxi − zkI2\n\n2.\n\nk {i:xi∈Ck}\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 318,
          "question": "What is a Voronoi partition in the context of K-Means?",
          "options": [
            "A method for initializing cluster centers.",
            "A way to visualize the cost function.",
            "A division of the space into cells, where each cell contains the region closest to a specific cluster center.",
            "A technique for determining the optimal number of clusters."
          ],
          "correctAnswer": 2,
          "explanation": "A Voronoi partition divides the space into cells, with each cell containing the region closest to one of the cluster centers. Some students may confuse this with a method for *finding* the clusters, but it's actually a way to *represent* the clusters once the centers are defined.",
          "source": {
            "chunkId": 38,
            "fileName": "Clustering.pdf",
            "chunkText": "Clustering\n\nMIT 15.097 Course Notes Cynthia Rudin and S¸eyda Ertekin\n\nCredit: Dasgupta, Hastie, Tibshirani, Friedman\n\nClustering (a.k.a. data segmentation) Let’s segment a collection of examples into “clusters” so that objects within a cluster are more closely related to one another than objects assigned to different clusters. We want to assign each ex­ ample xi to a cluster k ∈{1, ...., K}.\n\nThe K-Means algorithm is a very popular way to do this. It assumes points lie\nin Euclidean space.\n\nn Input: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, ..., zK ) :=\nmin Ixi − zkI2\n\n2. k i The choice of the squared norm is fortuitous, it really helps simplify the math!\n\nn If we’re given points {zk}k, they can induce a Voronoi p artition of R : they break the space into cells where each cell corresponds to one of the zk’s. That is, each cell contains the region of space whose nearest representative is zk.\n\nDraw a picture\n\nWe can look at the examples in each of these regions of space, which are the clusters. Specifically,\n\nCk := {xi : the closest representative to xi is zk}.\n\nLet’s compute the cost another way. Before, we summed over examples, and then picked the right representative zk for each example. This time, we’ll sum over clusters, and look at all the examples in that cluster:\n\ncost(z1, ..., zK ) =\nIxi − zkI2\n\n2.\n\nk {i:xi∈Ck}\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 319,
          "question": "According to Lemma 1, where should the representative (z) be placed to minimize the cost for a single cluster C?",
          "options": [
            "At a randomly chosen point within C.",
            "At the median of the points in C.",
            "At the point in C farthest from all other points.",
            "At the mean of the points in C."
          ],
          "correctAnswer": 3,
          "explanation": "Lemma 1 states that the cost is minimized when the representative 'z' is placed at the mean of the examples in cluster C. Students may think any point 'within' the cluster would work, but the mean is the mathematically optimal location to minimize the sum of squared distances.",
          "source": {
            "chunkId": 39,
            "fileName": "Clustering.pdf",
            "chunkText": "While we’re analyzing, we’ll need to consider suboptimal partitions of the data, where an example might not be assigned to the nearest representative. So we redefine the cost:\n\nX X\n\nk ∥xi −zk∥2 2. (1)\n\ncost(C1, ..., CK; z1, ..., zK) =\n\n{i:xi∈Ck}\n\nLet’s say we only have one cluster to deal with. Call it C. The representative is z. The cost is then:\ncost(C; z) =\n∥xi −z∥2 2.\n\nX\n\n{ ∈} Where should we place z?\n\ni:xi C\n\nAs you probably guessed, we would put it at the mean of the examples in C. But\nalso, the additional cost incurred by picking z = mean(C) can be characterized\nvery simply:\n\n̸\n\nLemma 1. For any set C ⊂Rn and any z ∈Rn,\n\ncost(C; z) = cost(C, mean(C)) + C\nz mean(C) 2 2. | | · ∥− ∥\n\nLet’s go ahead and prove it. In order to do that, we need to do another biasvariance decomposition (this one’s pretty much identical to one of the ones we did before).\n\nLemma 2. Let X Rn be any random variable. For any z Rn, we have: ∈ ∈\n\nEX∥X −z∥2\n2 = EX∥X −EXX∥2\n2 + ∥z −EXX∥2\n2.\n\nProof. Let x¯ := EXX.\n\nj −z(j))2 X\n\nEX∥X −z∥2\n2 = EX\n(X(j)\n\nj\n−x¯(j) + x¯(j) −zj)2\nX\n\n= EX\n(X(j)\n\nj\n−x¯(j))2 + EX\n(x¯(j)\n\nj −zj)2 X X\n\n= EX\n(X(j)\n\nj −x¯(j))(x¯(j) −z(j)) X\n\n+2EX\n(X(j)\n\n= EX∥X −x¯∥2\n2 + EX∥x¯ −z∥2\n2 + 0. ■\n\n2"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 320,
          "question": "What does Lemma 3 state about the cost during the K-Means algorithm?",
          "options": [
            "The cost monotonically increases.",
            "The cost remains constant.",
            "The cost oscillates randomly.",
            "The cost monotonically decreases."
          ],
          "correctAnswer": 3,
          "explanation": "Lemma 3 proves that the cost monotonically decreases during the K-Means algorithm. Some students might assume the cost could fluctuate, but the algorithm is designed to iteratively improve the cluster assignments and center locations, leading to a consistent reduction in cost.",
          "source": {
            "chunkId": 41,
            "fileName": "Clustering.pdf",
            "chunkText": "The K-Means Algorithm\n\nChoose the value of K before you start.\n\nn Initialize centers z1, ..., zK ∈ R and clusters C1, ..., CK in any way. Repeat until there is no further change in cost: for each k: Ck ←{xi : the closest representative is zk}\nfor each k: zk = mean(Ck)\n\nThis is simple enough, and takes O(Km) time per iteration.\n\nPPT demo\n\nOf course, it doesn’t always converge to the optimal solution.\n\nBut does the cost converge?\n\nLemma 3. During the course of the K-Means algorithm, the cost monotonically\ndecreases.\n\n(t) (t) (t) (t) Proof. Let z1 , ..., zK , C1 , ..., CK denote the centers and clusters at the start of\nthe tth iterate of K-Means. The first step of the iteration assigns each data point\nto its closest center, therefore, the cluster assignment is better:\n\n(t+1)\n(t+1)\n(t) (t) (t) (t) (t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\nOn the second step, each cluster is re-centered at its mean, so the representatives\nare better. By Lemma 1,\n\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\n•\n\nSo does the cost converge?\n\n4"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 321,
          "question": "What is a major drawback of K-Means clustering as K (the number of clusters) changes?",
          "options": [
            "The algorithm becomes computationally less efficient.",
            "The cluster membership can change arbitrarily.",
            "The cost function always increases.",
            "The algorithm always converges to the optimal solution."
          ],
          "correctAnswer": 1,
          "explanation": "A significant issue with K-Means is that as K changes, cluster membership can change arbitrarily. Many students might think that increasing K will always lead to a 'better' or more refined clustering, but the changes can be unpredictable and not necessarily meaningful.",
          "source": {
            "chunkId": 44,
            "fileName": "Clustering.pdf",
            "chunkText": "It’s pretty good at keeping the same cancers in the same cluster. The two breast cancers in the 2nd cluster were actually melanomas that metastasized.\n\nGenerally we cluster genes, not patients. Would really like to get something like this in practice:\n\nCourtesy of the Rockefeller University Press. Used with permission.\n\nFigure 7 from Rumfelt, Lynn, et al. \"Lineage Specification and Plasticity in CD19- Early B\ncell Precursors.\" Journal of Experimental Medicine 203 (2006): 675-87.\n\nwhere each row is a gene, and the columns are different immune cell types.\n\nA major issue with K-means: as K changes, cluster membership can change\narbitrarily. A solution is Hierarchical Clustering.\n\n• clusters at the next level of the hierarchy are created by merging clusters at the next lowest level.\n\n– lowest level: each cluster has 1 example\n\n– highest level: there’s only 1 cluster, containing all of the data.\n\n7\n\n[IMAGE_OCR 1] MLP CLP Fr.A Fr. B&C A uma B pw EBFI c D ot"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 322,
          "question": "What is Hierarchical Clustering used for, in the context of the K-Means drawbacks?",
          "options": [
            "To speed up the K-Means algorithm.",
            "To avoid the arbitrary changes in cluster membership as K changes.",
            "To automatically determine the optimal value of K.",
            "To improve the visualization of K-Means results."
          ],
          "correctAnswer": 1,
          "explanation": "Hierarchical clustering is presented as a solution to the issue of arbitrary changes in cluster membership that occur as K changes in K-Means. Students may think it's just another clustering method, but it's specifically highlighted as an alternative to address a limitation of K-Means.",
          "source": {
            "chunkId": 44,
            "fileName": "Clustering.pdf",
            "chunkText": "It’s pretty good at keeping the same cancers in the same cluster. The two breast cancers in the 2nd cluster were actually melanomas that metastasized.\n\nGenerally we cluster genes, not patients. Would really like to get something like this in practice:\n\nCourtesy of the Rockefeller University Press. Used with permission.\n\nFigure 7 from Rumfelt, Lynn, et al. \"Lineage Specification and Plasticity in CD19- Early B\ncell Precursors.\" Journal of Experimental Medicine 203 (2006): 675-87.\n\nwhere each row is a gene, and the columns are different immune cell types.\n\nA major issue with K-means: as K changes, cluster membership can change\narbitrarily. A solution is Hierarchical Clustering.\n\n• clusters at the next level of the hierarchy are created by merging clusters at the next lowest level.\n\n– lowest level: each cluster has 1 example\n\n– highest level: there’s only 1 cluster, containing all of the data.\n\n7\n\n[IMAGE_OCR 1] MLP CLP Fr.A Fr. B&C A uma B pw EBFI c D ot"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 323,
          "question": "In K-Nearest Neighbors (K-NN), what is the primary factor used to classify a new data point?",
          "options": [
            "The average distance to all other data points.",
            "The majority vote of the labels of its K nearest neighbors.",
            "The cluster center closest to the data point.",
            "A predefined decision boundary."
          ],
          "correctAnswer": 1,
          "explanation": "K-NN classifies a new data point based on the majority vote of the labels of its K nearest neighbors. Students may confuse this with K-Means, thinking it involves cluster centers, but K-NN directly uses the labels of nearby data points.",
          "source": {
            "chunkId": 48,
            "fileName": "k-nearest neighbpurs.pdf",
            "chunkText": "K-Nearest Neighbors\n\n• Classify using the majority vote of the k closest training points\n\nX X X\n\n(a) 1-nearest neighbor\n(b) 2-nearest neighbor\n(c) 3-nearest neighbor\n\nK-Nearest Neighbors\n\n• K-NN algorithm does not explicitly compute decision\nboundaries. The boundaries between distinct classes form a subset of the Voronoi diagram of the training data.\n\nEach line segment is equidistant to neighboring points.\n\n2"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 324,
          "question": "What happens if the value of K is too small in K-NN?",
          "options": [
            "The algorithm will be more robust to noise.",
            "The algorithm will model the noise in the data.",
            "The algorithm will include too many points from other classes.",
            "The decision boundaries will be smoother."
          ],
          "correctAnswer": 1,
          "explanation": "If K is too small in K-NN, the algorithm will model the noise in the data. Students might think a smaller K is always better for capturing local patterns, but it makes the model overly sensitive to individual noisy data points.",
          "source": {
            "chunkId": 49,
            "fileName": "k-nearest neighbpurs.pdf",
            "chunkText": "K-Nearest Neighbors\n\n• For regression: the value for the test eXample becomes the (weighted) average of the values of the K neighbors.\n\nMaking K-NN More Powerful\n\n• A good value for K can be determined by considering a range of K values.\n\n– K too small: we’ll model the noise\n\n– K too large: neighbors include too many points from other classes\n\n• There are problems when there is a spread of distances among the K­\nNN. Use a distance-based voting scheme, where closer neighbors have\nmore influence.\n\n• The distance measure has to be meaningful – attributes should be scaled\n\n– Eg. Income varies 10,000-1,000,000 while height varies 1.5-1.8 meters\n\n3"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 325,
          "question": "What is the role of the Voronoi diagram in K-Nearest Neighbors (KNN)?",
          "options": [
            "To define the cluster centers.",
            "To visualize the cost function.",
            "To explicitly compute decision boundaries.",
            "To represent the boundaries between distinct classes."
          ],
          "correctAnswer": 3,
          "explanation": "In KNN, the boundaries between distinct classes form a subset of the Voronoi diagram of the training data. The KNN algorithm does not explicitly compute decision boundaries. Students may think that KNN explicitly calculates decision boundaries, but the boundaries are implicitly defined by the Voronoi diagram.",
          "source": {
            "chunkId": 48,
            "fileName": "k-nearest neighbpurs.pdf",
            "chunkText": "K-Nearest Neighbors\n\n• Classify using the majority vote of the k closest training points\n\nX X X\n\n(a) 1-nearest neighbor\n(b) 2-nearest neighbor\n(c) 3-nearest neighbor\n\nK-Nearest Neighbors\n\n• K-NN algorithm does not explicitly compute decision\nboundaries. The boundaries between distinct classes form a subset of the Voronoi diagram of the training data.\n\nEach line segment is equidistant to neighboring points.\n\n2"
          },
          "grade": null,
          "feedback": null
        }
      ]
    },
    "num_questions_requested": 10,
    "num_questions_generated": 10,
    "latency": 26.43,
    "error": null
  }
]