[
  {
    "config_label": "quiz_prompt_quiz_misconception",
    "test_type": "materials_5_6",
    "materialIds": [
      5,
      6
    ],
    "quiz_data": null,
    "num_questions_requested": 15,
    "num_questions_generated": 0,
    "latency": 0,
    "error": "HTTP 404"
  },
  {
    "config_label": "quiz_prompt_quiz_misconception",
    "test_type": "full_course",
    "topic": null,
    "quiz_data": {
      "quizId": 71,
      "courseId": 11,
      "materialIds": [],
      "section": null,
      "quizTitle": "Decision Tree Fundamentals Quiz",
      "questions": [
        {
          "id": 886,
          "question": "What is the primary purpose of pruning a decision tree?",
          "options": [
            "To increase the tree's complexity and ability to fit the training data perfectly.",
            "To reduce overfitting by simplifying the tree and improving its generalization to unseen data.",
            "To speed up the tree's training time by removing irrelevant branches early on.",
            "To ensure that the tree uses all available attributes in the dataset."
          ],
          "correctAnswer": 1,
          "explanation": "Pruning aims to prevent overfitting. Many students think that a more complex tree is always better, but according to the material, a simpler tree generalizes better to unseen data because it avoids memorizing noise in the training set. Overfitting leads to poor performance on new data.",
          "source": {
            "chunkId": 224,
            "fileName": "Decision trees.pdf",
            "chunkText": "-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “splitting” attributes one by one. To determine which attribute to split, look at “node impurity.”\n\n• Assign leaf nodes the majority vote in the leaf.\n\n• When we get to the bottom, prune the tree to prevent overfitting\n\nWhy is this a good way to build a tree?\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 887,
          "question": "Which of the following best describes how a decision tree is typically built?",
          "options": [
            "Start with a small tree and iteratively add branches based on random attribute selection.",
            "Start at the top and recursively split attributes based on node impurity, assigning leaf nodes the majority vote.",
            "Build multiple small trees and combine their predictions using averaging.",
            "Use a single attribute to create a simple tree, then refine it using complex mathematical formulas."
          ],
          "correctAnswer": 1,
          "explanation": "Decision trees are built top-down by splitting attributes based on node impurity. Many students might think trees are built randomly or by averaging, but according to the material, the process involves a structured, recursive splitting approach to create branches and leaves based on data characteristics.",
          "source": {
            "chunkId": 224,
            "fileName": "Decision trees.pdf",
            "chunkText": "-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “splitting” attributes one by one. To determine which attribute to split, look at “node impurity.”\n\n• Assign leaf nodes the majority vote in the leaf.\n\n• When we get to the bottom, prune the tree to prevent overfitting\n\nWhy is this a good way to build a tree?\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 888,
          "question": "What is 'node impurity' used for in the context of building a decision tree?",
          "options": [
            "To measure the amount of missing data in a node.",
            "To determine which attribute to split at a given node.",
            "To calculate the probability of misclassification at a leaf node.",
            "To identify irrelevant attributes that should be removed from the dataset."
          ],
          "correctAnswer": 1,
          "explanation": "Node impurity helps determine the best attribute to split on. Many students might confuse it with missing data or misclassification probability, but according to the material, node impurity is specifically used to guide the attribute selection process during tree construction, aiming to create more homogeneous child nodes.",
          "source": {
            "chunkId": 224,
            "fileName": "Decision trees.pdf",
            "chunkText": "-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “splitting” attributes one by one. To determine which attribute to split, look at “node impurity.”\n\n• Assign leaf nodes the majority vote in the leaf.\n\n• When we get to the bottom, prune the tree to prevent overfitting\n\nWhy is this a good way to build a tree?\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 889,
          "question": "What is the purpose of the Gini index in the CART algorithm?",
          "options": [
            "To measure the overall accuracy of the decision tree.",
            "To determine the optimal depth of the decision tree.",
            "To evaluate the impurity of a node and guide the splitting process.",
            "To calculate the probability of a specific class in the dataset."
          ],
          "correctAnswer": 2,
          "explanation": "The Gini index measures node impurity in CART. Many students might think it's related to overall accuracy or tree depth, but according to the material, the Gini index is a specific metric used to assess the homogeneity of a node and guide the selection of the best split.",
          "source": {
            "chunkId": 256,
            "fileName": "Decision trees.pdf",
            "chunkText": "CART - Classification and Regression Trees (Breiman, Freedman, Olshen, Stone,\n1984)\n\nDoes only binary splits, not multiway splits (less interpretable, but simplifies splitting criteria).\n\nLet’s do classification first, and regression later. For splitting, CART uses the Gini index. The Gini index again is\n\nvariance of Bin(n, p)\np(1 −p) =\n= variance of Bernoulli(p).\nn For pruning, CART uses “minimal cost complexity.”\n\nEach subtree is assigned a cost. The first term in the cost is a misclassification error. Th"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 890,
          "question": "In the context of decision trees, what does 'CART' stand for?",
          "options": [
            "Classification and Regression Trees",
            "Continuous Attribute Regression Tree",
            "Categorical Attribute Regression Technique",
            "Classification and Rule-based Trees"
          ],
          "correctAnswer": 0,
          "explanation": "CART stands for Classification and Regression Trees. Many students may not know the acronym, but according to the material, CART is a specific type of decision tree algorithm capable of both classification and regression tasks.",
          "source": {
            "chunkId": 256,
            "fileName": "Decision trees.pdf",
            "chunkText": "CART - Classification and Regression Trees (Breiman, Freedman, Olshen, Stone,\n1984)\n\nDoes only binary splits, not multiway splits (less interpretable, but simplifies splitting criteria).\n\nLet’s do classification first, and regression later. For splitting, CART uses the Gini index. The Gini index again is\n\nvariance of Bin(n, p)\np(1 −p) =\n= variance of Bernoulli(p).\nn For pruning, CART uses “minimal cost complexity.”\n\nEach subtree is assigned a cost. The first term in the cost is a misclassification error. Th"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 891,
          "question": "What is the key difference between CART and other decision tree algorithms regarding splits?",
          "options": [
            "CART allows for multiway splits, creating more interpretable trees.",
            "CART only performs binary splits, simplifying the splitting criteria.",
            "CART uses entropy to determine splits, while other algorithms use the Gini index.",
            "CART can handle missing values, while other algorithms cannot."
          ],
          "correctAnswer": 1,
          "explanation": "CART performs only binary splits. Many students might think CART allows multiway splits for better interpretability, but according to the material, CART's binary splits simplify the splitting criteria, even if it slightly reduces interpretability compared to multiway splits.",
          "source": {
            "chunkId": 256,
            "fileName": "Decision trees.pdf",
            "chunkText": "CART - Classification and Regression Trees (Breiman, Freedman, Olshen, Stone,\n1984)\n\nDoes only binary splits, not multiway splits (less interpretable, but simplifies splitting criteria).\n\nLet’s do classification first, and regression later. For splitting, CART uses the Gini index. The Gini index again is\n\nvariance of Bin(n, p)\np(1 −p) =\n= variance of Bernoulli(p).\nn For pruning, CART uses “minimal cost complexity.”\n\nEach subtree is assigned a cost. The first term in the cost is a misclassification error. Th"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 892,
          "question": "What is the purpose of 'minimal cost complexity' in the context of CART?",
          "options": [
            "To minimize the computational cost of building the decision tree.",
            "To simplify the tree by removing branches that do not significantly reduce misclassification error, balancing accuracy and complexity.",
            "To reduce the memory usage of the decision tree model.",
            "To ensure that the decision tree is easily interpretable by non-technical users."
          ],
          "correctAnswer": 1,
          "explanation": "Minimal cost complexity is used for pruning in CART. Many students might think it's about computational cost or interpretability, but according to the material, it's a method to balance accuracy and complexity by pruning branches that don't significantly improve performance.",
          "source": {
            "chunkId": 256,
            "fileName": "Decision trees.pdf",
            "chunkText": "CART - Classification and Regression Trees (Breiman, Freedman, Olshen, Stone,\n1984)\n\nDoes only binary splits, not multiway splits (less interpretable, but simplifies splitting criteria).\n\nLet’s do classification first, and regression later. For splitting, CART uses the Gini index. The Gini index again is\n\nvariance of Bin(n, p)\np(1 −p) =\n= variance of Bernoulli(p).\nn For pruning, CART uses “minimal cost complexity.”\n\nEach subtree is assigned a cost. The first term in the cost is a misclassification error. Th"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 893,
          "question": "What is the first step in building a decision tree?",
          "options": [
            "Pruning the tree to avoid overfitting.",
            "Assigning leaf nodes the majority vote.",
            "Splitting attributes based on information gain.",
            "Starting at the top of the tree."
          ],
          "correctAnswer": 3,
          "explanation": "The first step is to start at the top. Many students might think splitting or pruning comes first, but according to the material, the process begins with establishing the root of the tree before any splitting or pruning can occur.",
          "source": {
            "chunkId": 224,
            "fileName": "Decision trees.pdf",
            "chunkText": "-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “splitting” attributes one by one. To determine which attribute to split, look at “node impurity.”\n\n• Assign leaf nodes the majority vote in the leaf.\n\n• When we get to the bottom, prune the tree to prevent overfitting\n\nWhy is this a good way to build a tree?\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 894,
          "question": "What is the purpose of calculating Information Gain when building a decision tree?",
          "options": [
            "To determine the probability of a specific outcome.",
            "To measure the reduction in entropy after splitting on an attribute.",
            "To calculate the overall accuracy of the decision tree.",
            "To identify the most frequent class in the dataset."
          ],
          "correctAnswer": 1,
          "explanation": "Information Gain measures the reduction in entropy. Many students might confuse it with overall accuracy or outcome probability, but according to the material, Information Gain specifically quantifies how much uncertainty is reduced by splitting on a particular attribute.",
          "source": {
            "chunkId": 236,
            "fileName": "Decision trees.pdf",
            "chunkText": "gatives.\n\nThe training probabilities\n\nin branch j are:\n\n#posj #negj ,\n#posj + #negj #posj + #negj\n\n.\n\nThe Information Gain is calculated like this:\n\nGain(S, A) = expected reduction in entropy due to branching on attribute A\n= original\n\nentropy −entropy after branching #pos #neg\n= H\n,\n#pos + #neg #pos + #neg\n\nJ\n\n−\nX #posj + #negj\n#posj #negj H , ."
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 895,
          "question": "What does the formula Gain(S, A) = H(S) - Σ [(|Sj| / |S|) * H(Sj)] represent?",
          "options": [
            "The overall accuracy of the decision tree.",
            "The information gained by splitting dataset S on attribute A.",
            "The entropy of the dataset S.",
            "The cost complexity of the decision tree."
          ],
          "correctAnswer": 1,
          "explanation": "The formula represents the information gained by splitting on attribute A. Many students might think it's about overall accuracy or entropy alone, but according to the material, the formula specifically calculates the reduction in entropy achieved by the split.",
          "source": {
            "chunkId": 236,
            "fileName": "Decision trees.pdf",
            "chunkText": "gatives.\n\nThe training probabilities\n\nin branch j are:\n\n#posj #negj ,\n#posj + #negj #posj + #negj\n\n.\n\nThe Information Gain is calculated like this:\n\nGain(S, A) = expected reduction in entropy due to branching on attribute A\n= original\n\nentropy −entropy after branching #pos #neg\n= H\n,\n#pos + #neg #pos + #neg\n\nJ\n\n−\nX #posj + #negj\n#posj #negj H , ."
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 896,
          "question": "What is the primary goal of training a supervised learning model, such as a decision tree?",
          "options": [
            "To minimize the amount of data used for training.",
            "To create a model that can accurately predict outcomes on unseen data.",
            "To maximize the complexity of the model.",
            "To identify irrelevant features in the dataset."
          ],
          "correctAnswer": 1,
          "explanation": "The goal is to predict outcomes on unseen data. Many students might think the goal is to minimize data usage or maximize complexity, but according to the material, the primary objective is to build a model that generalizes well to new, unseen data.",
          "source": {
            "chunkId": 146,
            "fileName": "Fundamentals of Learning.pdf",
            "chunkText": "1|x) as possible.\n\n• estimate probability of failure, probability to default on loan\n\nRule mining and clustering are unsupervised methods (no ground truth), and classification, ranking, and density estimation are supervised methods (there is ground truth). In all of these problems, we do not assume we know the distribution that the data are drawn from!\n\nTraining and Testing (in-sample and out-of-sample) for supervised learning\n\nTraining: training data are input, and model f is the output.\n\ni=1 =⇒ Algorithm "
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 897,
          "question": "What is the purpose of a 'loss function' in the context of training a supervised learning model?",
          "options": [
            "To measure the computational cost of training the model.",
            "To quantify the difference between the model's predictions and the actual values.",
            "To identify missing values in the training data.",
            "To determine the optimal learning rate for the model."
          ],
          "correctAnswer": 1,
          "explanation": "A loss function quantifies the difference between predictions and actual values. Many students might think it's about computational cost or data quality, but according to the material, the loss function specifically measures the error made by the model, guiding the learning process.",
          "source": {
            "chunkId": 147,
            "fileName": "Fundamentals of Learning.pdf",
            "chunkText": "upervised learning\n\nTraining: training data are input, and model f is the output.\n\ni=1 =⇒ Algorithm =⇒ f.\n\n{(xi, yi)}m\n\nTesting: You want to predict y for a new x, where (x, y) comes from the same distribution as {(xi, yi)}m\n\ni=1.\n\nThat is, (x, y) ∼ D(X , Y) and each (xi, yi) ∼ D(X , Y).\n\nCompute f(x) and compare it to y. How well does f(x) match y? Measure goodness of f using a loss function R : Y × Y → R:\n\nRtest(f) = E(x,y)∼DR(f(x), y)\n\n=\nR(f(x), y)dD(x, y). (x,y)∼D\n\n2"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 898,
          "question": "In the context of decision tree pruning with C4.5, what is being compared when deciding whether to prune a subtree to a leaf?",
          "options": [
            "The number of nodes in the subtree versus the number of leaves.",
            "The upper bound on the probability of error for the subtree versus the upper bound on the probability of error for the leaf.",
            "The training accuracy of the subtree versus the testing accuracy of the leaf.",
            "The depth of the subtree versus the overall depth of the tree."
          ],
          "correctAnswer": 1,
          "explanation": "C4.5 compares the upper bound on the probability of error. Many students might think it's about accuracy or tree structure, but according to the material, the decision to prune is based on comparing error estimates to determine if pruning improves generalization.",
          "source": {
            "chunkId": 246,
            "fileName": "Decision trees.pdf",
            "chunkText": "ce that part of the tree with a leaf corresponding to the most frequent label in the data S going to that part of the tree.\n\n• Option 3: replace that part of the tree with one of its subtrees, corresponding to the most common branch in the split\n\nDemo To figure out which decision to make, C4.5 computes upper bounds on the probability of error for each option. I’ll show you how to do that shortly.\n\n• Prob of error for Option 1 ≤UpperBound1\n\n• Prob of error for Option 2 ≤UpperBound2\n\n10"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 899,
          "question": "What is the main advantage of using decision trees?",
          "options": [
            "They always provide the most accurate predictions compared to other machine learning models.",
            "They are easy to interpret and visualize, making them useful for understanding the decision-making process.",
            "They can handle an unlimited number of features without overfitting.",
            "They automatically handle missing data without any preprocessing."
          ],
          "correctAnswer": 1,
          "explanation": "Decision trees are easy to interpret. Many students might think they are always the most accurate or can handle unlimited features, but according to the material, a key advantage is their interpretability, allowing users to understand how decisions are made.",
          "source": {
            "chunkId": 227,
            "fileName": "Decision trees.pdf",
            "chunkText": "Two examples of de cision trees. Opti on\n\nfir\n\non B is\n\nA i s sp lit\n\nsplit first by Ty pe.\n\nwh ile\n\nO pti\n\nst by\n\nP atr\n\non s,\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWhat we need is a formula to compute “information.” Before we do that, here’s another example. Let’s say we pick one of them (Patrons). Maybe then we’ll pick Hungry next, because it has a lot of “information”:\n\nA decision tree sp lit first by Patron s,\n\ngry\n\nthe n by Hun\n\n."
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 900,
          "question": "What is the purpose of 'holdout data' when building a decision tree with cost complexity pruning?",
          "options": [
            "To train the decision tree model.",
            "To select the optimal value of the complexity parameter C by evaluating the performance of different pruned trees.",
            "To estimate the amount of missing data in the dataset.",
            "To visualize the structure of the decision tree."
          ],
          "correctAnswer": 1,
          "explanation": "Holdout data is used to select the optimal complexity parameter C. Many students might think it's used for training or data estimation, but according to the material, holdout data is specifically used to evaluate different pruned trees and choose the best level of complexity.",
          "source": {
            "chunkId": 258,
            "fileName": "Decision trees.pdf",
            "chunkText": "imizes the cost will have better training accuracy.\n\nX\n1[yi=leaf’s class] + C [#leaves in subtree] .\n\nX\n\ncost(subtree) =\n\n̸\n\nves j xi∈\n\nlea\n\nleaf j\n\nWe could create a sequence of nested subtrees by gradually increasing C.\n\nDraw a picture\n\nNow we need to choose C. Here’s one way to do this:\n\n• Step 1: For each C, hold out some data, split, then prune, producing a tree for each C.\n\n• Step 2: see which tree is the best on the holdout data, choose C.\n\n• Step 3: use all data, use chosen C, run split, then prune "
          },
          "grade": null,
          "feedback": null
        }
      ]
    },
    "num_questions_requested": 15,
    "num_questions_generated": 15,
    "latency": 31.02,
    "error": null
  },
  {
    "config_label": "quiz_prompt_quiz_misconception",
    "test_type": "topic",
    "topic": "decision trees",
    "quiz_data": null,
    "num_questions_requested": 15,
    "num_questions_generated": 0,
    "latency": 0,
    "error": "HTTP 404"
  }
]