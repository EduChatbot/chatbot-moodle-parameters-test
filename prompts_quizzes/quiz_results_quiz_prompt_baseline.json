[
  {
    "config_label": "quiz_prompt_baseline",
    "test_type": "materials_5_6",
    "materialIds": [
      5,
      6
    ],
    "quiz_data": {
      "quizId": 31,
      "courseId": 11,
      "materialIds": [
        5,
        6
      ],
      "section": null,
      "quizTitle": "Clustering and K-Nearest Neighbors Quiz",
      "questions": [
        {
          "id": 256,
          "question": "What is the primary goal of clustering?",
          "options": [
            "To segment a collection of examples into clusters where objects within a cluster are closely related.",
            "To predict future data points based on past data.",
            "To classify data points into predefined categories.",
            "To reduce the dimensionality of the data."
          ],
          "correctAnswer": 0,
          "explanation": "Clustering aims to group similar objects together into clusters, ensuring high similarity within clusters and dissimilarity between them. The other options describe different machine learning tasks.",
          "source": {
            "chunkId": 38,
            "fileName": "Clustering.pdf",
            "chunkText": "Clustering\n\nMIT 15.097 Course Notes Cynthia Rudin and S¸eyda Ertekin\n\nCredit: Dasgupta, Hastie, Tibshirani, Friedman\n\nClustering (a.k.a. data segmentation) Let’s segment a collection of examples into “clusters” so that objects within a cluster are more closely related to one another than objects assigned to different clusters. We want to assign each ex­ ample xi to a cluster k ∈{1, ...., K}.\n\nThe K-Means algorithm is a very popular way to do this. It assumes points lie\nin Euclidean space.\n\nn Input: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, ..., zK ) :=\nmin Ixi − zkI2\n\n2. k i The choice of the squared norm is fortuitous, it really helps simplify the math!\n\nn If we’re given points {zk}k, they can induce a Voronoi p artition of R : they break the space into cells where each cell corresponds to one of the zk’s. That is, each cell contains the region of space whose nearest representative is zk.\n\nDraw a picture\n\nWe can look at the examples in each of these regions of space, which are the clusters. Specifically,\n\nCk := {xi : the closest representative to xi is zk}.\n\nLet’s compute the cost another way. Before, we summed over examples, and then picked the right representative zk for each example. This time, we’ll sum over clusters, and look at all the examples in that cluster:\n\ncost(z1, ..., zK ) =\nIxi − zkI2\n\n2.\n\nk {i:xi∈Ck}\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 257,
          "question": "What space does the K-Means algorithm assume points lie in?",
          "options": [
            "Non-Euclidean space",
            "Euclidean space",
            "Hyperbolic space",
            "Spherical space"
          ],
          "correctAnswer": 1,
          "explanation": "The K-Means algorithm is designed to work with data points in Euclidean space, where distances can be calculated using the Euclidean norm.",
          "source": {
            "chunkId": 38,
            "fileName": "Clustering.pdf",
            "chunkText": "Clustering\n\nMIT 15.097 Course Notes Cynthia Rudin and S¸eyda Ertekin\n\nCredit: Dasgupta, Hastie, Tibshirani, Friedman\n\nClustering (a.k.a. data segmentation) Let’s segment a collection of examples into “clusters” so that objects within a cluster are more closely related to one another than objects assigned to different clusters. We want to assign each ex­ ample xi to a cluster k ∈{1, ...., K}.\n\nThe K-Means algorithm is a very popular way to do this. It assumes points lie\nin Euclidean space.\n\nn Input: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, ..., zK ) :=\nmin Ixi − zkI2\n\n2. k i The choice of the squared norm is fortuitous, it really helps simplify the math!\n\nn If we’re given points {zk}k, they can induce a Voronoi p artition of R : they break the space into cells where each cell corresponds to one of the zk’s. That is, each cell contains the region of space whose nearest representative is zk.\n\nDraw a picture\n\nWe can look at the examples in each of these regions of space, which are the clusters. Specifically,\n\nCk := {xi : the closest representative to xi is zk}.\n\nLet’s compute the cost another way. Before, we summed over examples, and then picked the right representative zk for each example. This time, we’ll sum over clusters, and look at all the examples in that cluster:\n\ncost(z1, ..., zK ) =\nIxi − zkI2\n\n2.\n\nk {i:xi∈Ck}\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 258,
          "question": "In the context of K-Means, what is a Voronoi partition?",
          "options": [
            "A method for calculating the mean of data points.",
            "A way to divide space into cells, where each cell corresponds to the nearest cluster center.",
            "A technique for reducing the dimensionality of data.",
            "A measure of the distance between data points."
          ],
          "correctAnswer": 1,
          "explanation": "A Voronoi partition divides the space such that each cell contains the region closest to a particular cluster center (zk).",
          "source": {
            "chunkId": 38,
            "fileName": "Clustering.pdf",
            "chunkText": "Clustering\n\nMIT 15.097 Course Notes Cynthia Rudin and S¸eyda Ertekin\n\nCredit: Dasgupta, Hastie, Tibshirani, Friedman\n\nClustering (a.k.a. data segmentation) Let’s segment a collection of examples into “clusters” so that objects within a cluster are more closely related to one another than objects assigned to different clusters. We want to assign each ex­ ample xi to a cluster k ∈{1, ...., K}.\n\nThe K-Means algorithm is a very popular way to do this. It assumes points lie\nin Euclidean space.\n\nn Input: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, ..., zK ) :=\nmin Ixi − zkI2\n\n2. k i The choice of the squared norm is fortuitous, it really helps simplify the math!\n\nn If we’re given points {zk}k, they can induce a Voronoi p artition of R : they break the space into cells where each cell corresponds to one of the zk’s. That is, each cell contains the region of space whose nearest representative is zk.\n\nDraw a picture\n\nWe can look at the examples in each of these regions of space, which are the clusters. Specifically,\n\nCk := {xi : the closest representative to xi is zk}.\n\nLet’s compute the cost another way. Before, we summed over examples, and then picked the right representative zk for each example. This time, we’ll sum over clusters, and look at all the examples in that cluster:\n\ncost(z1, ..., zK ) =\nIxi − zkI2\n\n2.\n\nk {i:xi∈Ck}\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 259,
          "question": "According to Lemma 1, where should the representative 'z' be placed to minimize the cost for a single cluster C?",
          "options": [
            "At the median of the examples in C.",
            "At the mode of the examples in C.",
            "At the mean of the examples in C.",
            "At a random point within C."
          ],
          "correctAnswer": 2,
          "explanation": "Lemma 1 suggests placing 'z' at the mean of the examples in C to minimize the cost. This is because the cost function is minimized when z = mean(C).",
          "source": {
            "chunkId": 39,
            "fileName": "Clustering.pdf",
            "chunkText": "While we’re analyzing, we’ll need to consider suboptimal partitions of the data, where an example might not be assigned to the nearest representative. So we redefine the cost:\n\nX X\n\nk ∥xi −zk∥2 2. (1)\n\ncost(C1, ..., CK; z1, ..., zK) =\n\n{i:xi∈Ck}\n\nLet’s say we only have one cluster to deal with. Call it C. The representative is z. The cost is then:\ncost(C; z) =\n∥xi −z∥2 2.\n\nX\n\n{ ∈} Where should we place z?\n\ni:xi C\n\nAs you probably guessed, we would put it at the mean of the examples in C. But\nalso, the additional cost incurred by picking z = mean(C) can be characterized\nvery simply:\n\n̸\n\nLemma 1. For any set C ⊂Rn and any z ∈Rn,\n\ncost(C; z) = cost(C, mean(C)) + C\nz mean(C) 2 2. | | · ∥− ∥\n\nLet’s go ahead and prove it. In order to do that, we need to do another biasvariance decomposition (this one’s pretty much identical to one of the ones we did before).\n\nLemma 2. Let X Rn be any random variable. For any z Rn, we have: ∈ ∈\n\nEX∥X −z∥2\n2 = EX∥X −EXX∥2\n2 + ∥z −EXX∥2\n2.\n\nProof. Let x¯ := EXX.\n\nj −z(j))2 X\n\nEX∥X −z∥2\n2 = EX\n(X(j)\n\nj\n−x¯(j) + x¯(j) −zj)2\nX\n\n= EX\n(X(j)\n\nj\n−x¯(j))2 + EX\n(x¯(j)\n\nj −zj)2 X X\n\n= EX\n(X(j)\n\nj −x¯(j))(x¯(j) −z(j)) X\n\n+2EX\n(X(j)\n\n= EX∥X −x¯∥2\n2 + EX∥x¯ −z∥2\n2 + 0. ■\n\n2"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 260,
          "question": "What does the Davies-Baldwin Index measure?",
          "options": [
            "Minimal intercluster distance and maximal intracluster distance.",
            "Average intracluster distance and intercluster distances between centroids.",
            "The number of data points in each cluster.",
            "The overall cost of the clustering."
          ],
          "correctAnswer": 1,
          "explanation": "The Davies-Baldwin Index considers the average intracluster distance (wanting it small) and intercluster distances between centroids (wanting it large) to evaluate the quality of clustering.",
          "source": {
            "chunkId": 42,
            "fileName": "Clustering.pdf",
            "chunkText": "Example of how K-Means could converge to the wrong thing\n\nHow might you make K-Means more likely to converge to the optimal?\n\nHow might you choose K? (Why can’t you measure test error?)\n\nOther ways to evaluate clusters (“cluster validation”)\n\nThere are loads of cluster validity measures, alternatives to the cost. Draw a picture\n\n• Davies-Baldwin Index - looks at average intracluster distance (within-cluster\ndistance) to the centroid (want it to be small), and intercluster distances between centroids (want it to be large).\n\n• Dunn Index - looks pairwise at minimal intercluster distance (want it to be\nlarge) and maximal intracluster distance (want it to be small).\n\nExample: Microarray data. Have 6830 genes (rows) and 64 patients (columns). The color of each box is a measurement of the expression level of a gene. The expression level of a gene is basically how much of its special protein it is pro­ ducing. The physical chip itself doesn’t actually measure protein levels, but a proxy for them (which is RNA, which sticks to the DNA on the chip). If the color is green, it means low expression levels, if the color is red, it means higher expression levels. Each patient is represented by a vector, which is the expression level of their genes. It’s a column vector with values given in color:\n\n5"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 261,
          "question": "What is a characteristic of Hierarchical Clustering that addresses a major issue with K-means?",
          "options": [
            "It always converges to the optimal solution.",
            "It is less computationally expensive than K-means.",
            "Cluster membership changes predictably as the number of clusters changes.",
            "It can handle non-Euclidean data."
          ],
          "correctAnswer": 2,
          "explanation": "Hierarchical clustering creates a hierarchy of clusters, so cluster membership changes in a structured way as the number of clusters changes, unlike K-means where changes can be arbitrary.",
          "source": {
            "chunkId": 44,
            "fileName": "Clustering.pdf",
            "chunkText": "It’s pretty good at keeping the same cancers in the same cluster. The two breast cancers in the 2nd cluster were actually melanomas that metastasized.\n\nGenerally we cluster genes, not patients. Would really like to get something like this in practice:\n\nCourtesy of the Rockefeller University Press. Used with permission.\n\nFigure 7 from Rumfelt, Lynn, et al. \"Lineage Specification and Plasticity in CD19- Early B\ncell Precursors.\" Journal of Experimental Medicine 203 (2006): 675-87.\n\nwhere each row is a gene, and the columns are different immune cell types.\n\nA major issue with K-means: as K changes, cluster membership can change\narbitrarily. A solution is Hierarchical Clustering.\n\n• clusters at the next level of the hierarchy are created by merging clusters at the next lowest level.\n\n– lowest level: each cluster has 1 example\n\n– highest level: there’s only 1 cluster, containing all of the data.\n\n7\n\n[IMAGE_OCR 1] MLP CLP Fr.A Fr. B&C A uma B pw EBFI c D ot"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 262,
          "question": "In K-NN, what is used to determine a test example's label?",
          "options": [
            "The average distance to all training points.",
            "The majority vote of the K-Nearest Neighbors.",
            "A predefined decision boundary.",
            "A random selection process."
          ],
          "correctAnswer": 1,
          "explanation": "K-NN classifies a test example based on the majority vote of its K nearest neighbors in the training data.",
          "source": {
            "chunkId": 47,
            "fileName": "k-nearest neighbpurs.pdf",
            "chunkText": "K-NN\n\n15.097 MIT, Spring 2012, Cynthia Rudin Credit: Seyda Ertekin\n\nK-Nearest Neighbors\n\n• Amongst the simplest of all machine learning algorithms. No eXplicit training or model. • Can be used both for classifcaton and regression.\n• Use XIs K-Nearest Neighbors to vote on what\nXIs label should be.\n\n1"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 263,
          "question": "What happens if the value of K is too small in K-NN?",
          "options": [
            "The algorithm will model the noise in the data.",
            "Neighbors will include too many points from other classes.",
            "The algorithm will be computationally inefficient.",
            "The decision boundaries will be overly smooth."
          ],
          "correctAnswer": 0,
          "explanation": "If K is too small, the K-NN algorithm becomes sensitive to noise in the data, leading to overfitting.",
          "source": {
            "chunkId": 49,
            "fileName": "k-nearest neighbpurs.pdf",
            "chunkText": "K-Nearest Neighbors\n\n• For regression: the value for the test eXample becomes the (weighted) average of the values of the K neighbors.\n\nMaking K-NN More Powerful\n\n• A good value for K can be determined by considering a range of K values.\n\n– K too small: we’ll model the noise\n\n– K too large: neighbors include too many points from other classes\n\n• There are problems when there is a spread of distances among the K­\nNN. Use a distance-based voting scheme, where closer neighbors have\nmore influence.\n\n• The distance measure has to be meaningful – attributes should be scaled\n\n– Eg. Income varies 10,000-1,000,000 while height varies 1.5-1.8 meters\n\n3"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 264,
          "question": "What is a distance-based voting scheme in K-NN designed to address?",
          "options": [
            "High dimensionality of the data.",
            "A spread of distances among the K-NN.",
            "Computational complexity of the algorithm.",
            "The presence of outliers in the data."
          ],
          "correctAnswer": 1,
          "explanation": "Distance-based voting addresses the issue where there's a significant variation in distances among the K nearest neighbors, giving more influence to closer neighbors.",
          "source": {
            "chunkId": 49,
            "fileName": "k-nearest neighbpurs.pdf",
            "chunkText": "K-Nearest Neighbors\n\n• For regression: the value for the test eXample becomes the (weighted) average of the values of the K neighbors.\n\nMaking K-NN More Powerful\n\n• A good value for K can be determined by considering a range of K values.\n\n– K too small: we’ll model the noise\n\n– K too large: neighbors include too many points from other classes\n\n• There are problems when there is a spread of distances among the K­\nNN. Use a distance-based voting scheme, where closer neighbors have\nmore influence.\n\n• The distance measure has to be meaningful – attributes should be scaled\n\n– Eg. Income varies 10,000-1,000,000 while height varies 1.5-1.8 meters\n\n3"
          },
          "grade": null,
          "feedback": null
        },
        {
          "id": 265,
          "question": "For K-NN regression, how is the value for the test example determined?",
          "options": [
            "By randomly selecting a value from the K neighbors.",
            "By using the value of the closest neighbor.",
            "By calculating the (weighted) average of the values of the K neighbors.",
            "By assigning the median value of the K neighbors."
          ],
          "correctAnswer": 2,
          "explanation": "In K-NN regression, the predicted value for a test example is the (weighted) average of the values of its K nearest neighbors.",
          "source": {
            "chunkId": 49,
            "fileName": "k-nearest neighbpurs.pdf",
            "chunkText": "K-Nearest Neighbors\n\n• For regression: the value for the test eXample becomes the (weighted) average of the values of the K neighbors.\n\nMaking K-NN More Powerful\n\n• A good value for K can be determined by considering a range of K values.\n\n– K too small: we’ll model the noise\n\n– K too large: neighbors include too many points from other classes\n\n• There are problems when there is a spread of distances among the K­\nNN. Use a distance-based voting scheme, where closer neighbors have\nmore influence.\n\n• The distance measure has to be meaningful – attributes should be scaled\n\n– Eg. Income varies 10,000-1,000,000 while height varies 1.5-1.8 meters\n\n3"
          },
          "grade": null,
          "feedback": null
        }
      ]
    },
    "num_questions_requested": 10,
    "num_questions_generated": 10,
    "latency": 25.35,
    "error": null
  }
]