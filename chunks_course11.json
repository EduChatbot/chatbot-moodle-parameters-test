{
  "1": "Rule Mining and the Apriori Algorithm\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nThe Apriori algorithm - often called the “first thing data miners try,” but somehow doesn’t appear in most data mining textbooks or courses!\n\nStart with market basket data:\n\nSome important definitions:\n\n• Itemset: a subset of items, e.g., (bananas, cherries, elderberries), indexed by {2, 3, 5}.\n\n• Support of an itemset: number of transactions containing it,\n\nm\nSupp(bananas, cherries, elderberries) =\nX Mi,2 · Mi,3\ni=1\n· Mi,5.\n\n• Confidence of rule a →b: the fraction of times itemset b is purchased when itemset a is purchased.\n\nSupp(a ∪b) #times a and b are purchased\nConf(a →b) =\n=\nSupp(a) #times a is purchased ˆ\n= P(b|a).\n\n1",
  "2": "We want to find all strong rules. These are rules a →b such that:\n\nSupp(a ∪b) ≥θ, and Conf(a →b) ≥minconf.\n\nHere θ is called the minimum support threshold.\n\nThe support has a monotonicity property called downward closure:\n\nIf Supp(a ∪b) ≥θ then Supp(a) ≥θ and Supp(b) ≥θ.\n\nThat is, if a ∪b is a frequent item set, then so are a and b.\n\nSupp(a ∪b) = #times a and b are purchased\n≤#times a is purchased = Supp(a).\n\nApriori finds all frequent itemsets (a such that Supp(a) ≥θ).\nWe can use Apriori’s result to get all strong rules a →b as follows:\n\n• For each frequent itemset ℓ:\n\n– Find all nonempty subsets of ℓ\n\n– For each subset a, output a →{ℓ\\ a} whenever\n\nSupp(ℓ) Supp(a) ≥minconf.\n\nNow for Apriori. Use the downward closure property: generate all k-itemsets\n(itemsets of size k) from (k −1)-itemsets. It’s a breadth-first-search.\n\n2",
  "3": "Example:\nθ = 10\n\nerries\n\nbananas\n\ncherries\n\nes grap\n\napples\n\nelderb\n\n1-itemsets:\na b c d e\n/f\ng supp: 25 20 30 45 29 5 17\n\n2-itemsets:\n\n{ } {\n\na,b a,c} { } {a,e}\n\na,d ... { e,g} supp: 7 25 15 23 3\n\n3-itemsets: {a,c,d}\n{a,c,e} {b,d,g} ... supp: 15 22 15\n\n4-itemsets: {a,c,d,e}\nsupp: 12\n\nApriori Algorithm:\n\nInput: Matrix M\n\nL1={frequent 1-itemsets; i such that Supp(i) ≥θ}.\n\nFor k = 2, while Lk−1 = ∅(while there are large k −1-itemsets), k + +\n\n̸\n\n• Ck = apriori gen(Lk\n1) generate candidate itemsets of size k −\n• Lk = {c : c ∈Ck, Supp(c) ≥θ} frequent itemsets of size k (loop over\ntransactions, scan the database)\n\nend\n\nS\n\nOutput: k Lk.\n\n3",
  "4": "The subroutine apriori gen joins Lk−1 to Lk−1.\n\napriori gen Subroutine:\n\nInput: Lk−1\n\nFind all pairs of itemsets in Lk−1 where the first k −2 items are identical.\n\ntoo big Union them (lexicographically) to get Ck ,\n\ne.g.,{a, b, c, d, e, f}, {a, b, c, d, e, g} →{a, b, c, d, e, f, g}\n\nPrune: Ck = {\ntoo big c ∈Ck\n, all (k −1)-subsets cs of c obey cs ∈Lk−1}.\n\nOutput: Ck.\n\nExample of Prune step: consider {a, b, c, d, e, f, g} too big which is in Ck , and I want to know whether it’s in Ck. Look at {a, b, c, d, e, f, g}, {a, b, c, d, e, f, g},{a, b, c, d, e, f, g}, {a, b, c, d, e, f, g}, etc. If any are not in L6, then prune {a, b, c, d, e, f, g} from L7.\n\nExample of the aprior\n\ni a lgori\n\nthm.\n\n4",
  "5": "• Apriori scans the database at most how many times?\n\n• Huge number of candidate sets.\n\n/\n\n•\nwned huge number of apriori-like papers.\n\nSpa\n\nWhat do you do with the rules after they’re generated?\n\n• Information overload (give up)\n\n• Order rules by “interestingness”\n\n– Confidence Supp(a\nˆP(b|a) =\n∪b) Supp(a)\n\n– “Lift”/“Interest”\nˆP(b|a) Supp(b)\n=\nˆP(b) −Supp(a b 1 ∪) Supp(a) :\n\n– Hundreds!\n\nResearch questions:\n\n• mining more than just itemsets (e.g, sequences, trees, graphs)\n\n• incorporating taxonomy in items\n\n• boolean logic and “logical analysis of data”\n\n• Cynthia’s questions: Can we use rules within ML to get good predictive models?\n\n5",
  "6": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "7": "R for Machine Learning\n\nAllison Chang\n\n1 Introduction\n\nIt is common for today’s scientific and business industries to collect large amounts of data, and the ability to analyze the data and learn from it is critical to making informed decisions. Familiarity with software such as R allows users to visualize data, run statistical tests, and apply machine learning algorithms. Even if you already know other software, there are still good reasons to learn R:\n\n1. R is free. If your future employer does not already have R installed, you can always download it for free, unlike other proprietary software packages that require expensive licenses. No matter where you travel, you can have access to R on your computer.\n\n2. R gives you access to cutting-edge technology. Top researchers develop statistical learning methods\nin R, and new algorithms are constantly added to the list of packages you can download.\n\n3. R is a useful skill. Employers that value analytics recognize R as useful and important. If for no other reason, learning R is worthwhile to help boost your r´esum´e.\n\nNote that R is a programming language, and there is no intuitive graphical user interface with buttons you can click to run different methods. However, with some practice, this kind of environment makes it easy to quickly code scripts and functions for various statistical purposes. To get the most out of this tutorial, follow the examples\nby typing them out in R on your own computer. A line that begins with > is input at the command prompt. We\ndo not include the output in most cases, but you should try out the commands yourself and see what happens. If you type something at the command line and decide not to execute, press the down arrow to clear the line; pressing the up arrow gives you the previous executed command.\n\n1.1 Getting Started\n\nThe R Project website is http://www.r-project.org/. In the menu on the left, click on CRAN under “Download,\nPackages.” Choose a location close to you. At MIT, you can go w",
  "8": "nds yourself and see what happens. If you type something at the command line and decide not to execute, press the down arrow to clear the line; pressing the up arrow gives you the previous executed command.\n\n1.1 Getting Started\n\nThe R Project website is http://www.r-project.org/. In the menu on the left, click on CRAN under “Download,\nPackages.” Choose a location close to you. At MIT, you can go with University of Toronto under Canada. This leads you to instructions on how to download R for Linux, Mac, or Windows.\n\nOnce you open R, to figure out your current directory, type getwd(). To change directory, use setwd (note that the “C:” notation is for Windows and would be different on a Mac):\n\n> setwd(\"C:\\\\Datasets\")\n\n1.2 Installing and loading packages\n\nFunctions in R are grouped into packages, a number of which are automatically loaded when you start R. These include “base,” “utils,” “graphics,” and “stats.” Many of the most essential and frequently used functions come in these packages. However, you may need to download additional packages to obtain other useful functions. For example, an important classification method called Support Vector Machines is contained in a package called\n\n1",
  "9": "“e1071.” To install this package, click “Packages” in the top menu, then “Install package(s)...” When asked to select a CRAN mirror, choose a location close to you, such as “Canada (ON).” Finally select “e1071.” To load the package, type library(e1071) at the command prompt. Note that you need to install a package only once, but that if you want to use it, you need to load it each time you start R.\n\n1.3 Running code\n\nYou could use R by simply typing everything at the command prompt, but this does not easily allow you to save, repeat, or share your code. Instead, go to “File” in the top menu and click on “New script.” This opens up a new window that you can save as a .R file. To execute the code you type into this window, highlight the lines you\nwish to run, and press Ctrl-R on a PC or Command-Enter on a Mac. If you want to run an entire script, make\nsure the script window is on top of all others, go to “Edit,” and click “Run all.” Any lines that are run appear in red at the command prompt.\n\n1.4 Help in R\n\nThe functions in R are generally well-documented. To find documentation for a particular function, type ?\nfollowed directly by the function name at the command prompt. For example, if you need help on the “sum” function, type ?",
  "10": " script, make\nsure the script window is on top of all others, go to “Edit,” and click “Run all.” Any lines that are run appear in red at the command prompt.\n\n1.4 Help in R\n\nThe functions in R are generally well-documented. To find documentation for a particular function, type ?\nfollowed directly by the function name at the command prompt. For example, if you need help on the “sum” function, type ?sum. The help window that pops up typically contains details on both the input and output for the function of interest. If you are getting errors or unexpected output, it is likely that your input is insufficient or invalid, so use the documentation to figure out the proper way to call the function.\n\nIf you want to run a certain algorithm but do not know the name of the function in R, doing a Google search of R plus the algorithm name usually brings up information on which function to use.\n\n2 Datasets\n\nWhen you test any machine learning algorithm, you should use a variety of datasets. R conveniently comes with its own datasets, and you can view a list of their names by typing data() at the command prompt. For instance, you may see a dataset called “cars.” Load the data by typing data(cars), and view the data by typing cars.\n\nAnother useful source of available data is the UCI Machine Learning Repository, which contains a couple hundred datasets, mostly from a variety of real applications in science and business. The repository is located at\nhttp://archive.ics.uci.edu/ml/datasets.html. These data are often used by machine learning researchers\nto develop and compare algorithms. We have downloaded a number of datasets for your use, and you can find the text files\n\n.",
  "11": "e UCI Machine Learning Repository, which contains a couple hundred datasets, mostly from a variety of real applications in science and business. The repository is located at\nhttp://archive.ics.uci.edu/ml/datasets.html. These data are often used by machine learning researchers\nto develop and compare algorithms. We have downloaded a number of datasets for your use, and you can find the text files\n\n. These\n\ninclude:\n\nin the Datasets section\n\nName Rows Cols Data Iris 150 4 Real Wine 178 13 Integer, Real Haberman’s Survival 306 3 Integer Housing 506 14 Categorical, Integer, Real Blood Transfusion Service Center 748 4 Integer Car Evaluation 1728 6 Categorical Mushroom 8124 119 Binary\nPen-based Recognition of Handwritten Digits\n10992 16 Integer\n\n2",
  "12": "You are encouraged to download your own datasets from the UCI site or other sources, and to use R to study the data. Note that for all except the Housing and Mushroom datasets, there is an additional class attribute column that is not included in the column counts. Also note that if you download the Mushroom dataset from the UCI site, it has 22 categorical features; in our version, these have been transformed into 119 binary features.\n\n3 Basic Functions\n\nIn this section, we cover how to create data tables, and analyze and plot data. We demonstrate by example how to use various functions. To see the value(s) of any variable, vector, or matrix at any time, simply enter its name in the command line; you are encouraged to do this often until you feel comfortable with how each data structure\nis being stored. To see all the objects in your workspace, type ls(). Also note that the arrow operator <- sets\nthe left-hand side equal to the right-hand side, and that a comment begins with #.\n\n3.1 Creating data\n\nTo create a variable x and set it equal to 1, type x <- 1. Now suppose we want to generate the vector [1, 2, 3, 4, 5],\nand call the vector v. There are a couple different ways to accomplish this:\n\n> v <- 1:5\n> v <- c(1,2,3,4,5)\n# c can be used to concatenate multiple vectors\n> v <- seq(from=1,to=5,by=1)\n\nThese can be row vectors or column vectors. To generate a vector v0 of six zeros, use either of the following. Clearly the second choice is better if you are generating a long vector.\n\n> v0 <- c(0,0,0,0,0,0)\n> v0 <- seq(from=0,to=0,length.out=6)\n\nWe can combine vectors into matrices using cbind and rbind. For instance, if v1, v2, v3, and v4 are vectors of the same length, we can combine them into matrices, using them either as columns or as rows:\n\n> v1 <- c(1,2,3,4,5)\n> v2 <- c(6,7,8,9,10)\n> v3 <- c(11,12,13,14,15)\n> v4 <- c(16,17,18,19,20)\n> cbind(v1,v2,v3,v4)\n> rbind(v1,v2,v3,v4)\n\nAnother way to create the second matrix is to use the matrix function to reshape a vector i",
  "13": "es using cbind and rbind. For instance, if v1, v2, v3, and v4 are vectors of the same length, we can combine them into matrices, using them either as columns or as rows:\n\n> v1 <- c(1,2,3,4,5)\n> v2 <- c(6,7,8,9,10)\n> v3 <- c(11,12,13,14,15)\n> v4 <- c(16,17,18,19,20)\n> cbind(v1,v2,v3,v4)\n> rbind(v1,v2,v3,v4)\n\nAnother way to create the second matrix is to use the matrix function to reshape a vector into a matrix of the right dimensions.\n\n> v <- seq(from=1,to=20,by=1)\n> matrix(v, nrow=4, ncol=5)\n\nNotice that this is not exactly right—we need to specify that we want to fill in the matrix by row.\n\n> matrix(v, nrow=4, ncol=5, byrow=TRUE)\n\nIt is often helpful to name the columns and rows of a matrix using colnames and rownames. In the following, first we save the matrix as matrix20, and then we name the columns and rows.\n\n3",
  "14": "> matrix20 <- matrix(v, nrow=4, ncol=5, byrow=TRUE)\n> colnames(matrix20) <- c(\"Col1\",\"Col2\",\"Col3\",\"Col4\",\"Col5\")\n> rownames(matrix20) <- c(\"Row1\",\"Row2\",\"Row3\",\"Row4\")\n\nYou can type colnames(matrix20)/rownames(matrix20) at any point to see the column/row names for matrix20.\nTo access a particular element in a vector or matrix, index it by number or by name with square braces:\n\n> v[3]\n# third element of v\n> matrix20[,\"Col2\"]\n# second column of matrix20\n> matrix20[\"Row4\",]\n# fourth row of matrix20\n> matrix20[\"Row3\",\"Col1\"]\n# element in third row and first column of matrix20\n> matrix20[3,1]\n# element in third row and first column of matrix20\n\nYou can find the length of a vector or number of rows or columns in a matrix using length, nrow, and ncol.\n\n> length(v1)\n> nrow(matrix20)\n> ncol(matrix20)\n\nSince you will be working with external datasets, you will need functions to read in data tables from text files. For instance, suppose you wanted to read in the Haberman’s Survival dataset (from the UCI Repository). Use the read.table function:\n\ndataset <- read.table(\"C:\\\\Datasets\\\\haberman.csv\", header=FALSE, sep=\",\")\n\nThe first argument is the location (full path) of the file. If the first row of data contains column names, then the\nsecond argument should be header = TRUE, and otherwise it is header = FALSE. The third argument contains\nthe delimiter. If the data are separated by spaces or tabs, then the argument is sep = \" \" and sep = \"\\t\"\nrespectively. The default delimiter (if you do not include this argument at all) is “white space” (one or more spaces, tabs, etc.). Alternatively, you can use setwd to change directory and use only the file name in the read.table function. If the delimiter is a comma, you can also use read.csv and leave off the sep argument:\n\ndataset <- read.csv(\"C:\\\\Datasets\\\\haberman.csv\", header=FALSE)\n\nUse write.table to write a table to a file. Type ?",
  "21": "ect(mushroom_rules)\n\nYou can modify the parameter settings depending on your desired support and confidence thresholds.\n\n4.3 Logistic Regression\n\nYou do not need to install an extra package for logistic regression. Using the same notation as in Section 3.6, the command is:\n\n> glm_mod <-glm(y ∼ x1+x2, family=binomial(link=\"logit\"), data=as.data.frame(cbind(y,x1,x2)))\n\n4.4\nK-Means Clustering\n\nYou do not need an extra package. If X is the data matrix and m is the number of clusters, then the command is:\n\n> kmeans_model <- kmeans(x=X, centers=m)\n\n4.5\nk-Nearest Neighbor Classification\n\nInstall and load the class package. Let X train and X test be matrices of the training and test data respectively, and labels be a binary vector of class attributes for the training examples. For k equal to K, the command is:\n\n> knn_model <- knn(train=X_train, test=X_test, cl=as.factor(labels), k=K)\n\nThen knn model is a factor vector of class attributes for the test set.\n\n7",
  "15": " do not include this argument at all) is “white space” (one or more spaces, tabs, etc.). Alternatively, you can use setwd to change directory and use only the file name in the read.table function. If the delimiter is a comma, you can also use read.csv and leave off the sep argument:\n\ndataset <- read.csv(\"C:\\\\Datasets\\\\haberman.csv\", header=FALSE)\n\nUse write.table to write a table to a file. Type ?write.table to see details about this function. If you need to write text to a file, use the cat function.\n\nA note about matrices versus data frames: A data frame is similar to a matrix, except it can also include\nnon-numeric attributes. For example, there may be a column of characters. Some functions require the data\npassed in to be in the form of a data frame, which would be stated in the documentation. You can always coerce a matrix into a data frame using as.data.frame. See Section 3.6 for an example.\n\nA note about factors: A factor is essentially a vector of categorical variables, encoded using integers. For instance, if each example in our dataset has a binary class attribute, say 0 or 1, then that attribute can be represented as a factor. Certain functions require one of their arguments to be a factor. Use as.factor to encode a vector as a factor. See Sections 4.5 and 4.9 for examples.\n\n3.2 Sampling from probability distributions\n\nThere are a number of functions for sampling from probability distributions. For example, the following commands\ngenerate random vectors of the user-specified length n from distributions (normal, exponential, poisson, uniform,\nbinomial) with user-specified parameters. There are other distributions as well.\n\n4",
  "16": "> norm_vec <- rnorm(n=10, mean=5, sd=2)\n> exp_vec <- rexp(n=100, rate=3)\n> pois_vec <- rpois(n=50, lambda=6)\n> unif_vec <- runif(n=20, min=1, max=9)\n> bin_vec <- rbinom(n=20, size=1000, prob=0.7)\n\nSuppose you have a vector v of numbers. To randomly sample, say, 25 of the numbers, use the sample function:\n\n> sample(v, size=25, replace=FALSE)\n\nIf you want to sample with replacement, set the replace argument to TRUE. If you want to generate the same random vector each time you call one of the random functions listed above, pick a “seed” for the random number generator using set.seed, for example set.seed(100).\n\n3.3 Analyzing data\n\nTo compute the mean, variance, standard deviation, minimum, maximum, and sum of a set of numbers, use mean, var, sd, min, max, and sum. There are also rowSum and colSum to find the row and column sums for a matrix.\nTo find the component-wise absolute value and square root of a set of numbers, use abs and sqrt. Correlation\nand covariance for two vectors are computed with cor and cov respectively.\n\nLike other programming languages, you can write if statements, and for and while loops. For instance, here is a simple loop that prints out even numbers between 1 and 10 (%% is the modulo operation):\n\n> for (i in 1:10){\n+ if (i %% 2 == 0){\n+\ncat(paste(i, \"is even.\\n\", sep=\" \"))\n# use paste to concatenate strings\n+\n}\n+ }\n\nThe 1:10 part of the for loop can be specified as a vector. For instance, if you wanted to loop over indices 1, 2, 3, 5, 6, and 7, you could type for (i in c(1:3,5:7)). To pick out the indices of elements in a vector that satisfy a certain property, use which, for example:\n\n> which(v >= 0)\n# indices of nonnegative elements of v\n> v[which(v >= 0)]\n# nonnegative elements of v\n\n3.4 Plotting data\n\nWe use the Haberman’s Survival data (read into data frame dataset) to demonstrate plotting functions. Each row of data represents a patient who had surgery for breast cancer. The three features are: the age of the patient at the time of surgery",
  "17": "roperty, use which, for example:\n\n> which(v >= 0)\n# indices of nonnegative elements of v\n> v[which(v >= 0)]\n# nonnegative elements of v\n\n3.4 Plotting data\n\nWe use the Haberman’s Survival data (read into data frame dataset) to demonstrate plotting functions. Each row of data represents a patient who had surgery for breast cancer. The three features are: the age of the patient at the time of surgery, the year of the surgery, and the number of positive axillary nodes detected. Here we plot:\n\n1. Scatterplot of the first and third features,\n\n2. Histogram of the second feature,\n\n3. Boxplot of the first feature.\n\nTo put all three plots in a 1 × 3 matrix, use par(mfrow=c(1,3)). To put each plot in its own window, use\nwin.graph() to create new windows.\n\n> plot(dataset[,1], dataset[,3], main=\"Scatterplot\", xlab=\"Age\", ylab=\"Number of Nodes\", pch=20)\n> hist(dataset[,2], main=\"Histogram\", xlab=\"Year\", ylab=\"Count\")\n> boxplot(dataset[,1], main=\"Boxplot\", xlab=\"Age\")\n\n5",
  "18": "Scatterplot Histogram Boxplot\n\n0 10 20 30 40 50\n\n0 10 20 30 40 50 60\n\n30 40 50 60 70 80\n\nNumber of Nodes\n\nCount\n\n30 40 50 60 70 80 58 60 62 64 66 68\n\nAge Year Age\n\nFigure 1: Plotting examples.\n\nThe pch argument in the plot function can be varied to change the marker. Use points and lines to add extra points and lines to an existing plot. You can save the plots in a number of different formats; make sure the plot window is on top, and go to “File” then “Save as.”\n\n3.5 Formulas\n\nCertain functions have a “formula” as one of their arguments. Usually this is a way to express the form of a model. Here is a simple example. Suppose you have a response variable y and independent variables x1, x2, and\nx3. To express that y depends linearly on x1, x2, and x3, you would use the formula y ∼ x1 + x2 + x3, where\ny, x1, x2, and x3 are also column names in your data matrix. See Section 3.6 for an example. Type ?formula for details on how to capture nonlinear models.\n\n3.6 Linear regression\n\nOne of the most common modeling approaches in statistical learning is linear regression. In R, use the lm function to generate these models. The general form of a linear regression model is\n\nY = β0 + β1X1 + β2X2 + · · · + βkXk + ε,\n\nwhere ε is normally distributed with mean 0 and some variance σ2 .\n\nLet y be a vector of dependent variables, and x1 and x2 be vectors of independent variables. We want to find\nthe coefficients of the linear regression model Y = β0 + β1X1 + β2X2 + ε. The following commands generate the\nlinear regression model and give a summary of it.\n\n> lm_model <- lm(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)))\n> summary(lm_model)\n\nThe vector of coefficients for the model is contained in lm model$coefficients.\n\n4 Machine Learning Algorithms\n\nWe give the functions corresponding to the algorithms covered in class. Look over the documentation for each function on your own as only the most basic details are given in this tutorial.\n\n6",
  "19": "4.1 Prediction\n\nFor most of the following algorithms (as well as linear regression), we would in practice first generate the model using training data, and then predict values for test data. To make predictions, we use the predict function. To see documentation, go to the help page for the algorithm, and then scroll through the Contents menu on the left side to find the corresponding predict function. Or simply type ?",
  "20": "ost of the following algorithms (as well as linear regression), we would in practice first generate the model using training data, and then predict values for test data. To make predictions, we use the predict function. To see documentation, go to the help page for the algorithm, and then scroll through the Contents menu on the left side to find the corresponding predict function. Or simply type ?predict.name, where name is the function corresponding to the algorithm. Typically, the first argument is the variable in which you saved the model, and the second argument is a matrix or data frame of test data. Note that when you call the function, you can just type predict instead of predict.name. For instance, if we were to predict for the linear regression model above, and x1 test and x2 test are vectors containing test data, we can use the command\n\n> predicted_values <- predict(lm_model, newdata=as.data.frame(cbind(x1_test, x2_test)))\n\n4.2 Apriori\n\nTo run the Apriori algorithm, first install the arules package and load it. See Section 1.2 for installing and loading new packages. Here is an example of how to run the Apriori algorithm using the Mushroom dataset. Note that the dataset must be a binary incidence matrix; the column names should correspond to the “items” that make up the “transactions.” The following commands print out a summary of the results and a list of the generated rules.\n\n> dataset <- read.csv(\"C:\\\\Datasets\\\\mushroom.csv\", header = TRUE)\n> mushroom_rules <- apriori(as.matrix(dataset), parameter = list(supp = 0.8, conf = 0.9))\n> summary(mushroom_rules)\n> inspect(mushroom_rules)\n\nYou can modify the parameter settings depending on your desired support and confidence thresholds.\n\n4.3 Logistic Regression\n\nYou do not need to install an extra package for logistic regression. Using the same notation as in Section 3.6, the command is:\n\n> glm_mod <-glm(y ∼ x1+x2, family=binomial(link=\"logit\"), data=as.data.frame(cbind(y,x1,x2)))\n\n4.4\nK-Means Clustering\n\nYou do",
  "22": "4.6 Na¨ıve Bayes\n\nInstall and load the e1071 package. Using the same notation as in Section 3.6, the command is:\n\n> nB_model <- naiveBayes(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)))\n\n4.7 Decision Trees (CART)\n\nCART is implemented in the rpart package. Again using the formula, the command is\n\n> cart_model <- rpart(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)), method=\"class\")\n\nYou can use plot.rpart and text.rpart to plot the decision tree.\n\n4.8 AdaBoost\n\nThere are a number of different boosting functions in R. We show here one implementation that uses decision trees as base classifiers. Thus the rpart package should be loaded. Also, the boosting function ada is in the ada\npackage. Let X be the matrix of features, and labels be a vector of 0-1 class labels. The command is\n\n> boost_model <- ada(x=X, y=labels)\n\n4.9 Support Vector Machines (SVM)\n\nThe SVM algorithm is in the e1071 package. Let X be the matrix of features, and labels be a vector of 0-1 class\nlabels. Let the regularization parameter be C. Use the following commands to generate the SVM model and view details:\n\n> svm_model <- svm(x=X, y=as.factor(labels), kernel =\"radial\", cost=C)\n> summary(svm_model)\n\nThere are a variety of parameters such as the kernel type and value of the C parameter, so check the documentation on how to alter them.\n\n8",
  "23": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "24": "Fundamentals of Learning\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nImportant Problems in Data Mining\n\n1. Finding patterns (correlations) in large datasets\n-e.g. (Diapers → Beer). Use Apriori!\n\n2. Clustering - grouping data into clusters that “belong” together - objects\nwithin a cluster are more similar to each other than to those in other clusters.\n\n• Kmeans, Kmedians\n\ni=1, xi ∈X ⊂ Rn\n\n• Input: {xi}m\n\n• Output: f : X →{1, . . . , K} (K clusters)\n\n• clustering consumers for market research, clustering genes into families, image segmentation (medical imaging)\n\n3. Classification\n\n• Input: {(xi, yi)}m “examples,” “instances with labels,” “observations”\ni=1\n• xi ∈X , yi ∈ {−1, 1} “binary”\n\n• Output: f : X → R and use sign(f) to classify.\n\n• automatic handwriting recognition, speech recognition, biometrics, doc­ ument classification\n\n• “LeNet”\n\n4. Regression\n\n1\n\n[IMAGE_OCR 1]\n3 +\n\n+ + o\n\nre -\n. SE",
  "25": "• Input: {(xi, yi)}m\n\ni=1, xi ∈X , yi ∈ R\n\n• Output: f : X → R\n\n• predicting an individual’s income, predict house prices, predict stock prices, predict test scores\n\n5. Ranking (later) - in between classification and regression. Search engines\nuse ranking methods\n\n6. Density Estimation - predict conditional probabilities\n\ni=1, xi ∈X , yi ∈ {−1, 1}\n\n• {(xi, yi)}m\n\n• Output: f : X → [0, 1] as “close” to P (y = 1|x) as possible.\n\n• estimate probability of failure, probability to default on loan\n\nRule mining and clustering are unsupervised methods (no ground truth), and classification, ranking, and density estimation are supervised methods (there is ground truth). In all of these problems, we do not assume we know the distribution that the data are drawn from!\n\nTraining and Testing (in-sample and out-of-sample) for supervised learning\n\nTraining: training data are input, and model f is the output.\n\ni=1 =⇒ Algorithm =⇒ f.\n\n{(xi, yi)}m\n\nTesting: You want to predict y for a new x, where (x, y) comes from the same distribution as {(xi, yi)}m\n\ni=1.\n\nThat is, (x, y) ∼ D(X , Y) and each (xi, yi) ∼ D(X , Y).\n\nCompute f(x) and compare it to y. How well does f(x) match y? Measure goodness of f using a loss function R : Y × Y → R:\n\nRtest(f) = E(x,y)∼DR(f(x), y)\n\n=\nR(f(x), y)dD(x, y). (x,y)∼D\n\n2",
  "26": "Rtest is also called the true risk or the test error.\n\nCan we calculate Rtest?\n\nWe want Rtest to be small, to indicate that f(x) would be a good predictor (“es­ timator”) of y.\n\nFor instance\n\nR(f(x), y) = (f(x) − y)2\nleast squares loss, or\nR(f(x), y) = 1[sign(f(x))\n(mis)classification error\n=y]\n\nWhich problems might these loss functions correspond to?\n\nHow can we ensure Rtest(f) is small?\n\nLook at how well f performs (on average) on {(xi, yi)}i.\n\nm m 1\nRtrain(f) =\nR(f(xi), yi).\nm i=1\nRtrain is also called the empirical risk or training error. For example,\n\nm m 1\nRtrain(f) =\n=yi].\n1[sign(f(xi))\nm i=1\n\n(How many handwritten digits did f classify incorrectly?)\n\nSay our algorithm constructs f so that Rtrain(f) is small. If Rtrain(f) is small, hopefully Rtest(f) is too.\n\nWe would like a guarantee on how close Rtrain is to Rtest . When would it be close to Rtest?\n\n• If m is large.\n\n• If f is “simple.”\n\n3",
  "27": "Illustration\n\nIn one of the figures in the illustration, f:\n\n• was overfitted to the data\n\n• modeled the noise\n\n• “memorized” the examples, but didn’t give us much other useful information\n\n• doesn’t “generalize,” i.e., predict. We didn’t “learn” anything!\n\nComputational Learning Theory, a.k.a. Statistical Learning Theory, a.k.a., learning theory, and in particular, Vapnik’s Structural Risk Minimization (SRM) addresses generalization. Here’s SRM’s classic picture:\n\nWhich is harder to check for, overfitting or underfitting?\n\n4",
  "28": "Computational learning theory addresses how to construct probabilistic guaran­ tees on the true risk. In order to do this, it quantifies the class of “simple models.”\n\nBias/Variance Tradeoff is related to learning theory (actually, bias is related to\nlearning theory).\n\nInference Notes - Bias/Variance Tradeoff\n\n5",
  "29": "Regularized Learning Expression\n\nStructural Risk Minimization says that we need some bias in order to learn/generalize\n(avoid overfitting). Bias can take many forms:\n\n• “simple” models f(x) =\nλjx(j) where IλI2 =\nλ2 < C\nj 2 j j\n\n• “prior” in Bayesian statistics\n\n• connectivity of neurons in the brain\n\nRegularized Learning Expression:\n\nm\n\nR(f(xi), yi) + CRreg(f)\n\ni This expression is kind of omnipresent. This form captures many algorithms: SVM, boosting, ridge regression, LASSO, and logistic regression.\n\nIn the regularized learning expression, the loss R(f(xi), yi) could be:\n\n• “least squares loss” (f(xi) − yi)2\n\n• “misclassification error” 1[yi\nf(xi))] = 1[yif(xi)≤0]\n\n– Note that minimizing 1[yif(xi)≤0] is computationally hard. i\n\n̸=sign(\n\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\n−yif(xi)\n• “logistic loss” log2 1 + e\n⇐= logistic regression\n\n6\n\n[IMAGE_OCR 1]\n35 — tj <0)\nexp)\n3 04,1 +exp yo)\n—— max(0,1-¥\"09)\n25 2 15 1\nu = SS\n==)\no\n-2 AS A -0.5 o 0.5 1 15 2\ny* F090.",
  "30": "• “hinge loss” max(0, 1 − yif(xi)) ⇐= SVM\n\n−yif(xi) ⇐ • “exponential loss” e\n= AdaBoost\n\nIn the regularized learning expression, we define a couple of options for Rreg(f).\nUsually f is linear, f(x) =\nj λjx(j). We choose Rreg(f) to be either:\n\nP\n\n2 =\nj λj\n\n2 ⇐= ridge regression, SVM\n\nP\n\n• IλI2\n\n• IλI1 =\n|λj| ⇐= LASSO, approximately AdaBoost\nj\n\nP\n\n7",
  "31": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "32": "Bias/Variance Tradeoff\n\nA parameter is some quantity about a distribution that we would like to know.\nWe’ll estimate the parameter θ using an estimator θˆ. The bias of estimator θˆ for\nparameter θ is defined as:\n\n• Bias(θˆ,θ) := E(θˆ) − θ, where the expectation is with respect to the distribution\nthat θˆ is constructed from.\n\nAn estimator whose bias is 0 is called unbiased. Contrast bias with:\n\n• Var(θˆ) = E(θˆ− E(θˆ))2 .\n\nFigure s howing the f our comb inations of l ow and hi gh bias, and low and high variance.\n\nOf course, we’d like an estimator with low bias and low variance.\n\nA little bit of decision theory\n\n(The following is based on notes of David McAllester.)\n\nLet’s say our data come from some distribution D on X × Y, where Y ⊂ R. Usually we don’t know D (we instead only have data) but for the moment, let’s say we know it. We want to learn a function f : X →Y.\n\nThen if we could choose any f we want, what would we choose? Maybe we’d choose f to minimize the least squares error:\n\nEx,y∼D[(y − f(x))2].\n\n1",
  "33": "It turns out that the f ∗ that minimizes the above error is the conditional expec­ tation!\n\nDraw a picture\n\nProposition.\n\nf ∗ (x) = Ey[y|x].\n\nProof. Consider each x separately. For each x there’s a marginal distribution on y. In other words, look at Ey[(y − f(x))2|x] for each x. So, pick an x. For this x, define ¯y to be Ey[y|x]. Now,\n\nEy[(y − f(x))2|x]\n= Ey[(y − y¯ + ¯y − f(x))2|x]\n= Ey[(y − y¯)2|x] + Ey[(¯y − f(x))2|x] + 2Ey[(y − y¯)(¯y − f(x))|x]\n= Ey[(y − y¯)2|x] + (¯y − f(x))2 + 2(¯y − f(x))Ey[(y − y¯)|x]\n= Ey[(y − y¯)2|x] + (¯y − f(x))2\n\nwhere the last step follows from the definition of ¯y.\n\nSo how do we pick f(x)? Well, we can’t do anything about the first term, it doesn’t depend on f(x). The best choice of f(x) minimizes the second term,\nwhich happens at f(x) = y¯, where remember ¯y = Ey[y|x].\n\nSo we know for each x what to choose in order to minimize Ey[(y − f(x))2|x]. To complete the argument, note that:\n\nEx,y[(y − f(x))2] = Ex[Ey[(y − f(x))2|x]]\n\nand we have found the minima of the inside term for each x. •\n\nNote that if we’re interested instead in the absolute loss Ex,y[|y − f(x)|], it is possible to show that the best predictor is the conditional median, that is,\nf(x) = median[y|x].\n\n2",
  "34": "Back to Bias/Variance Decomposition\n\nLet’s think about a situation where we created our function f using data S. Why would we do that of course?\n\nWe have training set S = (x1, y1), . . . , (xm, ym), where each example is drawn iid\nfrom D. We want to use S to learn a function fS : X →Y.\n\nWe want to know what the error of fS is on average. In other words, we want to know what Ex,y,S[(y−fS(x))2] is. That will help us figure out how to minimize it.\nThis is going to be a neat result - it’s going to decompose into bias and variance\nterms!\n\nFirst, let’s consider some learning algorithm (which produced fS) and its ex­ pected prediction error:\n\nEx,y,S[(y − fS(x))2].\n\nRemember that the estimator fS is random, since it depends on the randomly drawn training data. Here, the expectation is taken with respect to a new ran­ domly drawn point x, y ∼ D and training data S ∼ Dm .\n\nLet us define the mean prediction of the algorithm at point x to be:\n\n¯f(x) = ES[fS(x)].\n\nIn other words, to get this value, we’d get infinitely many training sets, run the learning algorithm on all of them to get infinite predictions fS(x) for each x.\n\n¯ Then for each x we’d average the predictions to get f(x).\n\nWe can now decompose the error, at a fixed x, as follows:\n\nEy,S[(y − fS(x))2]\n\n= Ey,S[(y − y¯ + ¯y − fS(x))2]\n= Ey(y − y¯)2 + ES(¯y − fS(x))2 + 2Ey,S[(y − y¯)(¯y − fS(x))].\n\nThe third term here is zero, since Ey,S[(y−y¯)(¯y−fS(x))] = Ey(y−y¯)ES(¯y−fS(x)),\nand the first part of that is Ey(y − y¯) = 0.\n\n3",
  "35": "The first term is the variance of y around its mean. We don’t have control over that when we choose fS. This term is zero if y is deterministically related to x.\n\nLet’s look at the second term:\n\nES(¯y − fS(x))2\n\n¯ ¯\n= ES(¯y − f(x) + f(x) − fS(x))2\n\n¯ ¯\n= ES(¯y − f(x))2 + ES(f¯(x) − fS(x))2 + 2ES[(¯y − f(x))(f¯(x) − fS(x))]\n\n¯ ¯ The last term is zero, since (¯y − f(x)) is a constant, and f(x) is the mean of ¯ fS(x) with respect to S. Also the first term isn’t random. It’s (¯y − f(x))2 .\n\nPutting things together, what we have is this (reversing some terms):\n\nEy,S[(y − fS(x))2] = Ey(y − y¯)2 + ES(f¯(x) − fS(x))2 + (¯y − f¯(x))2 .\n\nIn this expression, the second term is the variance of our estimator around its mean. It controls how our predictions vary around its average prediction. The third term is the bias squared, where the bias is the difference between the av­ erage prediction and the true conditional mean.\n\nWe’ve just proved the following:\n\nTheorem.\n\nFor each fixed x, Ey,S[(y − fS(x))2] = vary|x(y) + varS(fS(x)) + bias(fS(x))2 .\n\nSo\nEx,y,S[(y − fS(x))2] = Ex[vary|x(y) + varS(fS(x)) + bias(fS(x))2].\n\nThat is the bias-variance decomposition.\n\nThe “Bias-Variance” tradeoff: want to choose fS to balance between reducing the\nsecond and third terms in order to make the lowest MSE. We can’t just minimize one or the other, it needs to be a balance. Sometimes, if you are willing to inject some bias, this can allow you to substantially reduce the variance. E.g., modeling with lower degree polynomials, rather than higher degree polynomials.\n\n4",
  "52": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "94": " and new algorithms are constantly added to the list of packages you can download.\n\n3. R is a useful skill. Employers that value analytics recognize R as useful and important. If for no other reason, learning R is worthwhile to help boost your r´esum´e.\n\nNote that R is a programming language, and there is no intuitive graphical user interface with buttons you can click to run different methods. However, with some practice, this kind of environment makes it easy to quickly code scripts and functions for vari",
  "36": "Question: Intuitively, what happens to the second term if fS fits the data per­ fectly every time (overfitting)?\n\nQuestion: Intuitively, what happens to the last two terms if fS is a flat line every time?\n\nThe bottom line: In order to predict well, you need to strike a balance between bias and variance.\n\n• The variance term controls wiggliness, so you’ll want to choose simple func­ tions that can’t yield predictions that are too varied.\n\n• The bias term controls how close the average model prediction is close to the truth, ¯y. You’ll need to pay attention to the data in order to reduce the bias term.\n\n• Since you can’t calculate either the bias or the variance term, what we usually do is just impose some “structure” into the functions we’re fitting with, so the class of functions we are working with is small (e.g., low degree polynomials). We then try to fit the data well using those functions. Hope­ fully this strikes the right balance of wiggliness (variance) and capturing the mean of the data (bias).\n\n• One thing we like to do is make assumptions on the distribution D, or at least on the class of functions that might be able to fit well. Those assumptions each lead to a different algorithm (i.e. model). How well the algorithm works or not depends on how true the assumption is.\n\n• Even when we’re not working with least squares error, we hope a similar idea holds (and will work on proving that later in the course). We’ll use the same type of idea, where we impose some structure, and hope it reduces wiggliness and will still give accurate predictions.\n\nGo back to the other notes!\n\n5",
  "37": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "38": "Clustering\n\nMIT 15.097 Course Notes Cynthia Rudin and S¸eyda Ertekin\n\nCredit: Dasgupta, Hastie, Tibshirani, Friedman\n\nClustering (a.k.a. data segmentation) Let’s segment a collection of examples into “clusters” so that objects within a cluster are more closely related to one another than objects assigned to different clusters. We want to assign each ex­ ample xi to a cluster k ∈{1, ...., K}.\n\nThe K-Means algorithm is a very popular way to do this. It assumes points lie\nin Euclidean space.\n\nn Input: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, ..., zK ) :=\nmin Ixi − zkI2\n\n2. k i The choice of the squared norm is fortuitous, it really helps simplify the math!\n\nn If we’re given points {zk}k, they can induce a Voronoi p artition of R : they break the space into cells where each cell corresponds to one of the zk’s. That is, each cell contains the region of space whose nearest representative is zk.\n\nDraw a picture\n\nWe can look at the examples in each of these regions of space, which are the clusters. Specifically,\n\nCk := {xi : the closest representative to xi is zk}.\n\nLet’s compute the cost another way. Before, we summed over examples, and then picked the right representative zk for each example. This time, we’ll sum over clusters, and look at all the examples in that cluster:\n\ncost(z1, ..., zK ) =\nIxi − zkI2\n\n2.\n\nk {i:xi∈Ck}\n\n1",
  "39": "While we’re analyzing, we’ll need to consider suboptimal partitions of the data, where an example might not be assigned to the nearest representative. So we redefine the cost:\n\nX X\n\nk ∥xi −zk∥2 2. (1)\n\ncost(C1, ..., CK; z1, ..., zK) =\n\n{i:xi∈Ck}\n\nLet’s say we only have one cluster to deal with. Call it C. The representative is z. The cost is then:\ncost(C; z) =\n∥xi −z∥2 2.\n\nX\n\n{ ∈} Where should we place z?\n\ni:xi C\n\nAs you probably guessed, we would put it at the mean of the examples in C. But\nalso, the additional cost incurred by picking z = mean(C) can be characterized\nvery simply:\n\n̸\n\nLemma 1. For any set C ⊂Rn and any z ∈Rn,\n\ncost(C; z) = cost(C, mean(C)) + C\nz mean(C) 2 2. | | · ∥− ∥\n\nLet’s go ahead and prove it. In order to do that, we need to do another biasvariance decomposition (this one’s pretty much identical to one of the ones we did before).\n\nLemma 2. Let X Rn be any random variable. For any z Rn, we have: ∈ ∈\n\nEX∥X −z∥2\n2 = EX∥X −EXX∥2\n2 + ∥z −EXX∥2\n2.\n\nProof. Let x¯ := EXX.\n\nj −z(j))2 X\n\nEX∥X −z∥2\n2 = EX\n(X(j)\n\nj\n−x¯(j) + x¯(j) −zj)2\nX\n\n= EX\n(X(j)\n\nj\n−x¯(j))2 + EX\n(x¯(j)\n\nj −zj)2 X X\n\n= EX\n(X(j)\n\nj −x¯(j))(x¯(j) −z(j)) X\n\n+2EX\n(X(j)\n\n= EX∥X −x¯∥2\n2 + EX∥x¯ −z∥2\n2 + 0. ■\n\n2",
  "40": "To prove Lemma 1, pick a specific choice for X, namely X is a uniform random draw from the points xi in set C. So X has a discrete distribution. What will happen with this choice of X is that the expectation will reduce to the cost we already defined above.\n\nX\n\nEX∥X −z∥2\n2 =\n(prob. that point i is chosen)∥xi −z∥2 2 i:xi C\n\n{ ∈} X 1\n=\n\n{i:xi∈C} |C ∥x | i −z∥2 1\n2 =\ncost(C, z) |C| (2)\n\nand if we use Lemma 2 substituting z to be x¯ (a.k.a., EXX, or mean(C)) and simplify as in (2):\n\n1 EX∥X −x¯∥2\n2 =\ncost(C, mean(C)). C | | (3)\n\nWe had already defined cost earlier, and the choice of X was nice because its expectation is just the cost. Let’s recopy Lemma 2’s statement here, using the x¯ notation:\n\nEX∥X −z∥2\n2 = EX∥X −x¯∥2\n2 + ∥z −x¯∥2\n2.\n\nPlugging in (2) and (3),\n\n1 1\ncost(C, z) =\ncost(C, mean(C)) +\n|C| |C ∥z −x¯∥2 | 2. (4)\n\nMultiplying through,\n\ncost(C; z) = cost(C, mean(C)) + |C| · ∥z −mean(C)∥2\n2.\n\nAnd that’s the statement of Lemma 1. ■\n\nTo really minimize the cost (1), you’d need to try all possible assignments of the m data points to K clusters. Uck! The number of distinct assignments is (Jain and Dubes 1988):\n\nK 1\nS(m, K) =\n(\nK! k=1\n−1)K−k K km k\n\nX\n\nS(10, 4) = 34K, S(19, 4)\n1010, ... so not doable. ≈\n\nLet’s try some heuristic gradient-descent-ish method instead.\n\n3",
  "41": "The K-Means Algorithm\n\nChoose the value of K before you start.\n\nn Initialize centers z1, ..., zK ∈ R and clusters C1, ..., CK in any way. Repeat until there is no further change in cost: for each k: Ck ←{xi : the closest representative is zk}\nfor each k: zk = mean(Ck)\n\nThis is simple enough, and takes O(Km) time per iteration.\n\nPPT demo\n\nOf course, it doesn’t always converge to the optimal solution.\n\nBut does the cost converge?\n\nLemma 3. During the course of the K-Means algorithm, the cost monotonically\ndecreases.\n\n(t) (t) (t) (t) Proof. Let z1 , ..., zK , C1 , ..., CK denote the centers and clusters at the start of\nthe tth iterate of K-Means. The first step of the iteration assigns each data point\nto its closest center, therefore, the cluster assignment is better:\n\n(t+1)\n(t+1)\n(t) (t) (t) (t) (t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\nOn the second step, each cluster is re-centered at its mean, so the representatives\nare better. By Lemma 1,\n\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\n•\n\nSo does the cost converge?\n\n4",
  "42": "Example of how K-Means could converge to the wrong thing\n\nHow might you make K-Means more likely to converge to the optimal?\n\nHow might you choose K? (Why can’t you measure test error?)\n\nOther ways to evaluate clusters (“cluster validation”)\n\nThere are loads of cluster validity measures, alternatives to the cost. Draw a picture\n\n• Davies-Baldwin Index - looks at average intracluster distance (within-cluster\ndistance) to the centroid (want it to be small), and intercluster distances between centroids (want it to be large).\n\n• Dunn Index - looks pairwise at minimal intercluster distance (want it to be\nlarge) and maximal intracluster distance (want it to be small).\n\nExample: Microarray data. Have 6830 genes (rows) and 64 patients (columns). The color of each box is a measurement of the expression level of a gene. The expression level of a gene is basically how much of its special protein it is pro­ ducing. The physical chip itself doesn’t actually measure protein levels, but a proxy for them (which is RNA, which sticks to the DNA on the chip). If the color is green, it means low expression levels, if the color is red, it means higher expression levels. Each patient is represented by a vector, which is the expression level of their genes. It’s a column vector with values given in color:\n\n5",
  "43": "Commons license. For more information, see http://ocw.mit.edu/fairuse.\n\nEach patient (column) has some type of cancer. Want to cluster patients to see whether patients with the same types of cancers cluster together. So each cluster center is an “average” patient expression level vector for some type of cancer. It’s also a column vector\n\nGraph of sum of squa\n\nst\n\nu\n\ne r s .\n\nres vs. number of cl\n\nHm, there’s no kink in this figure. Compare K = 3 solution with “true” clusters:\n\nT a b l e\n\nc o m p a r\n\ni n g\n\nK\n=\n\nue clust ers.\n\n3\n\ns o l u\n\nt i o n\n\nw\n\ni t h\n\nt r\n\nSpringer, 2009.\n\n6\n\n[IMAGE_OCR 1] oes ge\n= mal = af aa 2\n\naa ee Mery cian A EJ NEA A “ip yall bala\n- ga el \"Ei a\" =\nELE Pizda z. pk el U mu ZAJ li ges hae HOF\n= z mimo cm s s '\nmyk rt) a tz Se c AE FEG 12 ie id» pet",
  "44": "It’s pretty good at keeping the same cancers in the same cluster. The two breast cancers in the 2nd cluster were actually melanomas that metastasized.\n\nGenerally we cluster genes, not patients. Would really like to get something like this in practice:\n\nCourtesy of the Rockefeller University Press. Used with permission.\n\nFigure 7 from Rumfelt, Lynn, et al. \"Lineage Specification and Plasticity in CD19- Early B\ncell Precursors.\" Journal of Experimental Medicine 203 (2006): 675-87.\n\nwhere each row is a gene, and the columns are different immune cell types.\n\nA major issue with K-means: as K changes, cluster membership can change\narbitrarily. A solution is Hierarchical Clustering.\n\n• clusters at the next level of the hierarchy are created by merging clusters at the next lowest level.\n\n– lowest level: each cluster has 1 example\n\n– highest level: there’s only 1 cluster, containing all of the data.\n\n7\n\n[IMAGE_OCR 1] MLP CLP Fr.A Fr. B&C A uma B pw EBFI c D ot",
  "45": "lusterin g.\n\nchical c\n\nlevels o f hierar\n\nSeveral\n\nStatistical Learning, Springer, 2009.\n\nApplication Slides\n\n8",
  "46": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "47": "K-NN\n\n15.097 MIT, Spring 2012, Cynthia Rudin Credit: Seyda Ertekin\n\nK-Nearest Neighbors\n\n• Amongst the simplest of all machine learning algorithms. No eXplicit training or model. • Can be used both for classifcaton and regression.\n• Use XIs K-Nearest Neighbors to vote on what\nXIs label should be.\n\n1",
  "48": "K-Nearest Neighbors\n\n• Classify using the majority vote of the k closest training points\n\nX X X\n\n(a) 1-nearest neighbor\n(b) 2-nearest neighbor\n(c) 3-nearest neighbor\n\nK-Nearest Neighbors\n\n• K-NN algorithm does not explicitly compute decision\nboundaries. The boundaries between distinct classes form a subset of the Voronoi diagram of the training data.\n\nEach line segment is equidistant to neighboring points.\n\n2",
  "49": "K-Nearest Neighbors\n\n• For regression: the value for the test eXample becomes the (weighted) average of the values of the K neighbors.\n\nMaking K-NN More Powerful\n\n• A good value for K can be determined by considering a range of K values.\n\n– K too small: we’ll model the noise\n\n– K too large: neighbors include too many points from other classes\n\n• There are problems when there is a spread of distances among the K­\nNN. Use a distance-based voting scheme, where closer neighbors have\nmore influence.\n\n• The distance measure has to be meaningful – attributes should be scaled\n\n– Eg. Income varies 10,000-1,000,000 while height varies 1.5-1.8 meters\n\n3",
  "50": "Pros/Cons to K-NN\n\nPros:\n\n• Simple and powerful. No need for tuning complex parameters to build a model.\n\n• No training involved (“lazy”). New training examples can be added easily.\n\nPros/Cons to K-NN\n\nCons:\n\n• Expensive and slow: O(md), m= # examples, d= # dimensions\n\n– To determine the nearest neighbor of a new point x, must compute the distance to all m training examples. Runtime performance is slow, but can be improved.\n\n• Pre-sort training examples into fast data structures\n\n• Compute only an approximate distance\n\n• Remove redundant data (condensing)\n\n4",
  "51": "K-NN Applicatons\n\n• Handwriten character classifcaton using nearest neighbor in large databases. Smith, S.J et. al.; IEEE PAMI, 2004. Classify handwriten characters into numbers.\n\n• Fast content-based image retrieval based on equal-average K-nearest-neighbor\nsearch schemes z. Lu, H. Burkhardt, S. Boehmer; LNCS, 2006. CBIR (Content based image retrieval), return the closest neighbors as the relevant items to a query.\n\n• Use of K-Nearest Neighbor classifer for intrusion detecton\nYihua Liao, V.Rao Vemuri; Computers and Security Journal, 2002 Classify program behavior as normal or intrusive.\n\n• Fault Detecton Using the k-Nearest Neighbor Rule for Semiconductor Manufacturing\nProcesses He, Q.P., Jin Wang; IEEE Transactons in Semiconductor Manufacturing, 2007 Early fault detecton in industrial systems.\n\n5",
  "53": "Na¨ıve Bayes\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nThanks to S¸eyda Ertekin Credit: Ng, Mitchell\n\nThe Na¨ıve Bayes algorithm comes from a generative model. There is an important distinction between generative and discriminative models. In all cases, we\nwant to predict the label y, given x, that is, we want P(Y = y|X = x). Throughout the paper, we’ll remember that the probability distribution for measure P is\nover an unknown distribution over X × Y.\n\nNa¨ıve Bayes Generative Model\nEstimate P(X = x|Y = y) and P(Y = y)\nand use Bayes rule to get P(Y = y|X = x)\nDiscriminative Model\nDirectly estimate P(Y = y|X = x)\n\nMost of the top 10 classification algorithms are discriminative (K-NN, CART,\nC4.5, SVM, AdaBoost).\n\nFor Na¨ıve Bayes, we make an assumption that if we know the class label y, then we know the mechanism (the random process) of how x is generated.\n\nNa¨ıve Bayes is great for very high dimensional problems because it makes a very strong assumption. Very high dimensional problems suffer from the curse of dimensionality – it’s difficult to understand what’s going on in a high dimensional space without tons of data.\n\nExample: Constructing a spam filter. Each example is an email, each dimension “j” of vector x represents the presence of a word.\n\n1",
  "54": " a\n\n1 0\n\naardvark\n\n\n\n0 .. x . 1 ..\n\n aardwolf  .  .. . 0\n\n=\n\n\n\n buy  ... zyxt\n\nThis x represents an email containing the words “a” and “buy”, but not “aardvark” or “zyxt”. The size of the vocabulary could be ∼50,000 words, so we are in a 50,000 dimensional space.\n\nNa¨ıve Bayes makes the assumption that the x(j)’s are conditionally independent\ngiven y. Say y = 1 means spam email, word 2,087 is “buy”, and word 39,831 is\n“price.” Na¨ıve Bayes assumes that if y = 1 (it’s spam), then knowing x(2,087) = 1\n(email contains “buy”) won’t effect your belief about x(39,381) (email contains “price”).\n\nNote: This does not mean x(2,087) and x(39,831) are independent, that is,\n\nP(X(2,087) = x(2,087)) = P(X(2,087) = x(2,087)|X(39,831) = x(39,831)).\n\nIt only means they are conditionally independent given y. Using the definition of conditional probability recursively,\n\nP(X(1) = x(1), . . . , X(50,000) = x(50,000)|Y = y) =\nP(X(1) = x(1)|Y = y)P(X(2) = x(2)|Y = y, X(1) = x(1))\nP(X(3) = x(3)|Y = y, X(1) = x(1), X(2) = x(2))\n. . . P(X(50,000) = x(50,000)|Y = y, X(1) = x(1), . . . , X(49,999) = x(49,999)).\n\nThe independence assumption gives:\n\nP(X(1) = x(1), . . . , X(n) = x(n)|Y = y)\n= P(X(1) = x(1)|Y = y)P(X(2) = x(2)|Y = y) . . . P(X(n) = x(n)|Y = y)\nn\n=\nY\nP(X(j) = x(j)\n\nj=1\n|Y = y).\n(1)\n\n2",
  "55": "Bayes rule says\n\n(1) (1)\nP(Y = y)P(X(1) = x(1), ..., X(n) = x(n) Y = y)\nP(Y = y|X\n= x\n, ..., X(n) = x(n)) =\n|\nP(X(1) = x(1), ..., X(n) = x(n))\n\nso plugging in (1), we have\n\nn\nP(Y = y)\nP(X(j) = x(j)\nj=1\nY = y)\nP(Y = y|X(1) = x(1), ..., X(n) = x(n)) =\n\nQ |\n\nP(X(1) = x(1), ..., X(n) = x(n))\n\nFor a new test instance, called xtest, we want to choose the most probable value of y, that is\n\n(1) (n)\nP(Y = y˜) Q\n(1) j P(X\n= xtest, ..., X(n) = x\nyNB ∈ test arg max\n|Y = y˜)\n\n(1) (n) y˜\nP(X(1) = x\n, ..., X(n) test\n= xtest)\nn\n= arg max P(Y = y˜)\nY\nP(X(j) = x(j)|Y = y˜).\ny˜\nj=1\n\n(j)\nSo now, we just need P(Y = y˜) for each possible y˜, and P(X(j) = xtest|Y = y˜)\nfor each j and y˜. Of course we can’t compute those. Let’s use the empirical probability estimates:\n\n1 ˆ\n[y =y˜]\nP(Y = y˜) =\nP i i\n= fraction of data where the label is y˜\nm 1 i [ (j) ˆ\n\nP\n\n(j) P\nxi =x\n,y =y˜\n(j) (j) ] P(X\n= x\n|Y = y˜) =\ntest i\n= Conf(Y = y˜ →X(j)\n(j) test\n= xtest).\n1 i\n[yi=y˜]\n\nThat’s the simplest version of Na¨ıve Bayes:\n\nn ˆ ˆ (j) y\n∈arg max P(Y = y˜)\nY P(X(j) NB\n= xtest\ny˜\nj=1\n|Y = y˜).\n\nThere could potentially be a problem that most of the conditional probabilities are 0 because the dimensionality of the data is very high compared to the amount ˆ (j)\nof data. This causes a problem because if even one P(X(j) = xtest|Y = y˜) is zero\nthen the whole right side is zero. In other words, if no training examples from class “spam” have the word “tomato,” we’d never classify a test example containing the word “tomato” as spam!\n\n3",
  "56": "To avoid this, we (sort of) set the probabilities to a small positive value when there are no data. In particular, we use a “Bayesian shrinkage estimate” of P(X(j) (j)\n= xtest|Y = y˜) where we add some hallucinated examples. There are K\nhallucinated examples spread evenly over the possible values of X(j). K is the\nnumber of distinct values of X(j). The probabilities are pulled toward 1/K. So,\nnow we replace:\n\n1 i (j) (j)\n+ 1\n=\n) [ ˆ x x\n,y =y˜\nP(X(j) (j ]\n= xtest|Y = y˜) =\n\nP\n\nP i test i 1 i\n[yi=y˜] + K\n\nˆP(Y = y˜) =\nP 1 i\n[yi=y˜] + 1\nm + K\nˆ\nThis is called Laplace smoothing. The smoothing for P(Y = y˜) is probably unnecessary and has little to no effect.\n\nNa¨ıve Bayes is not necessarily the best algorithm, but is a good first thing to try, and performs surprisingly well given its simplicity!\n\nThere are extensions to continuous data and other variations too.\n\nPPT Slides\n\n4",
  "57": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "58": "Decision Trees\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nCredit: Russell & Norvig, Mitchell, Kohavi & Quinlan, Carter, Vanden Berghen\n\nWhy trees?\n\n• interpretable/intuitive, popular in medical applications because they mimic\nthe way a doctor thinks\n\n• model discrete outcomes nicely\n\n• can be very powerful, can be as complex as you need them\n\n• C4.5 and CART - from “top 10” - decision trees are very popular\n\nSome real examples (from Russell & Norvig, Mitchell)\n\n• BP’s GasOIL system for separating gas and oil on offshore platforms - decision trees replaced a hand-designed rules system with 2500 rules. C4.5-based\nsystem outperformed human experts and saved BP millions. (1986)\n\n• learning to fly a Cessna on a flight simulator by watching human experts fly the simulator (1992)\n\n• can also learn to play tennis, analyze C-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “splitting” attributes one by one. To determine which attribute to split, look at “node impurity.”\n\n• Assign leaf nodes the majority vote in the leaf.\n\n• When we get to the bottom, prune the tree to prevent overfitting\n\nWhy is this a good way to build a tree?\n\n1",
  "119": "or sampling from probability distributions. For example, the following commands\ngenerate random vectors of the user-specified length n from distributions (normal, exponential, poisson, uniform,\nbinomial) with user-specified parameters. There are other distributions as well.\n\n4",
  "59": "I have to warn you that C4.5 and CART are not elegant by any means that I can define elegant. But the resulting trees can be very elegant. Plus there are 2 of the top 10 algorithms in data mining that are decision tree algorithms! So it’s worth it for us to know what’s under the hood... even though, well... let’s just say it ain’t pretty.\n\nExample: Will the customer wait for a table? (from Russell & Norvig)\n\nHere are the attributes:\n\nList of ten attributes that contribute to whether the customer will wait for a table.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are the examples:\n\nTab le of twe lv e ex am ples , t he att ribu\n\ntes fo r ea ch , an d pred i ctio n of\n\nwhe the r th e cu st om er w i ll wai t.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are two options for the first feature to split at the top of the tree. Which one should we choose? Which one gives me the most information?\n\n2",
  "60": "Two examples of de cision trees. Opti on\n\nfir\n\non B is\n\nA i s sp lit\n\nsplit first by Ty pe.\n\nwh ile\n\nO pti\n\nst by\n\nP atr\n\non s,\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWhat we need is a formula to compute “information.” Before we do that, here’s another example. Let’s say we pick one of them (Patrons). Maybe then we’ll pick Hungry next, because it has a lot of “information”:\n\nA decision tree sp lit first by Patron s,\n\ngry\n\nthe n by Hun\n\n.\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWe’ll build up to the derivation of C4.5. Origins: Hunt 1962, ID3 of Quinlan 1979 (600 lines of Pascal), C4 (Quinlan 1987). C4.5 is 9000 lines of C (Quinlan 1993). We start with some basic information theory.\n\n3",
  "61": "Information Theory (from slides of Tom Carter, June 2011)\n\n“Information” from observing the occurrence of an event\n:= #bits needed to encode the probability of the event p = −log2 p.\n\nE.g., a coin flip from a fair coin contains 1 bit of information. If the event has probability 1, we get no information from the occurrence of the event.\n\nWhere did this definition of information come from? Turns out it’s pretty cool. We want to define I so that it obeys all these things:\n\n• I(p) ≥0, I(1) = 0; the information of any event is non-negative, no information from events with prob 1\n\n• I(p1 · p2) = I(p1) + I(p2); the information from two independent events\nshould be the sum of their informations\n\n• I(p) is continuous, slight changes in probability correspond to slight changes in information\n\nTogether these lead to:\n\nI(p2) = 2I(p) or generally I(pn) = nI(p),\n\nthis means that\n\n1\nI(p) = I\n\n(p1/m)m\n= mI\n\np1/m\nso\nI(p) = I\n\np1/m\nm\n\nand more generally,\n\nI\n\npn/m\nn\n=\nI(p). m\nThis is true for any fraction n/m, which includes rationals, so just define it for\nall positive reals:\nI(pa) = aI(p).\n\nThe functions that do this are I(p) = −logb(p) for some b. Choose b = 2 for\n“bits.”\n\n4",
  "62": "Flipping a fair coin gives −log2(1/2) = 1 bit of information if it comes up either\nheads or tails.\n\nA biased coin landing on heads with p = .99 gives −log2(.99) = .0145 bits of\ninformation.\n\nA biased coin landing on heads with p = .01 gives −log2(.01) = 6.643 bits of\ninformation.\n\nNow, if we had lots of events, what’s the mean information of those events? Assume the events v1, ..., vJ occur with probabilities p1, ..., pJ, where [p1, ..., pJ] is a discrete probability distribution.\n\nX J\nEp [p1,...,pJ]I(p) =\npjI(p ∼\nj) =\n\nj=1\n− X\npj log2 pj =: H(p)\n\nj\n\nwhere p is the vector [p1, ..., pJ]. H(p) is called the entropy of discrete distribution p.\n\nSo if there are only 2 events (binary), with probabilities p = [p, 1 −p],\n\nH(p) = −p log2(p) −(1 −p) log2(1 −p).\n\nIf the probabilities were [1/2, 1/2],\n\n1 1\nH(p) = −2\nlog2\n= 1\n(Yes, we knew that.) 2 2\n\nOr if the probabilities were [0.99, 0.01],\n\nH(p) = 0.08 bits.\n\nAs one of the probabilities in the vector p goes to 1, H(p) →0, which is what we want.\n\nBack to C4.5, which uses Information Gain as the splitting criteria.\n\n5",
  "63": "Back to C4.5 (source material: Russell & Norvig, Mitchell, Quinlan) We consider a “test” split on attribute A at a branch.\n\nIn S we have #pos positives and #neg negatives. For each branch j, we have #posj positives and #negj negatives.\n\nThe training probabilities\n\nin branch j are:\n\n#posj #negj ,\n#posj + #negj #posj + #negj\n\n.\n\nThe Information Gain is calculated like this:\n\nGain(S, A) = expected reduction in entropy due to branching on attribute A\n= original\n\nentropy −entropy after branching #pos #neg\n= H\n,\n#pos + #neg #pos + #neg\n\nJ\n\n−\nX #posj + #negj\n#posj #negj H , .\n#pos + #neg\n\n#posj + #negj #posj + #negj\nj=1\n\nBack to the example with\n\nthe restauran\n\nts.\n\n1 1 2 4 6 2 4\nGain(S, Patrons) = H\n, −\nH([0, 1]) +\nH([1, 0]) +\nH , 2 2 12 12 12\n\n6 6\n\n≈0.541 bits.\n\n2 1 1 2 1 1\nGain(S, Type) = 1 −\nH ,\n+\nH , 12 2 2 12 2 2 4\n\n2 2 4 2 2 ,\n+\nH , 0 bits. 4 4\n\n≈\n\n+\nH 12\n\n12\n\n4 4\n\n6",
  "64": "Actually Patrons has the highest gain among the attributes, and is chosen to be the root of the tree. In general, we want to choose the feature A that maximizes Gain(S, A).\n\nOne problem with Gain is that it likes to partition too much, and favors numerous splits: e.g., if each branch contains 1 example:\n\nThen,\n\nH\n\n#posj #negj ,\n= 0 for all j,\n#posj + #negj #posj + #negj\nso all those negative terms would be zero and we’d\n\nchoose that attribute over all the others.\n\nAn alternative to Gain is the Gain Ratio. We want to have a large Gain, but also we want small partitions. We’ll choose our attribute according to:\n\nGain(S, A) ←want large SplitInfo(S, A) ←want small\n\nwhere SplitInfo(S, A) comes from the partition:\n\nJ Sj Sj\nSplitInfo(S, A) =\nX | | − log |S|\n\n| |\n\nj=1\n|S|\n\nwhere |Sj| is the number of examples in branch j. We want each term in the S sum to be large. That means we want | j| to be large, meaning that we want |S| lots of examples in each branch.\n\nKeep splitting until:\n\n7\n\n[IMAGE_OCR 1] S$ taining data going to this branch\nA=[0,0,0,0,0\nA=[1,1,1,1,1]\nA=[00901]|\n#pos=1 I ] #pos=0\n#neg=0 A=[0,00,1,1] #neg=1\na #pos=1\nmer #neg=0",
  "65": "• no more examples left (no point trying to split)\n\n• all examples have the same class\n\n• no more attributes to split\n\nFor the restaurant example, we get this:\n\nA decisi\n\nee s plit by\n\n, Type,\n\non tr\n\nPa tro\n\nand Fr\n\ni/Sat.\n\nun gry\n\nns, H\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nA wrinkle: actually, it turns out that the class labels for the data were themselves generated from a tree. So to get the label for an example, they fed it into a tree, and got the label from the leaf. That tree is here:\n\n8",
  "284": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "66": "A decisi\n\non tree for t\n\nant exampl e, usin\n\nhe restaur\n\ng most of th e attrib\n\nutes.\n\nA Modern Approach, Prentice Hall, 2009.\n\nBut the one we found is simpler! Does that mean our algorithm isn’t doing a good job?\n\nThere are possibilities to replace H([p, 1 −p]), it is not the only thing we can use! One example is the Gini index 2p(1 −p) used by CART. Another example is the value 1 −max(p, 1 −p).1\n\n1If an event has prob p of success, this value is the proportion of time you guess incorrectly if you classify the\nevent to happen when p > 1/2 (and classify the event not to happen when p ≤1/2).\n\n9",
  "67": "Gin\n\nf p bet\n\nclassifica\n\ny,\n\nrop\n\ntion error for values o\n\nent\n\nof\n\nGra ph\n\ni i nde x, and mi s\n\nween 0 and 1\n\n.\n\nLearning, Springer, 2009.\n\nC4.5 uses information gain for splitting, and CART uses the Gini index. (CART only has binary splits.)\n\nPruning Let’s start with C4.5’s pruning. C4.5 recursively makes choices as to whether to prune on an attribute:\n\n• Option 1: leaving the tree as is\n\n• Option 2: replace that part of the tree with a leaf corresponding to the most frequent label in the data S going to that part of the tree.\n\n• Option 3: replace that part of the tree with one of its subtrees, corresponding to the most common branch in the split\n\nDemo To figure out which decision to make, C4.5 computes upper bounds on the probability of error for each option. I’ll show you how to do that shortly.\n\n• Prob of error for Option 1 ≤UpperBound1\n\n• Prob of error for Option 2 ≤UpperBound2\n\n10",
  "68": "• Prob of error for Option 3 ≤UpperBound3\n\nC4.5 chooses the option that has the lowest of these three upper bounds. This ensures that (w.h.p.) the error rate is fairly low.\n\nE.g., which has the smallest upper bound:\n\n• 1 incorrect out of 3\n\n• 5 incorrect out of 17, or\n\n• 9 incorrect out of 32?\n\nFor each option, we count the number correct and the number incorrect. We need upper confidence intervals on the proportion that are incorrect. To calculate the upper bounds, calculate confidence intervals on proportions.\n\nThe abstract problem is: say you flip a coin N times, with M heads. (Here N is the number of examples in the leaf, M is the number incorrectly classified.) What is an upper bound for the probability p of heads for the coin?\n\nThink visually about the binomial distribution, where we have N coin flips, and how it changes as p changes:\n\np=0.5\np=0.1\np=0.9\n\nWe want the upper bound to be as large as possible (largest possible p, it’s an upper bound), but still there needs to be a probability α to get as few errors as we got. In other words, we want:\n\nPM ( ∼Bin(N,preasonable upper bound) M or fewer errors) ≥α\n\n11\n\n[IMAGE_OCR 1] ae O a4 aż m 0.03 0.05 004 002 % 5 10 15 20 25\n\n[IMAGE_OCR 2] 0.35 03 025 02 as a 05 % 5 10 15 20\n\n[IMAGE_OCR 3] 0.35 03 025 02 as a 05 % 5 10 15 20",
  "69": "which means we want to choose our upper bound, call it pα), so that it’s the largest possible value of preasonable upper bound that still satisfies that inequality.\n\nThat is,\n\nPM∼Bin(N,pα)(M or fewer errors) ≈α\n\nX M Bin(z, N, pα)\n\nz=0\n≈α\n\nX M N\npz (1 −p )N−z ≈α for M > 0\n(for M = 0 it’s (1 −p )N\nα z α α\nz=0\n≈α)\n\nWe can calculate this numerically without a problem. So now if you give me α\nM and N, I can give you pα. C4.5 uses α = .25 by default. M for a given branch\nis how many misclassified examples are in the branch. N for a given branch is just the number of examples in the branch, |Sj|.\n\nSo we can calculate the upper bound on a branch, but it’s still not clear how to calculate the upper bound on a tree. Actually, we calculate an upper confidence bound on each branch on the tree and average it over the relative frequencies of landing in each branch of the tree. It’s best explained by example:\n\nLet’s consider a dataset of 16 examples describing toys (from the Kranf Site). We want to know if the toy is fun or not.\n\n12\n\n[IMAGE_OCR 1]\nl-a\na ‘Fash diiuton this way",
  "70": "Color Max number of players Fun? red 2 yes red 3 yes green 2 yes red 2 yes green 2 yes green 4 yes green 2 yes green 1 yes red 2 yes green 2 yes red 1 yes blue 2 no green 2 yes green 1 yes red 3 yes green 1 yes Think of a split on color.\n\nTo calculate the upper bound on the tree for Option 1, calculate p.25 for each branch, which are respectively .206, .143, and .75. Then the average is: 1 1\nAve of the upper bounds for tree =\n(6 .206 + 9\n16 ·\n· .143 + 1 · .75) = 3.273 × 16\nLet’s compare that to the error rate of Option 2, where we’d replace the tree\nwith a leaf with 6+9+1 = 16 examples in it, where 15 are positive, and 1 is\n1\nnegative. Calculate pα that solves α = P\nz=0 Bin(z, 16, pα), which is .157. The\naverage is: 1 1\nAve of the upper bounds for leaf =\n16 · .157 = 2.512\n16 × . 16\nSay we had to make the decision amongst only Options 1 and 2. Since 2.512 <\n3.273, the upper bound on the error for Option 2 is lower, so we’ll prune the tree\nto a leaf. Look at the data - does it make sense to do this?\n\n13",
  "71": "CART - Classification and Regression Trees (Breiman, Freedman, Olshen, Stone,\n1984)\n\nDoes only binary splits, not multiway splits (less interpretable, but simplifies splitting criteria).\n\nLet’s do classification first, and regression later. For splitting, CART uses the Gini index. The Gini index again is\n\nvariance of Bin(n, p)\np(1 −p) =\n= variance of Bernoulli(p).\nn For pruning, CART uses “minimal cost complexity.”\n\nEach subtree is assigned a cost. The first term in the cost is a misclassification error. The second term is a regularization term. If we choose C to be large, the tree that minimizes the cost will be sparser. If C is small, the tree that minimizes the cost will have better training accuracy.\n\nX\n1[yi=leaf’s class] + C [#leaves in subtree] .\n\nX\n\ncost(subtree) =\n\n̸\n\nves j xi∈\n\nlea\n\nleaf j\n\nWe could create a sequence of nested subtrees by gradually increasing C.\n\nDraw a picture\n\nNow we need to choose C. Here’s one way to do this:\n\n• Step 1: For each C, hold out some data, split, then prune, producing a tree for each C.\n\n• Step 2: see which tree is the best on the holdout data, choose C.\n\n• Step 3: use all data, use chosen C, run split, then prune to produce the final tree.\n\nThere are other ways to do this! You’ll use cross-validation in the homework.\n\nReview C4.5 and CART\n\n14",
  "72": "CART’s Regression Trees\n\nFour-part figure in\n\ncl\n\nud ing partitions, a corresponding tree , and a perspective plot of the predicti on surface .\n\nLearning, Springer, 2009.\n\nCART decides which attributes to split and where to split them. In each leaf, we’re going to assign f(x) to be a constant.\n\nCan you guess what value to assign?\n\nConsider the empirical error, using the least squares loss:\n\nRtrain(f) =\nX (yi i −f(xi))2\n\n15",
  "73": "Break it up by leaves. Call the value of f(x) in leaf j by fj since it’s a constant.\n\nX (y f(x 2 i i))\n\nX\n\nRtrain(f) =\n\nX ves j i −\n\n∈\n\nlea\n\nleaf j\n\n=\n(y\n2 =:\nRtrain i fj) j (fj).\n\nX\n\nX\n\nleaf j −\n\nleaves j i∈\n\nlea\n\nves j\n\nTo choose the value of the fj’s so that they minimize Rtrain j , take the derivative, set it to 0. Let |Sj| be the number of examples in leaf j.\n\nd\n0 =\n˜ df i∈\n\nX ˜ (yi f)2\n˜f=fj\nleaf j −\n\n!\n\nX\n\nyi i −|Sj| ˜f\n\n!\n\n= −2\nX (yi i −˜f)\n\n= −2\n˜f=fj\n\n˜f=fj\n1\nfj =\nX\nyi = y¯\n\n|Sj| Sj,\n\ni∈leaf j\n\nwhere y¯Sj is the sample average of the labels for leaf j’s examples.\n\nSo now we know what value to assign for f in each leaf. How to split? Greedily want attribute A and split point s solving the following.\n\n\n\n\n\nmin (yi\nC1)2 + min\n(yi C2)2 . A, s C1 x\n\nX\n\nX\n\nC2 leaf| (A) − −\n\nmin\n\n\n\n( i leaf for each attribute A ∈{\n|x A)>s\ndo }\n\ni∈{ x ≤s} x\n\na\n\nlinesearch over s\n\nThe first term means that we’ll choose the optimal C1 = y¯{leaf|x(A)≤s . The second\n}\nterm means we’ll choose C2 = y¯{leaf x(A)>s .\n| }\n\nFor pruning, again CART does minimal cost complexity pruning:\n\ni∈S\n−y¯Sj)2 + C[# leaves in tree]\n\nX\n\nX yi\n\ncost =\n(\n\nves j x\n\nlea\n\nj\n\n16",
  "74": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "75": "Logistic Regression\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nThanks to Ashia Wilson Credit: J.S. Cramer’s “The Origin of Logistic Regression”\n\nOrigins: 19th Century.\n\n• Studying growth of populations and the course of chemical reactions using\n\nd W(t) = βW(t)\ndt ⇒\nW(t) = Aeβt\n\nwhich is a good model for unopposed growth, like the US population’s growth at the time.\n\n• Adolphe Quetelet (1796 - 1874), Belgian astronomer turned statistician,\nknew it produced impossible values and asked his pupil Pierre-Fran¸cois Verhulst (1804-1849) to help him work on a more realistic model. They chose\n\nd W(t) = βW(t) −Φ(W(t))\ndt\n\nto resist further growth, and with the choice of Φ to be a quadratic function, they got:\n\nd W(t) = βW(t)(Ω\ndt −W(t)),\n\nW(t)\nwhere Ωis the saturation limit of W. Writing P(t) =\nas the proportion of Ω saturation limit:\nd P(t) = βP(t)(1 −P(t))\ndt\ne(α+βt)\nP(t) = 1 + e(α+βt)\n\nwhere α is the constant from the integration. Verhulst’s “logistic” function has values between 0 and 1.\n\nDraw the logistic function\n\n1",
  "76": "He published in 3 papers between 1838 and 1847. The first paper demonstrated that the curve agrees very well with the actual course of the population in France, Belgium, Essex, and Russia for periods up to 1833. He did not say how he fitted\nthe curves. In the second paper he tried to fit the logistic to 3 points using 20-30\nyears of data (which in general is a not a great way to get a good model). His estimates of the limiting population Ωof 6.6 million for Belgium and 40 million\nfor France were a little off- these populations are now 11 million for Belgium\nand 65 million for France. In another paper, the estimate was corrected, and they estimated 9.5 million for Belgium (pretty close!).\n\nVerhulst died young in poor heath, and his work was forgotten about, but reinvented in 1920 by Raymond Pearl and Lowell Reed who were studying population growth of the US. They also tried to fit the logistic function to population growth, and estimated Ωfor the US to be 197 million (the current population is 312 million). Actually, Pearl and collaborators spent 20 years applying the logistic growth curve to almost any living population (fruit flies, humans in North Africa, cantaloupes).\n\nVerhulst’s work was rediscovered just after Pearl and Reed’s first paper in 1920, but they didn’t acknowledge him in their second paper, and only in a footnote in a third paper (by Pearl) in 1922. They cited him in 1923, but didn’t use his terminology and called his papers “long since forgotten.” The name logistic was revived by Yule, in a presidential address to the Royal Statistical Society in 1925.\n\nThere was a lot of debate over whether the logistic function could replace the cdf of the normal distribution. The person who really showed this could happen was\nJoseph Berkson (1899-1982), who was the chief statistician at the Mayo Clinic,\nand who was a collaborator of Reed’s. But because of his personality and attacks on the method of maximum likelihood, controversy ensued... it took until around t",
  "77": "re was a lot of debate over whether the logistic function could replace the cdf of the normal distribution. The person who really showed this could happen was\nJoseph Berkson (1899-1982), who was the chief statistician at the Mayo Clinic,\nand who was a collaborator of Reed’s. But because of his personality and attacks on the method of maximum likelihood, controversy ensued... it took until around the 1960’s to resolve it!\n\nLet’s start with a probabilistic model. Assume x ∈Rn, and that x is deterministic (chosen, not random):\n\nY ∼Bernoulli(P(Y = 1|x)).\n\n2",
  "78": "Here Y takes either 0 or 1, but we need a ±1 for classification. I’ll write ˜0 for\nnow to stand for either 0 or −1. The model is:\n\nP(Y = 1\nln\n|x, λ)\n= λTx\nP(Y = ˜0|x, λ)\n\n|\n\n{z\n\n}\n\n“odds ratio” Why does this model make an\n\nWe want to make a linear combination of feature values, like in regular regression, which explains the right hand side. But that can take any real values. We need to turn it into a model for\nP(Y = 1|x, λ), which takes values only between 0 and 1. The odds ratio turns\nthose probabilities into positive real numbers, then the log turns it into any real numbers.\n\ny sense\n\nat all?\n\nP(Y = 1|x, λ) = eλT x\nP(Y = ˜0|x, λ)\n\nT\n(Y = 1|\nT P\nx, λ) = eλ xP(Y = ˜0|x, λ) = eλ x(1 −P(Y = 1|x, λ))\n\nP(Y = 1|\nT\nx, λ)\nh\n1 + eλ xi\n= eλT x\n\neλT x\nP(Y = 1|x, λ) = 1 + eλT\nlogistic x ← function\n\nWe’ll use maximum likelihood estimation. We choose parameters λ to maximize the likelihood of the data given the model.\n\nm iid\nL(λ) := P(Y1 = y1, . . . , Ym = ym|λ, x1, . . . , xm) =\nY\nP(Yi = yi\ni=1\n|λ, xi).\n\nChoose\nλ∗∈argmax L(λ) = argmax log L(λ)\nλ\nλ\nlog-likelihood\nwhere we take the log for convenience since it doesn’t\n\n| {z\n\n}\n\neffect\n\nthe argmax.\n\nBut first, we’ll simplify. We need P(Y = yi|λ, xi) for both yi = 1 and yi = −1.\n \n\nT \n\nλ\nx\nyi = 1,\nneed P(Y = 1|λ, x\n1\ni) := pi =\ne i 1 T T T\n1+eλ\nx =\n=\ni\n1+e−λ\nxi\n1+e−y λ\nx i i T T If  \nyi = −\nλ\nx\nλ\nx 1 (i.e., ˜0),\nneed P(Y = ˜0|λ, xi) = 1 −p = 1+e\ni T T\nλ\nx − e i i\nλ\nx =\n1 T\n1+e\ni\n1+e\ni\n1+eλ\nxi\n=\n1\n1+e−\nT\ny λ\nx . i i \n\n3",
  "79": "So, we can just write\n\n1\nP(Y = yi|λ, xi) = 1 + e−y λT\n. i xi\n\nThen,\n\nλ∗∈argmax log L(λ)\nλ\nm\n= argmax\nX 1\nlog 1 + e−yiλT x\nλ\ni\ni=1\nm\n= argmin\nλ\n\nX\nlog(1 + e−yiλT xi).\n\ni=1\n\nThis agrees with the “frequentist” derivation we had before.\n\nThe loss is convex in λ so we can minimize by gradient descent.\n\n4",
  "80": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "81": "Rule Mining and the Apriori Algorithm\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nThe Apriori algorithm - often called the “first thing data miners try,” but somehow doesn’t appear in most data mining textbooks or courses!\n\nStart with market basket data:\n\nSome important definitions:\n\n• Itemset: a subset of items, e.g., (bananas, cherries, elderberries), indexed by {2, 3, 5}.\n\n• Support of an itemset: number of transactions containing it,\n\nm\nSupp(bananas, cherries, elderberries) =\nX Mi,2 · Mi,3\ni=1\n· Mi,5.\n\n• Co",
  "82": "ansactions containing it,\n\nm\nSupp(bananas, cherries, elderberries) =\nX Mi,2 · Mi,3\ni=1\n· Mi,5.\n\n• Confidence of rule a →b: the fraction of times itemset b is purchased when itemset a is purchased.\n\nSupp(a ∪b) #times a and b are purchased\nConf(a →b) =\n=\nSupp(a) #times a is purchased ˆ\n= P(b|a).\n\n1",
  "83": "We want to find all strong rules. These are rules a →b such that:\n\nSupp(a ∪b) ≥θ, and Conf(a →b) ≥minconf.\n\nHere θ is called the minimum support threshold.\n\nThe support has a monotonicity property called downward closure:\n\nIf Supp(a ∪b) ≥θ then Supp(a) ≥θ and Supp(b) ≥θ.\n\nThat is, if a ∪b is a frequent item set, then so are a and b.\n\nSupp(a ∪b) = #times a and b are purchased\n≤#times a is purchased = Supp(a).\n\nApriori finds all frequent itemsets (a such that Supp(a) ≥θ).\nWe can use Apriori’s result to get al",
  "84": "\nApriori finds all frequent itemsets (a such that Supp(a) ≥θ).\nWe can use Apriori’s result to get all strong rules a →b as follows:\n\n• For each frequent itemset ℓ:\n\n– Find all nonempty subsets of ℓ\n\n– For each subset a, output a →{ℓ\\ a} whenever\n\nSupp(ℓ) Supp(a) ≥minconf.\n\nNow for Apriori. Use the downward closure property: generate all k-itemsets\n(itemsets of size k) from (k −1)-itemsets. It’s a breadth-first-search.\n\n2",
  "85": "Example:\nθ = 10\n\nerries\n\nbananas\n\ncherries\n\nes grap\n\napples\n\nelderb\n\n1-itemsets:\na b c d e\n/f\ng supp: 25 20 30 45 29 5 17\n\n2-itemsets:\n\n{ } {\n\na,b a,c} { } {a,e}\n\na,d ... { e,g} supp: 7 25 15 23 3\n\n3-itemsets: {a,c,d}\n{a,c,e} {b,d,g} ... supp: 15 22 15\n\n4-itemsets: {a,c,d,e}\nsupp: 12\n\nApriori Algorithm:\n\nInput: Matrix M\n\nL1={frequent 1-itemsets; i such that Supp(i) ≥θ}.\n\nFor k = 2, while Lk−1 = ∅(while there are large k −1-itemsets), k + +\n\n̸\n\n• Ck = apriori gen(Lk\n1) generate candidate itemsets of size k −",
  "86": "are large k −1-itemsets), k + +\n\n̸\n\n• Ck = apriori gen(Lk\n1) generate candidate itemsets of size k −\n• Lk = {c : c ∈Ck, Supp(c) ≥θ} frequent itemsets of size k (loop over\ntransactions, scan the database)\n\nend\n\nS\n\nOutput: k Lk.\n\n3",
  "87": "The subroutine apriori gen joins Lk−1 to Lk−1.\n\napriori gen Subroutine:\n\nInput: Lk−1\n\nFind all pairs of itemsets in Lk−1 where the first k −2 items are identical.\n\ntoo big Union them (lexicographically) to get Ck ,\n\ne.g.,{a, b, c, d, e, f}, {a, b, c, d, e, g} →{a, b, c, d, e, f, g}\n\nPrune: Ck = {\ntoo big c ∈Ck\n, all (k −1)-subsets cs of c obey cs ∈Lk−1}.\n\nOutput: Ck.\n\nExample of Prune step: consider {a, b, c, d, e, f, g} too big which is in Ck , and I want to know whether it’s in Ck. Look at {a, b, c, d, e,",
  "88": " d, e, f, g} too big which is in Ck , and I want to know whether it’s in Ck. Look at {a, b, c, d, e, f, g}, {a, b, c, d, e, f, g},{a, b, c, d, e, f, g}, {a, b, c, d, e, f, g}, etc. If any are not in L6, then prune {a, b, c, d, e, f, g} from L7.\n\nExample of the aprior\n\ni a lgori\n\nthm.\n\n4",
  "89": "• Apriori scans the database at most how many times?\n\n• Huge number of candidate sets.\n\n/\n\n•\nwned huge number of apriori-like papers.\n\nSpa\n\nWhat do you do with the rules after they’re generated?\n\n• Information overload (give up)\n\n• Order rules by “interestingness”\n\n– Confidence Supp(a\nˆP(b|a) =\n∪b) Supp(a)\n\n– “Lift”/“Interest”\nˆP(b|a) Supp(b)\n=\nˆP(b) −Supp(a b 1 ∪) Supp(a) :\n\n– Hundreds!\n\nResearch questions:\n\n• mining more than just itemsets (e.g, sequences, trees, graphs)\n\n• incorporating taxonomy in items",
  "90": "\n• mining more than just itemsets (e.g, sequences, trees, graphs)\n\n• incorporating taxonomy in items\n\n• boolean logic and “logical analysis of data”\n\n• Cynthia’s questions: Can we use rules within ML to get good predictive models?\n\n5",
  "91": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "92": "R for Machine Learning\n\nAllison Chang\n\n1 Introduction\n\nIt is common for today’s scientific and business industries to collect large amounts of data, and the ability to analyze the data and learn from it is critical to making informed decisions. Familiarity with software such as R allows users to visualize data, run statistical tests, and apply machine learning algorithms. Even if you already know other software, there are still good reasons to learn R:\n\n1. R is free. If your future employer does not already",
  "93": "re, there are still good reasons to learn R:\n\n1. R is free. If your future employer does not already have R installed, you can always download it for free, unlike other proprietary software packages that require expensive licenses. No matter where you travel, you can have access to R on your computer.\n\n2. R gives you access to cutting-edge technology. Top researchers develop statistical learning methods\nin R, and new algorithms are constantly added to the list of packages you can download.\n\n3. R is a useful",
  "95": "some practice, this kind of environment makes it easy to quickly code scripts and functions for various statistical purposes. To get the most out of this tutorial, follow the examples\nby typing them out in R on your own computer. A line that begins with > is input at the command prompt. We\ndo not include the output in most cases, but you should try out the commands yourself and see what happens. If you type something at the command line and decide not to execute, press the down arrow to clear the line; pres",
  "96": "omething at the command line and decide not to execute, press the down arrow to clear the line; pressing the up arrow gives you the previous executed command.\n\n1.1 Getting Started\n\nThe R Project website is http://www.r-project.org/. In the menu on the left, click on CRAN under “Download,\nPackages.” Choose a location close to you. At MIT, you can go with University of Toronto under Canada. This leads you to instructions on how to download R for Linux, Mac, or Windows.\n\nOnce you open R, to figure out your cur",
  "97": "structions on how to download R for Linux, Mac, or Windows.\n\nOnce you open R, to figure out your current directory, type getwd(). To change directory, use setwd (note that the “C:” notation is for Windows and would be different on a Mac):\n\n> setwd(\"C:\\\\Datasets\")\n\n1.2 Installing and loading packages\n\nFunctions in R are grouped into packages, a number of which are automatically loaded when you start R. These include “base,” “utils,” “graphics,” and “stats.” Many of the most essential and frequently used func",
  "98": "nclude “base,” “utils,” “graphics,” and “stats.” Many of the most essential and frequently used functions come in these packages. However, you may need to download additional packages to obtain other useful functions. For example, an important classification method called Support Vector Machines is contained in a package called\n\n1",
  "99": "“e1071.” To install this package, click “Packages” in the top menu, then “Install package(s)...” When asked to select a CRAN mirror, choose a location close to you, such as “Canada (ON).” Finally select “e1071.” To load the package, type library(e1071) at the command prompt. Note that you need to install a package only once, but that if you want to use it, you need to load it each time you start R.\n\n1.3 Running code\n\nYou could use R by simply typing everything at the command prompt, but this does not easily",
  "100": "ng code\n\nYou could use R by simply typing everything at the command prompt, but this does not easily allow you to save, repeat, or share your code. Instead, go to “File” in the top menu and click on “New script.” This opens up a new window that you can save as a .R file. To execute the code you type into this window, highlight the lines you\nwish to run, and press Ctrl-R on a PC or Command-Enter on a Mac. If you want to run an entire script, make\nsure the script window is on top of all others, go to “Edit,” ",
  "101": "ou want to run an entire script, make\nsure the script window is on top of all others, go to “Edit,” and click “Run all.” Any lines that are run appear in red at the command prompt.\n\n1.4 Help in R\n\nThe functions in R are generally well-documented. To find documentation for a particular function, type ?\nfollowed directly by the function name at the command prompt. For example, if you need help on the “sum” function, type ?",
  "102": "the function name at the command prompt. For example, if you need help on the “sum” function, type ?sum. The help window that pops up typically contains details on both the input and output for the function of interest. If you are getting errors or unexpected output, it is likely that your input is insufficient or invalid, so use the documentation to figure out the proper way to call the function.\n\nIf you want to run a certain algorithm but do not know the name of the function in R, doing a Google search of",
  "103": "t to run a certain algorithm but do not know the name of the function in R, doing a Google search of R plus the algorithm name usually brings up information on which function to use.\n\n2 Datasets\n\nWhen you test any machine learning algorithm, you should use a variety of datasets. R conveniently comes with its own datasets, and you can view a list of their names by typing data() at the command prompt. For instance, you may see a dataset called “cars.” Load the data by typing data(cars), and view the data by t",
  "104": "nce, you may see a dataset called “cars.” Load the data by typing data(cars), and view the data by typing cars.\n\nAnother useful source of available data is the UCI Machine Learning Repository, which contains a couple hundred datasets, mostly from a variety of real applications in science and business. The repository is located at\nhttp://archive.ics.uci.edu/ml/datasets.html. These data are often used by machine learning researchers\nto develop and compare algorithms. We have downloaded a number of datasets fo",
  "105": "e learning researchers\nto develop and compare algorithms. We have downloaded a number of datasets for your use, and you can find the text files\n\n. These\n\ninclude:\n\nin the Datasets section\n\nName Rows Cols Data Iris 150 4 Real Wine 178 13 Integer, Real Haberman’s Survival 306 3 Integer Housing 506 14 Categorical, Integer, Real Blood Transfusion Service Center 748 4 Integer Car Evaluation 1728 6 Categorical Mushroom 8124 119 Binary\nPen-based Recognition of Handwritten Digits\n10992 16 Integer\n\n2",
  "106": "You are encouraged to download your own datasets from the UCI site or other sources, and to use R to study the data. Note that for all except the Housing and Mushroom datasets, there is an additional class attribute column that is not included in the column counts. Also note that if you download the Mushroom dataset from the UCI site, it has 22 categorical features; in our version, these have been transformed into 119 binary features.\n\n3 Basic Functions\n\nIn this section, we cover how to create data tables, ",
  "107": " into 119 binary features.\n\n3 Basic Functions\n\nIn this section, we cover how to create data tables, and analyze and plot data. We demonstrate by example how to use various functions. To see the value(s) of any variable, vector, or matrix at any time, simply enter its name in the command line; you are encouraged to do this often until you feel comfortable with how each data structure\nis being stored. To see all the objects in your workspace, type ls(). Also note that the arrow operator <- sets\nthe left-hand ",
  "108": "l the objects in your workspace, type ls(). Also note that the arrow operator <- sets\nthe left-hand side equal to the right-hand side, and that a comment begins with #.\n\n3.1 Creating data\n\nTo create a variable x and set it equal to 1, type x <- 1. Now suppose we want to generate the vector [1, 2, 3, 4, 5],\nand call the vector v. There are a couple different ways to accomplish this:\n\n> v <- 1:5\n> v <- c(1,2,3,4,5)\n# c can be used to concatenate multiple vectors\n> v <- seq(from=1,to=5,by=1)\n\nThese can be row ",
  "109": "4,5)\n# c can be used to concatenate multiple vectors\n> v <- seq(from=1,to=5,by=1)\n\nThese can be row vectors or column vectors. To generate a vector v0 of six zeros, use either of the following. Clearly the second choice is better if you are generating a long vector.\n\n> v0 <- c(0,0,0,0,0,0)\n> v0 <- seq(from=0,to=0,length.out=6)\n\nWe can combine vectors into matrices using cbind and rbind. For instance, if v1, v2, v3, and v4 are vectors of the same length, we can combine them into matrices, using them either a",
  "110": "2, v3, and v4 are vectors of the same length, we can combine them into matrices, using them either as columns or as rows:\n\n> v1 <- c(1,2,3,4,5)\n> v2 <- c(6,7,8,9,10)\n> v3 <- c(11,12,13,14,15)\n> v4 <- c(16,17,18,19,20)\n> cbind(v1,v2,v3,v4)\n> rbind(v1,v2,v3,v4)\n\nAnother way to create the second matrix is to use the matrix function to reshape a vector into a matrix of the right dimensions.\n\n> v <- seq(from=1,to=20,by=1)\n> matrix(v, nrow=4, ncol=5)\n\nNotice that this is not exactly right—we need to specify that ",
  "111": "20,by=1)\n> matrix(v, nrow=4, ncol=5)\n\nNotice that this is not exactly right—we need to specify that we want to fill in the matrix by row.\n\n> matrix(v, nrow=4, ncol=5, byrow=TRUE)\n\nIt is often helpful to name the columns and rows of a matrix using colnames and rownames. In the following, first we save the matrix as matrix20, and then we name the columns and rows.\n\n3",
  "112": "> matrix20 <- matrix(v, nrow=4, ncol=5, byrow=TRUE)\n> colnames(matrix20) <- c(\"Col1\",\"Col2\",\"Col3\",\"Col4\",\"Col5\")\n> rownames(matrix20) <- c(\"Row1\",\"Row2\",\"Row3\",\"Row4\")\n\nYou can type colnames(matrix20)/rownames(matrix20) at any point to see the column/row names for matrix20.\nTo access a particular element in a vector or matrix, index it by number or by name with square braces:\n\n> v[3]\n# third element of v\n> matrix20[,\"Col2\"]\n# second column of matrix20\n> matrix20[\"Row4\",]\n# fourth row of matrix20\n> matrix20",
  "113": "atrix20[,\"Col2\"]\n# second column of matrix20\n> matrix20[\"Row4\",]\n# fourth row of matrix20\n> matrix20[\"Row3\",\"Col1\"]\n# element in third row and first column of matrix20\n> matrix20[3,1]\n# element in third row and first column of matrix20\n\nYou can find the length of a vector or number of rows or columns in a matrix using length, nrow, and ncol.\n\n> length(v1)\n> nrow(matrix20)\n> ncol(matrix20)\n\nSince you will be working with external datasets, you will need functions to read in data tables from text files. For i",
  "114": "orking with external datasets, you will need functions to read in data tables from text files. For instance, suppose you wanted to read in the Haberman’s Survival dataset (from the UCI Repository). Use the read.table function:\n\ndataset <- read.table(\"C:\\\\Datasets\\\\haberman.csv\", header=FALSE, sep=\",\")\n\nThe first argument is the location (full path) of the file. If the first row of data contains column names, then the\nsecond argument should be header = TRUE, and otherwise it is header = FALSE. The third argu",
  "115": "then the\nsecond argument should be header = TRUE, and otherwise it is header = FALSE. The third argument contains\nthe delimiter. If the data are separated by spaces or tabs, then the argument is sep = \" \" and sep = \"\\t\"\nrespectively. The default delimiter (if you do not include this argument at all) is “white space” (one or more spaces, tabs, etc.). Alternatively, you can use setwd to change directory and use only the file name in the read.table function. If the delimiter is a comma, you can also use read.c",
  "116": " only the file name in the read.table function. If the delimiter is a comma, you can also use read.csv and leave off the sep argument:\n\ndataset <- read.csv(\"C:\\\\Datasets\\\\haberman.csv\", header=FALSE)\n\nUse write.table to write a table to a file. Type ?write.table to see details about this function. If you need to write text to a file, use the cat function.\n\nA note about matrices versus data frames: A data frame is similar to a matrix, except it can also include\nnon-numeric attributes. For example, there may ",
  "117": "e is similar to a matrix, except it can also include\nnon-numeric attributes. For example, there may be a column of characters. Some functions require the data\npassed in to be in the form of a data frame, which would be stated in the documentation. You can always coerce a matrix into a data frame using as.data.frame. See Section 3.6 for an example.\n\nA note about factors: A factor is essentially a vector of categorical variables, encoded using integers. For instance, if each example in our dataset has a binar",
  "118": "egorical variables, encoded using integers. For instance, if each example in our dataset has a binary class attribute, say 0 or 1, then that attribute can be represented as a factor. Certain functions require one of their arguments to be a factor. Use as.factor to encode a vector as a factor. See Sections 4.5 and 4.9 for examples.\n\n3.2 Sampling from probability distributions\n\nThere are a number of functions for sampling from probability distributions. For example, the following commands\ngenerate random vect",
  "120": "> norm_vec <- rnorm(n=10, mean=5, sd=2)\n> exp_vec <- rexp(n=100, rate=3)\n> pois_vec <- rpois(n=50, lambda=6)\n> unif_vec <- runif(n=20, min=1, max=9)\n> bin_vec <- rbinom(n=20, size=1000, prob=0.7)\n\nSuppose you have a vector v of numbers. To randomly sample, say, 25 of the numbers, use the sample function:\n\n> sample(v, size=25, replace=FALSE)\n\nIf you want to sample with replacement, set the replace argument to TRUE. If you want to generate the same random vector each time you call one of the random functions ",
  "121": "TRUE. If you want to generate the same random vector each time you call one of the random functions listed above, pick a “seed” for the random number generator using set.seed, for example set.seed(100).\n\n3.3 Analyzing data\n\nTo compute the mean, variance, standard deviation, minimum, maximum, and sum of a set of numbers, use mean, var, sd, min, max, and sum. There are also rowSum and colSum to find the row and column sums for a matrix.\nTo find the component-wise absolute value and square root of a set of num",
  "122": " column sums for a matrix.\nTo find the component-wise absolute value and square root of a set of numbers, use abs and sqrt. Correlation\nand covariance for two vectors are computed with cor and cov respectively.\n\nLike other programming languages, you can write if statements, and for and while loops. For instance, here is a simple loop that prints out even numbers between 1 and 10 (%% is the modulo operation):\n\n> for (i in 1:10){\n+ if (i %% 2 == 0){\n+\ncat(paste(i, \"is even.\\n\", sep=\" \"))\n# use paste to concat",
  "123": "\n> for (i in 1:10){\n+ if (i %% 2 == 0){\n+\ncat(paste(i, \"is even.\\n\", sep=\" \"))\n# use paste to concatenate strings\n+\n}\n+ }\n\nThe 1:10 part of the for loop can be specified as a vector. For instance, if you wanted to loop over indices 1, 2, 3, 5, 6, and 7, you could type for (i in c(1:3,5:7)). To pick out the indices of elements in a vector that satisfy a certain property, use which, for example:\n\n> which(v >= 0)\n# indices of nonnegative elements of v\n> v[which(v >= 0)]\n# nonnegative elements of v\n\n3.4 Plottin",
  "124": ")\n# indices of nonnegative elements of v\n> v[which(v >= 0)]\n# nonnegative elements of v\n\n3.4 Plotting data\n\nWe use the Haberman’s Survival data (read into data frame dataset) to demonstrate plotting functions. Each row of data represents a patient who had surgery for breast cancer. The three features are: the age of the patient at the time of surgery, the year of the surgery, and the number of positive axillary nodes detected. Here we plot:\n\n1. Scatterplot of the first and third features,\n\n2. Histogram of t",
  "125": "ry nodes detected. Here we plot:\n\n1. Scatterplot of the first and third features,\n\n2. Histogram of the second feature,\n\n3. Boxplot of the first feature.\n\nTo put all three plots in a 1 × 3 matrix, use par(mfrow=c(1,3)). To put each plot in its own window, use\nwin.graph() to create new windows.\n\n> plot(dataset[,1], dataset[,3], main=\"Scatterplot\", xlab=\"Age\", ylab=\"Number of Nodes\", pch=20)\n> hist(dataset[,2], main=\"Histogram\", xlab=\"Year\", ylab=\"Count\")\n> boxplot(dataset[,1], main=\"Boxplot\", xlab=\"Age\")\n\n5",
  "126": "Scatterplot Histogram Boxplot\n\n0 10 20 30 40 50\n\n0 10 20 30 40 50 60\n\n30 40 50 60 70 80\n\nNumber of Nodes\n\nCount\n\n30 40 50 60 70 80 58 60 62 64 66 68\n\nAge Year Age\n\nFigure 1: Plotting examples.\n\nThe pch argument in the plot function can be varied to change the marker. Use points and lines to add extra points and lines to an existing plot. You can save the plots in a number of different formats; make sure the plot window is on top, and go to “File” then “Save as.”\n\n3.5 Formulas\n\nCertain functions have a “form",
  "127": "lot window is on top, and go to “File” then “Save as.”\n\n3.5 Formulas\n\nCertain functions have a “formula” as one of their arguments. Usually this is a way to express the form of a model. Here is a simple example. Suppose you have a response variable y and independent variables x1, x2, and\nx3. To express that y depends linearly on x1, x2, and x3, you would use the formula y ∼ x1 + x2 + x3, where\ny, x1, x2, and x3 are also column names in your data matrix. See Section 3.6 for an example. Type ?",
  "128": "\ny, x1, x2, and x3 are also column names in your data matrix. See Section 3.6 for an example. Type ?formula for details on how to capture nonlinear models.\n\n3.6 Linear regression\n\nOne of the most common modeling approaches in statistical learning is linear regression. In R, use the lm function to generate these models. The general form of a linear regression model is\n\nY = β0 + β1X1 + β2X2 + · · · + βkXk + ε,\n\nwhere ε is normally distributed with mean 0 and some variance σ2 .",
  "129": " β1X1 + β2X2 + · · · + βkXk + ε,\n\nwhere ε is normally distributed with mean 0 and some variance σ2 .\n\nLet y be a vector of dependent variables, and x1 and x2 be vectors of independent variables. We want to find\nthe coefficients of the linear regression model Y = β0 + β1X1 + β2X2 + ε. The following commands generate the\nlinear regression model and give a summary of it.\n\n> lm_model <- lm(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)))\n> summary(lm_model)\n\nThe vector of coefficients for the model is contained",
  "130": "ta.frame(cbind(y,x1,x2)))\n> summary(lm_model)\n\nThe vector of coefficients for the model is contained in lm model$coefficients.\n\n4 Machine Learning Algorithms\n\nWe give the functions corresponding to the algorithms covered in class. Look over the documentation for each function on your own as only the most basic details are given in this tutorial.\n\n6",
  "131": "4.1 Prediction\n\nFor most of the following algorithms (as well as linear regression), we would in practice first generate the model using training data, and then predict values for test data. To make predictions, we use the predict function. To see documentation, go to the help page for the algorithm, and then scroll through the Contents menu on the left side to find the corresponding predict function. Or simply type ?",
  "132": "ough the Contents menu on the left side to find the corresponding predict function. Or simply type ?predict.name, where name is the function corresponding to the algorithm. Typically, the first argument is the variable in which you saved the model, and the second argument is a matrix or data frame of test data. Note that when you call the function, you can just type predict instead of predict.name. For instance, if we were to predict for the linear regression model above, and x1 test and x2 test are vectors",
  "133": "ce, if we were to predict for the linear regression model above, and x1 test and x2 test are vectors containing test data, we can use the command\n\n> predicted_values <- predict(lm_model, newdata=as.data.frame(cbind(x1_test, x2_test)))\n\n4.2 Apriori\n\nTo run the Apriori algorithm, first install the arules package and load it. See Section 1.2 for installing and loading new packages. Here is an example of how to run the Apriori algorithm using the Mushroom dataset. Note that the dataset must be a binary incidenc",
  "134": "un the Apriori algorithm using the Mushroom dataset. Note that the dataset must be a binary incidence matrix; the column names should correspond to the “items” that make up the “transactions.” The following commands print out a summary of the results and a list of the generated rules.\n\n> dataset <- read.csv(\"C:\\\\Datasets\\\\mushroom.csv\", header = TRUE)\n> mushroom_rules <- apriori(as.matrix(dataset), parameter = list(supp = 0.8, conf = 0.9))\n> summary(mushroom_rules)\n> inspect(mushroom_rules)\n\nYou can modify ",
  "135": "= list(supp = 0.8, conf = 0.9))\n> summary(mushroom_rules)\n> inspect(mushroom_rules)\n\nYou can modify the parameter settings depending on your desired support and confidence thresholds.\n\n4.3 Logistic Regression\n\nYou do not need to install an extra package for logistic regression. Using the same notation as in Section 3.6, the command is:\n\n> glm_mod <-glm(y ∼ x1+x2, family=binomial(link=\"logit\"), data=as.data.frame(cbind(y,x1,x2)))\n\n4.4\nK-Means Clustering\n\nYou do not need an extra package. If X is the data mat",
  "136": "ame(cbind(y,x1,x2)))\n\n4.4\nK-Means Clustering\n\nYou do not need an extra package. If X is the data matrix and m is the number of clusters, then the command is:\n\n> kmeans_model <- kmeans(x=X, centers=m)\n\n4.5\nk-Nearest Neighbor Classification\n\nInstall and load the class package. Let X train and X test be matrices of the training and test data respectively, and labels be a binary vector of class attributes for the training examples. For k equal to K, the command is:\n\n> knn_model <- knn(train=X_train, test=X_test",
  "137": " training examples. For k equal to K, the command is:\n\n> knn_model <- knn(train=X_train, test=X_test, cl=as.factor(labels), k=K)\n\nThen knn model is a factor vector of class attributes for the test set.\n\n7",
  "138": "4.6 Na¨ıve Bayes\n\nInstall and load the e1071 package. Using the same notation as in Section 3.6, the command is:\n\n> nB_model <- naiveBayes(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)))\n\n4.7 Decision Trees (CART)\n\nCART is implemented in the rpart package. Again using the formula, the command is\n\n> cart_model <- rpart(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)), method=\"class\")\n\nYou can use plot.rpart and text.rpart to plot the decision tree.\n\n4.8 AdaBoost\n\nThere are a number of different boosting func",
  "139": "d text.rpart to plot the decision tree.\n\n4.8 AdaBoost\n\nThere are a number of different boosting functions in R. We show here one implementation that uses decision trees as base classifiers. Thus the rpart package should be loaded. Also, the boosting function ada is in the ada\npackage. Let X be the matrix of features, and labels be a vector of 0-1 class labels. The command is\n\n> boost_model <- ada(x=X, y=labels)\n\n4.9 Support Vector Machines (SVM)\n\nThe SVM algorithm is in the e1071 package. Let X be the matri",
  "140": "s)\n\n4.9 Support Vector Machines (SVM)\n\nThe SVM algorithm is in the e1071 package. Let X be the matrix of features, and labels be a vector of 0-1 class\nlabels. Let the regularization parameter be C. Use the following commands to generate the SVM model and view details:\n\n> svm_model <- svm(x=X, y=as.factor(labels), kernel =\"radial\", cost=C)\n> summary(svm_model)\n\nThere are a variety of parameters such as the kernel type and value of the C parameter, so check the documentation on how to alter them.\n\n8",
  "141": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "142": "Fundamentals of Learning\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nImportant Problems in Data Mining\n\n1. Finding patterns (correlations) in large datasets\n-e.g. (Diapers → Beer). Use Apriori!\n\n2. Clustering - grouping data into clusters that “belong” together - objects\nwithin a cluster are more similar to each other than to those in other clusters.\n\n• Kmeans, Kmedians\n\ni=1, xi ∈X ⊂ Rn\n\n• Input: {xi}m\n\n• Output: f : X →{1, . . .",
  "143": " in other clusters.\n\n• Kmeans, Kmedians\n\ni=1, xi ∈X ⊂ Rn\n\n• Input: {xi}m\n\n• Output: f : X →{1, . . . , K} (K clusters)\n\n• clustering consumers for market research, clustering genes into families, image segmentation (medical imaging)\n\n3. Classification\n\n• Input: {(xi, yi)}m “examples,” “instances with labels,” “observations”\ni=1\n• xi ∈X , yi ∈ {−1, 1} “binary”\n\n• Output: f : X → R and use sign(f) to classify.\n\n• automatic handwriting recognition, speech recognition, biometrics, doc­ ument classification\n\n• “",
  "144": "\n• automatic handwriting recognition, speech recognition, biometrics, doc­ ument classification\n\n• “LeNet”\n\n4. Regression\n\n1\n\n[IMAGE_OCR 1]\n3 +\n\n+ + o\n\nre -\n. SE",
  "145": "• Input: {(xi, yi)}m\n\ni=1, xi ∈X , yi ∈ R\n\n• Output: f : X → R\n\n• predicting an individual’s income, predict house prices, predict stock prices, predict test scores\n\n5. Ranking (later) - in between classification and regression. Search engines\nuse ranking methods\n\n6. Density Estimation - predict conditional probabilities\n\ni=1, xi ∈X , yi ∈ {−1, 1}\n\n• {(xi, yi)}m\n\n• Output: f : X → [0, 1] as “close” to P (y = 1|x) as possible.\n\n• estimate probability of failure, probability to default on loan\n\nRule mining an",
  "146": "1|x) as possible.\n\n• estimate probability of failure, probability to default on loan\n\nRule mining and clustering are unsupervised methods (no ground truth), and classification, ranking, and density estimation are supervised methods (there is ground truth). In all of these problems, we do not assume we know the distribution that the data are drawn from!\n\nTraining and Testing (in-sample and out-of-sample) for supervised learning\n\nTraining: training data are input, and model f is the output.\n\ni=1 =⇒ Algorithm ",
  "147": "upervised learning\n\nTraining: training data are input, and model f is the output.\n\ni=1 =⇒ Algorithm =⇒ f.\n\n{(xi, yi)}m\n\nTesting: You want to predict y for a new x, where (x, y) comes from the same distribution as {(xi, yi)}m\n\ni=1.\n\nThat is, (x, y) ∼ D(X , Y) and each (xi, yi) ∼ D(X , Y).\n\nCompute f(x) and compare it to y. How well does f(x) match y? Measure goodness of f using a loss function R : Y × Y → R:\n\nRtest(f) = E(x,y)∼DR(f(x), y)\n\n=\nR(f(x), y)dD(x, y). (x,y)∼D\n\n2",
  "148": "Rtest is also called the true risk or the test error.\n\nCan we calculate Rtest?\n\nWe want Rtest to be small, to indicate that f(x) would be a good predictor (“es­ timator”) of y.\n\nFor instance\n\nR(f(x), y) = (f(x) − y)2\nleast squares loss, or\nR(f(x), y) = 1[sign(f(x))\n(mis)classification error\n=y]\n\nWhich problems might these loss functions correspond to?\n\nHow can we ensure Rtest(f) is small?\n\nLook at how well f performs (on average) on {(xi, yi)}i.\n\nm m 1\nRtrain(f) =\nR(f(xi), yi).\nm i=1\nRtrain is also called t",
  "149": "performs (on average) on {(xi, yi)}i.\n\nm m 1\nRtrain(f) =\nR(f(xi), yi).\nm i=1\nRtrain is also called the empirical risk or training error. For example,\n\nm m 1\nRtrain(f) =\n=yi].\n1[sign(f(xi))\nm i=1\n\n(How many handwritten digits did f classify incorrectly?)\n\nSay our algorithm constructs f so that Rtrain(f) is small. If Rtrain(f) is small, hopefully Rtest(f) is too.\n\nWe would like a guarantee on how close Rtrain is to Rtest . When would it be close to Rtest?\n\n• If m is large.\n\n• If f is “simple.”\n\n3",
  "150": "Illustration\n\nIn one of the figures in the illustration, f:\n\n• was overfitted to the data\n\n• modeled the noise\n\n• “memorized” the examples, but didn’t give us much other useful information\n\n• doesn’t “generalize,” i.e., predict. We didn’t “learn” anything!\n\nComputational Learning Theory, a.k.a. Statistical Learning Theory, a.k.a., learning theory, and in particular, Vapnik’s Structural Risk Minimization (SRM) addresses generalization. Here’s SRM’s classic picture:\n\nWhich is harder to check for, overfitting ",
  "151": " addresses generalization. Here’s SRM’s classic picture:\n\nWhich is harder to check for, overfitting or underfitting?\n\n4",
  "152": "Computational learning theory addresses how to construct probabilistic guaran­ tees on the true risk. In order to do this, it quantifies the class of “simple models.”\n\nBias/Variance Tradeoff is related to learning theory (actually, bias is related to\nlearning theory).\n\nInference Notes - Bias/Variance Tradeoff\n\n5",
  "153": "Regularized Learning Expression\n\nStructural Risk Minimization says that we need some bias in order to learn/generalize\n(avoid overfitting). Bias can take many forms:\n\n• “simple” models f(x) =\nλjx(j) where IλI2 =\nλ2 < C\nj 2 j j\n\n• “prior” in Bayesian statistics\n\n• connectivity of neurons in the brain\n\nRegularized Learning Expression:\n\nm\n\nR(f(xi), yi) + CRreg(f)\n\ni This expression is kind of omnipresent. This form captures many algorithms: SVM, boosting, ridge regression, LASSO, and logistic regression.\n\nIn t",
  "154": "orm captures many algorithms: SVM, boosting, ridge regression, LASSO, and logistic regression.\n\nIn the regularized learning expression, the loss R(f(xi), yi) could be:\n\n• “least squares loss” (f(xi) − yi)2\n\n• “misclassification error” 1[yi\nf(xi))] = 1[yif(xi)≤0]\n\n– Note that minimizing 1[yif(xi)≤0] is computationally hard. i\n\n̸=sign(\n\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\n−yif(xi)\n• “logistic loss” log2 1 + e\n⇐= logistic regression\n\n6\n\n[IMAGE_OCR 1]\n35 — tj <0)\nexp)\n3 04,1 ",
  "155": "f(xi)\n• “logistic loss” log2 1 + e\n⇐= logistic regression\n\n6\n\n[IMAGE_OCR 1]\n35 — tj <0)\nexp)\n3 04,1 +exp yo)\n—— max(0,1-¥\"09)\n25 2 15 1\nu = SS\n==)\no\n-2 AS A -0.5 o 0.5 1 15 2\ny* F090.",
  "156": "• “hinge loss” max(0, 1 − yif(xi)) ⇐= SVM\n\n−yif(xi) ⇐ • “exponential loss” e\n= AdaBoost\n\nIn the regularized learning expression, we define a couple of options for Rreg(f).\nUsually f is linear, f(x) =\nj λjx(j). We choose Rreg(f) to be either:\n\nP\n\n2 =\nj λj\n\n2 ⇐= ridge regression, SVM\n\nP\n\n• IλI2\n\n• IλI1 =\n|λj| ⇐= LASSO, approximately AdaBoost\nj\n\nP\n\n7",
  "157": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "521": "R on your own computer. A line that begins with > is input at the command prompt. We\ndo not include the output in most cases, but you should try out the commands yourself and see what happens. If you type something at the command line and decide not to exe",
  "158": "Bias/Variance Tradeoff\n\nA parameter is some quantity about a distribution that we would like to know.\nWe’ll estimate the parameter θ using an estimator θˆ. The bias of estimator θˆ for\nparameter θ is defined as:\n\n• Bias(θˆ,θ) := E(θˆ) − θ, where the expectation is with respect to the distribution\nthat θˆ is constructed from.\n\nAn estimator whose bias is 0 is called unbiased. Contrast bias with:\n\n• Var(θˆ) = E(θˆ− E(θˆ))2 .",
  "159": ".\n\nAn estimator whose bias is 0 is called unbiased. Contrast bias with:\n\n• Var(θˆ) = E(θˆ− E(θˆ))2 .\n\nFigure s howing the f our comb inations of l ow and hi gh bias, and low and high variance.\n\nOf course, we’d like an estimator with low bias and low variance.\n\nA little bit of decision theory\n\n(The following is based on notes of David McAllester.)\n\nLet’s say our data come from some distribution D on X × Y, where Y ⊂ R. Usually we don’t know D (we instead only have data) but for the moment, let’s say we know ",
  "160": "re Y ⊂ R. Usually we don’t know D (we instead only have data) but for the moment, let’s say we know it. We want to learn a function f : X →Y.\n\nThen if we could choose any f we want, what would we choose? Maybe we’d choose f to minimize the least squares error:\n\nEx,y∼D[(y − f(x))2].\n\n1",
  "161": "It turns out that the f ∗ that minimizes the above error is the conditional expec­ tation!\n\nDraw a picture\n\nProposition.\n\nf ∗ (x) = Ey[y|x].\n\nProof. Consider each x separately. For each x there’s a marginal distribution on y. In other words, look at Ey[(y − f(x))2|x] for each x. So, pick an x. For this x, define ¯y to be Ey[y|x]. Now,\n\nEy[(y − f(x))2|x]\n= Ey[(y − y¯ + ¯y − f(x))2|x]\n= Ey[(y − y¯)2|x] + Ey[(¯y − f(x))2|x] + 2Ey[(y − y¯)(¯y − f(x))|x]\n= Ey[(y − y¯)2|x] + (¯y − f(x))2 + 2(¯y − f(x))Ey[(y − y¯)",
  "162": " − f(x))2|x] + 2Ey[(y − y¯)(¯y − f(x))|x]\n= Ey[(y − y¯)2|x] + (¯y − f(x))2 + 2(¯y − f(x))Ey[(y − y¯)|x]\n= Ey[(y − y¯)2|x] + (¯y − f(x))2\n\nwhere the last step follows from the definition of ¯y.\n\nSo how do we pick f(x)? Well, we can’t do anything about the first term, it doesn’t depend on f(x). The best choice of f(x) minimizes the second term,\nwhich happens at f(x) = y¯, where remember ¯y = Ey[y|x].\n\nSo we know for each x what to choose in order to minimize Ey[(y − f(x))2|x]. To complete the argument, note t",
  "163": "w for each x what to choose in order to minimize Ey[(y − f(x))2|x]. To complete the argument, note that:\n\nEx,y[(y − f(x))2] = Ex[Ey[(y − f(x))2|x]]\n\nand we have found the minima of the inside term for each x. •\n\nNote that if we’re interested instead in the absolute loss Ex,y[|y − f(x)|], it is possible to show that the best predictor is the conditional median, that is,\nf(x) = median[y|x].\n\n2",
  "164": "Back to Bias/Variance Decomposition\n\nLet’s think about a situation where we created our function f using data S. Why would we do that of course?\n\nWe have training set S = (x1, y1), . . . , (xm, ym), where each example is drawn iid\nfrom D. We want to use S to learn a function fS : X →Y.\n\nWe want to know what the error of fS is on average. In other words, we want to know what Ex,y,S[(y−fS(x))2] is. That will help us figure out how to minimize it.\nThis is going to be a neat result - it’s going to decompose int",
  "165": "lp us figure out how to minimize it.\nThis is going to be a neat result - it’s going to decompose into bias and variance\nterms!\n\nFirst, let’s consider some learning algorithm (which produced fS) and its ex­ pected prediction error:\n\nEx,y,S[(y − fS(x))2].\n\nRemember that the estimator fS is random, since it depends on the randomly drawn training data. Here, the expectation is taken with respect to a new ran­ domly drawn point x, y ∼ D and training data S ∼ Dm .",
  "166": "xpectation is taken with respect to a new ran­ domly drawn point x, y ∼ D and training data S ∼ Dm .\n\nLet us define the mean prediction of the algorithm at point x to be:\n\n¯f(x) = ES[fS(x)].\n\nIn other words, to get this value, we’d get infinitely many training sets, run the learning algorithm on all of them to get infinite predictions fS(x) for each x.\n\n¯ Then for each x we’d average the predictions to get f(x).\n\nWe can now decompose the error, at a fixed x, as follows:\n\nEy,S[(y − fS(x))2]\n\n= Ey,S[(y − y¯ +",
  "167": "x).\n\nWe can now decompose the error, at a fixed x, as follows:\n\nEy,S[(y − fS(x))2]\n\n= Ey,S[(y − y¯ + ¯y − fS(x))2]\n= Ey(y − y¯)2 + ES(¯y − fS(x))2 + 2Ey,S[(y − y¯)(¯y − fS(x))].\n\nThe third term here is zero, since Ey,S[(y−y¯)(¯y−fS(x))] = Ey(y−y¯)ES(¯y−fS(x)),\nand the first part of that is Ey(y − y¯) = 0.\n\n3",
  "168": "The first term is the variance of y around its mean. We don’t have control over that when we choose fS. This term is zero if y is deterministically related to x.\n\nLet’s look at the second term:\n\nES(¯y − fS(x))2\n\n¯ ¯\n= ES(¯y − f(x) + f(x) − fS(x))2\n\n¯ ¯\n= ES(¯y − f(x))2 + ES(f¯(x) − fS(x))2 + 2ES[(¯y − f(x))(f¯(x) − fS(x))]\n\n¯ ¯ The last term is zero, since (¯y − f(x)) is a constant, and f(x) is the mean of ¯ fS(x) with respect to S. Also the first term isn’t random. It’s (¯y − f(x))2 .",
  "169": "f(x) is the mean of ¯ fS(x) with respect to S. Also the first term isn’t random. It’s (¯y − f(x))2 .\n\nPutting things together, what we have is this (reversing some terms):\n\nEy,S[(y − fS(x))2] = Ey(y − y¯)2 + ES(f¯(x) − fS(x))2 + (¯y − f¯(x))2 .\n\nIn this expression, the second term is the variance of our estimator around its mean. It controls how our predictions vary around its average prediction. The third term is the bias squared, where the bias is the difference between the av­ erage prediction and the tr",
  "522": "omething at the command line and decide not to execute, press the down arrow to clear the line; pressing the up arrow gives you the previous executed command.\n\n1.1 Getting Started\n\nThe R Project website is http://www.r-project.org/. In the menu on the left",
  "170": "rm is the bias squared, where the bias is the difference between the av­ erage prediction and the true conditional mean.\n\nWe’ve just proved the following:\n\nTheorem.\n\nFor each fixed x, Ey,S[(y − fS(x))2] = vary|x(y) + varS(fS(x)) + bias(fS(x))2 .\n\nSo\nEx,y,S[(y − fS(x))2] = Ex[vary|x(y) + varS(fS(x)) + bias(fS(x))2].\n\nThat is the bias-variance decomposition.\n\nThe “Bias-Variance” tradeoff: want to choose fS to balance between reducing the\nsecond and third terms in order to make the lowest MSE. We can’t just mi",
  "171": "alance between reducing the\nsecond and third terms in order to make the lowest MSE. We can’t just minimize one or the other, it needs to be a balance. Sometimes, if you are willing to inject some bias, this can allow you to substantially reduce the variance. E.g., modeling with lower degree polynomials, rather than higher degree polynomials.\n\n4",
  "172": "Question: Intuitively, what happens to the second term if fS fits the data per­ fectly every time (overfitting)?\n\nQuestion: Intuitively, what happens to the last two terms if fS is a flat line every time?\n\nThe bottom line: In order to predict well, you need to strike a balance between bias and variance.\n\n• The variance term controls wiggliness, so you’ll want to choose simple func­ tions that can’t yield predictions that are too varied.\n\n• The bias term controls how close the average model prediction is clo",
  "173": "ictions that are too varied.\n\n• The bias term controls how close the average model prediction is close to the truth, ¯y. You’ll need to pay attention to the data in order to reduce the bias term.\n\n• Since you can’t calculate either the bias or the variance term, what we usually do is just impose some “structure” into the functions we’re fitting with, so the class of functions we are working with is small (e.g., low degree polynomials). We then try to fit the data well using those functions. Hope­ fully this",
  "174": "., low degree polynomials). We then try to fit the data well using those functions. Hope­ fully this strikes the right balance of wiggliness (variance) and capturing the mean of the data (bias).\n\n• One thing we like to do is make assumptions on the distribution D, or at least on the class of functions that might be able to fit well. Those assumptions each lead to a different algorithm (i.e. model). How well the algorithm works or not depends on how true the assumption is.\n\n• Even when we’re not working with",
  "175": "he algorithm works or not depends on how true the assumption is.\n\n• Even when we’re not working with least squares error, we hope a similar idea holds (and will work on proving that later in the course). We’ll use the same type of idea, where we impose some structure, and hope it reduces wiggliness and will still give accurate predictions.\n\nGo back to the other notes!\n\n5",
  "176": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "177": "Clustering\n\nMIT 15.097 Course Notes Cynthia Rudin and S¸eyda Ertekin\n\nCredit: Dasgupta, Hastie, Tibshirani, Friedman\n\nClustering (a.k.a. data segmentation) Let’s segment a collection of examples into “clusters” so that objects within a cluster are more closely related to one another than objects assigned to different clusters. We want to assign each ex­ ample xi to a cluster k ∈{1, .",
  "178": "an objects assigned to different clusters. We want to assign each ex­ ample xi to a cluster k ∈{1, ...., K}.\n\nThe K-Means algorithm is a very popular way to do this. It assumes points lie\nin Euclidean space.\n\nn Input: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, .",
  "179": "nput: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, ..., zK ) :=\nmin Ixi − zkI2\n\n2. k i The choice of the squared norm is fortuitous, it really helps simplify the math!\n\nn If we’re given points {zk}k, they can induce a Voronoi p artition of R : they break the space into cells where each cell corresponds to one of the zk’s. That is, each cell contains the region of space whose nearest representative is zk.\n\nDraw a picture\n\nWe can look at the examples in each of ",
  "180": "f space whose nearest representative is zk.\n\nDraw a picture\n\nWe can look at the examples in each of these regions of space, which are the clusters. Specifically,\n\nCk := {xi : the closest representative to xi is zk}.\n\nLet’s compute the cost another way. Before, we summed over examples, and then picked the right representative zk for each example. This time, we’ll sum over clusters, and look at all the examples in that cluster:\n\ncost(z1, ..., zK ) =\nIxi − zkI2\n\n2.\n\nk {i:xi∈Ck}\n\n1",
  "181": "While we’re analyzing, we’ll need to consider suboptimal partitions of the data, where an example might not be assigned to the nearest representative. So we redefine the cost:\n\nX X\n\nk ∥xi −zk∥2 2. (1)\n\ncost(C1, ..., CK; z1, ..., zK) =\n\n{i:xi∈Ck}\n\nLet’s say we only have one cluster to deal with. Call it C. The representative is z. The cost is then:\ncost(C; z) =\n∥xi −z∥2 2.\n\nX\n\n{ ∈} Where should we place z?\n\ni:xi C\n\nAs you probably guessed, we would put it at the mean of the examples in C. But\nalso, the addit",
  "182": "xi C\n\nAs you probably guessed, we would put it at the mean of the examples in C. But\nalso, the additional cost incurred by picking z = mean(C) can be characterized\nvery simply:\n\n̸\n\nLemma 1. For any set C ⊂Rn and any z ∈Rn,\n\ncost(C; z) = cost(C, mean(C)) + C\nz mean(C) 2 2. | | · ∥− ∥\n\nLet’s go ahead and prove it. In order to do that, we need to do another biasvariance decomposition (this one’s pretty much identical to one of the ones we did before).\n\nLemma 2. Let X Rn be any random variable. For any z Rn, we",
  "183": "tical to one of the ones we did before).\n\nLemma 2. Let X Rn be any random variable. For any z Rn, we have: ∈ ∈\n\nEX∥X −z∥2\n2 = EX∥X −EXX∥2\n2 + ∥z −EXX∥2\n2.\n\nProof. Let x¯ := EXX.\n\nj −z(j))2 X\n\nEX∥X −z∥2\n2 = EX\n(X(j)\n\nj\n−x¯(j) + x¯(j) −zj)2\nX\n\n= EX\n(X(j)\n\nj\n−x¯(j))2 + EX\n(x¯(j)\n\nj −zj)2 X X\n\n= EX\n(X(j)\n\nj −x¯(j))(x¯(j) −z(j)) X\n\n+2EX\n(X(j)\n\n= EX∥X −x¯∥2\n2 + EX∥x¯ −z∥2\n2 + 0. ■\n\n2",
  "184": "To prove Lemma 1, pick a specific choice for X, namely X is a uniform random draw from the points xi in set C. So X has a discrete distribution. What will happen with this choice of X is that the expectation will reduce to the cost we already defined above.\n\nX\n\nEX∥X −z∥2\n2 =\n(prob. that point i is chosen)∥xi −z∥2 2 i:xi C\n\n{ ∈} X 1\n=\n\n{i:xi∈C} |C ∥x | i −z∥2 1\n2 =\ncost(C, z) |C| (2)\n\nand if we use Lemma 2 substituting z to be x¯ (a.k.a., EXX, or mean(C)) and simplify as in (2):\n\n1 EX∥X −x¯∥2\n2 =\ncost(C, mea",
  "185": "stituting z to be x¯ (a.k.a., EXX, or mean(C)) and simplify as in (2):\n\n1 EX∥X −x¯∥2\n2 =\ncost(C, mean(C)). C | | (3)\n\nWe had already defined cost earlier, and the choice of X was nice because its expectation is just the cost. Let’s recopy Lemma 2’s statement here, using the x¯ notation:\n\nEX∥X −z∥2\n2 = EX∥X −x¯∥2\n2 + ∥z −x¯∥2\n2.\n\nPlugging in (2) and (3),\n\n1 1\ncost(C, z) =\ncost(C, mean(C)) +\n|C| |C ∥z −x¯∥2 | 2. (4)\n\nMultiplying through,\n\ncost(C; z) = cost(C, mean(C)) + |C| · ∥z −mean(C)∥2\n2.\n\nAnd that’s the ",
  "186": ". (4)\n\nMultiplying through,\n\ncost(C; z) = cost(C, mean(C)) + |C| · ∥z −mean(C)∥2\n2.\n\nAnd that’s the statement of Lemma 1. ■\n\nTo really minimize the cost (1), you’d need to try all possible assignments of the m data points to K clusters. Uck! The number of distinct assignments is (Jain and Dubes 1988):\n\nK 1\nS(m, K) =\n(\nK! k=1\n−1)K−k K km k\n\nX\n\nS(10, 4) = 34K, S(19, 4)\n1010, ... so not doable. ≈\n\nLet’s try some heuristic gradient-descent-ish method instead.\n\n3",
  "187": "The K-Means Algorithm\n\nChoose the value of K before you start.\n\nn Initialize centers z1, ..., zK ∈ R and clusters C1, ..., CK in any way. Repeat until there is no further change in cost: for each k: Ck ←{xi : the closest representative is zk}\nfor each k: zk = mean(Ck)\n\nThis is simple enough, and takes O(Km) time per iteration.\n\nPPT demo\n\nOf course, it doesn’t always converge to the optimal solution.\n\nBut does the cost converge?\n\nLemma 3. During the course of the K-Means algorithm, the cost monotonically\ndec",
  "188": " the cost converge?\n\nLemma 3. During the course of the K-Means algorithm, the cost monotonically\ndecreases.\n\n(t) (t) (t) (t) Proof. Let z1 , ..., zK , C1 , ..., CK denote the centers and clusters at the start of\nthe tth iterate of K-Means. The first step of the iteration assigns each data point\nto its closest center, therefore, the cluster assignment is better:\n\n(t+1)\n(t+1)\n(t) (t) (t) (t) (t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , .",
  "189": "\n(t+1)\n(t+1)\n(t) (t) (t) (t) (t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\nOn the second step, each cluster is re-centered at its mean, so the representatives\nare better. By Lemma 1,\n\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\n•\n\nSo does the cost converge?\n\n4",
  "190": "Example of how K-Means could converge to the wrong thing\n\nHow might you make K-Means more likely to converge to the optimal?\n\nHow might you choose K? (Why can’t you measure test error?)\n\nOther ways to evaluate clusters (“cluster validation”)\n\nThere are loads of cluster validity measures, alternatives to the cost. Draw a picture\n\n• Davies-Baldwin Index - looks at average intracluster distance (within-cluster\ndistance) to the centroid (want it to be small), and intercluster distances between centroids (want i",
  "191": "istance) to the centroid (want it to be small), and intercluster distances between centroids (want it to be large).\n\n• Dunn Index - looks pairwise at minimal intercluster distance (want it to be\nlarge) and maximal intracluster distance (want it to be small).\n\nExample: Microarray data. Have 6830 genes (rows) and 64 patients (columns). The color of each box is a measurement of the expression level of a gene. The expression level of a gene is basically how much of its special protein it is pro­ ducing. The phy",
  "192": "e expression level of a gene is basically how much of its special protein it is pro­ ducing. The physical chip itself doesn’t actually measure protein levels, but a proxy for them (which is RNA, which sticks to the DNA on the chip). If the color is green, it means low expression levels, if the color is red, it means higher expression levels. Each patient is represented by a vector, which is the expression level of their genes. It’s a column vector with values given in color:\n\n5",
  "193": "Commons license. For more information, see http://ocw.mit.edu/fairuse.\n\nEach patient (column) has some type of cancer. Want to cluster patients to see whether patients with the same types of cancers cluster together. So each cluster center is an “average” patient expression level vector for some type of cancer. It’s also a column vector\n\nGraph of sum of squa\n\nst\n\nu\n\ne r s .",
  "194": "evel vector for some type of cancer. It’s also a column vector\n\nGraph of sum of squa\n\nst\n\nu\n\ne r s .\n\nres vs. number of cl\n\nHm, there’s no kink in this figure. Compare K = 3 solution with “true” clusters:\n\nT a b l e\n\nc o m p a r\n\ni n g\n\nK\n=\n\nue clust ers.\n\n3\n\ns o l u\n\nt i o n\n\nw\n\ni t h\n\nt r\n\nSpringer, 2009.\n\n6\n\n[IMAGE_OCR 1] oes ge\n= mal = af aa 2\n\naa ee Mery cian A EJ NEA A “ip yall bala\n- ga el \"Ei a\" =\nELE Pizda z. pk el U mu ZAJ li ges hae HOF\n= z mimo cm s s '\nmyk rt) a tz Se c AE FEG 12 ie id» pet",
  "195": "It’s pretty good at keeping the same cancers in the same cluster. The two breast cancers in the 2nd cluster were actually melanomas that metastasized.\n\nGenerally we cluster genes, not patients. Would really like to get something like this in practice:\n\nCourtesy of the Rockefeller University Press. Used with permission.\n\nFigure 7 from Rumfelt, Lynn, et al. \"Lineage Specification and Plasticity in CD19- Early B\ncell Precursors.\" Journal of Experimental Medicine 203 (2006): 675-87.\n\nwhere each row is a gene, a",
  "196": "\ncell Precursors.\" Journal of Experimental Medicine 203 (2006): 675-87.\n\nwhere each row is a gene, and the columns are different immune cell types.\n\nA major issue with K-means: as K changes, cluster membership can change\narbitrarily. A solution is Hierarchical Clustering.\n\n• clusters at the next level of the hierarchy are created by merging clusters at the next lowest level.\n\n– lowest level: each cluster has 1 example\n\n– highest level: there’s only 1 cluster, containing all of the data.\n\n7\n\n[IMAGE_OCR 1] ML",
  "197": "1 example\n\n– highest level: there’s only 1 cluster, containing all of the data.\n\n7\n\n[IMAGE_OCR 1] MLP CLP Fr.A Fr. B&C A uma B pw EBFI c D ot",
  "198": "lusterin g.\n\nchical c\n\nlevels o f hierar\n\nSeveral\n\nStatistical Learning, Springer, 2009.\n\nApplication Slides\n\n8",
  "199": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "200": "K-NN\n\n15.097 MIT, Spring 2012, Cynthia Rudin Credit: Seyda Ertekin\n\nK-Nearest Neighbors\n\n• Amongst the simplest of all machine learning algorithms. No eXplicit training or model. • Can be used both for classifcaton and regression.\n• Use XIs K-Nearest Neighbors to vote on what\nXIs label should be.\n\n1",
  "201": "K-Nearest Neighbors\n\n• Classify using the majority vote of the k closest training points\n\nX X X\n\n(a) 1-nearest neighbor\n(b) 2-nearest neighbor\n(c) 3-nearest neighbor\n\nK-Nearest Neighbors\n\n• K-NN algorithm does not explicitly compute decision\nboundaries. The boundaries between distinct classes form a subset of the Voronoi diagram of the training data.\n\nEach line segment is equidistant to neighboring points.\n\n2",
  "202": "K-Nearest Neighbors\n\n• For regression: the value for the test eXample becomes the (weighted) average of the values of the K neighbors.\n\nMaking K-NN More Powerful\n\n• A good value for K can be determined by considering a range of K values.\n\n– K too small: we’ll model the noise\n\n– K too large: neighbors include too many points from other classes\n\n• There are problems when there is a spread of distances among the K­\nNN. Use a distance-based voting scheme, where closer neighbors have\nmore influence.\n\n• The dista",
  "203": " K­\nNN. Use a distance-based voting scheme, where closer neighbors have\nmore influence.\n\n• The distance measure has to be meaningful – attributes should be scaled\n\n– Eg. Income varies 10,000-1,000,000 while height varies 1.5-1.8 meters\n\n3",
  "204": "Pros/Cons to K-NN\n\nPros:\n\n• Simple and powerful. No need for tuning complex parameters to build a model.\n\n• No training involved (“lazy”). New training examples can be added easily.\n\nPros/Cons to K-NN\n\nCons:\n\n• Expensive and slow: O(md), m= # examples, d= # dimensions\n\n– To determine the nearest neighbor of a new point x, must compute the distance to all m training examples. Runtime performance is slow, but can be improved.\n\n• Pre-sort training examples into fast data structures\n\n• Compute only an approxima",
  "205": "an be improved.\n\n• Pre-sort training examples into fast data structures\n\n• Compute only an approximate distance\n\n• Remove redundant data (condensing)\n\n4",
  "206": "K-NN Applicatons\n\n• Handwriten character classifcaton using nearest neighbor in large databases. Smith, S.J et. al.; IEEE PAMI, 2004. Classify handwriten characters into numbers.\n\n• Fast content-based image retrieval based on equal-average K-nearest-neighbor\nsearch schemes z. Lu, H. Burkhardt, S. Boehmer; LNCS, 2006. CBIR (Content based image retrieval), return the closest neighbors as the relevant items to a query.\n\n• Use of K-Nearest Neighbor classifer for intrusion detecton\nYihua Liao, V.Rao Vemuri; Comp",
  "207": " query.\n\n• Use of K-Nearest Neighbor classifer for intrusion detecton\nYihua Liao, V.Rao Vemuri; Computers and Security Journal, 2002 Classify program behavior as normal or intrusive.\n\n• Fault Detecton Using the k-Nearest Neighbor Rule for Semiconductor Manufacturing\nProcesses He, Q.P., Jin Wang; IEEE Transactons in Semiconductor Manufacturing, 2007 Early fault detecton in industrial systems.\n\n5",
  "208": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "209": "Na¨ıve Bayes\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nThanks to S¸eyda Ertekin Credit: Ng, Mitchell\n\nThe Na¨ıve Bayes algorithm comes from a generative model. There is an important distinction between generative and discriminative models. In all cases, we\nwant to predict the label y, given x, that is, we want P(Y = y|X = x). Throughout the paper, we’ll remember that the probability distribution for measure P is\nover an unknown distribution over X × Y.\n\nNa¨ıve Bayes Generative Model\nEstimate P(X = x|Y = y) an",
  "210": "s\nover an unknown distribution over X × Y.\n\nNa¨ıve Bayes Generative Model\nEstimate P(X = x|Y = y) and P(Y = y)\nand use Bayes rule to get P(Y = y|X = x)\nDiscriminative Model\nDirectly estimate P(Y = y|X = x)\n\nMost of the top 10 classification algorithms are discriminative (K-NN, CART,\nC4.5, SVM, AdaBoost).\n\nFor Na¨ıve Bayes, we make an assumption that if we know the class label y, then we know the mechanism (the random process) of how x is generated.\n\nNa¨ıve Bayes is great for very high dimensional problems b",
  "211": "e random process) of how x is generated.\n\nNa¨ıve Bayes is great for very high dimensional problems because it makes a very strong assumption. Very high dimensional problems suffer from the curse of dimensionality – it’s difficult to understand what’s going on in a high dimensional space without tons of data.\n\nExample: Constructing a spam filter. Each example is an email, each dimension “j” of vector x represents the presence of a word.\n\n1",
  "212": " a\n\n1 0\n\naardvark\n\n\n\n0 .. x . 1 ..\n\n aardwolf  .  .. . 0\n\n=\n\n\n\n buy  ... zyxt\n\nThis x represents an email containing the words “a” and “buy”, but not “aardvark” or “zyxt”. The size of the vocabulary could be ∼50,000 words, so we are in a 50,000 dimensional space.\n\nNa¨ıve Bayes makes the assumption that the x(j)’s are conditionally independent\ngiven y. Say y = 1 means spam email, word 2,087 is “buy”, and word 39,831 is\n“price.” Na¨ıve Bayes assumes that if y = 1 (it’s spam), then knowin",
  "213": "87 is “buy”, and word 39,831 is\n“price.” Na¨ıve Bayes assumes that if y = 1 (it’s spam), then knowing x(2,087) = 1\n(email contains “buy”) won’t effect your belief about x(39,381) (email contains “price”).\n\nNote: This does not mean x(2,087) and x(39,831) are independent, that is,\n\nP(X(2,087) = x(2,087)) = P(X(2,087) = x(2,087)|X(39,831) = x(39,831)).\n\nIt only means they are conditionally independent given y. Using the definition of conditional probability recursively,\n\nP(X(1) = x(1), . . .",
  "214": "ependent given y. Using the definition of conditional probability recursively,\n\nP(X(1) = x(1), . . . , X(50,000) = x(50,000)|Y = y) =\nP(X(1) = x(1)|Y = y)P(X(2) = x(2)|Y = y, X(1) = x(1))\nP(X(3) = x(3)|Y = y, X(1) = x(1), X(2) = x(2))\n. . . P(X(50,000) = x(50,000)|Y = y, X(1) = x(1), . . . , X(49,999) = x(49,999)).\n\nThe independence assumption gives:\n\nP(X(1) = x(1), . . . , X(n) = x(n)|Y = y)\n= P(X(1) = x(1)|Y = y)P(X(2) = x(2)|Y = y) . . . P(X(n) = x(n)|Y = y)\nn\n=\nY\nP(X(j) = x(j)\n\nj=1\n|Y = y).\n(1)\n\n2",
  "215": "Bayes rule says\n\n(1) (1)\nP(Y = y)P(X(1) = x(1), ..., X(n) = x(n) Y = y)\nP(Y = y|X\n= x\n, ..., X(n) = x(n)) =\n|\nP(X(1) = x(1), ..., X(n) = x(n))\n\nso plugging in (1), we have\n\nn\nP(Y = y)\nP(X(j) = x(j)\nj=1\nY = y)\nP(Y = y|X(1) = x(1), ..., X(n) = x(n)) =\n\nQ |\n\nP(X(1) = x(1), ..., X(n) = x(n))\n\nFor a new test instance, called xtest, we want to choose the most probable value of y, that is\n\n(1) (n)\nP(Y = y˜) Q\n(1) j P(X\n= xtest, ..., X(n) = x\nyNB ∈ test arg max\n|Y = y˜)\n\n(1) (n) y˜\nP(X(1) = x\n, .",
  "216": "\nP(Y = y˜) Q\n(1) j P(X\n= xtest, ..., X(n) = x\nyNB ∈ test arg max\n|Y = y˜)\n\n(1) (n) y˜\nP(X(1) = x\n, ..., X(n) test\n= xtest)\nn\n= arg max P(Y = y˜)\nY\nP(X(j) = x(j)|Y = y˜).\ny˜\nj=1\n\n(j)\nSo now, we just need P(Y = y˜) for each possible y˜, and P(X(j) = xtest|Y = y˜)\nfor each j and y˜. Of course we can’t compute those. Let’s use the empirical probability estimates:\n\n1 ˆ\n[y =y˜]\nP(Y = y˜) =\nP i i\n= fraction of data where the label is y˜\nm 1 i [ (j) ˆ\n\nP\n\n(j) P\nxi =x\n,y =y˜\n(j) (j) ] P(X\n= x\n|Y = y˜) =\ntest i\n= Con",
  "217": "where the label is y˜\nm 1 i [ (j) ˆ\n\nP\n\n(j) P\nxi =x\n,y =y˜\n(j) (j) ] P(X\n= x\n|Y = y˜) =\ntest i\n= Conf(Y = y˜ →X(j)\n(j) test\n= xtest).\n1 i\n[yi=y˜]\n\nThat’s the simplest version of Na¨ıve Bayes:\n\nn ˆ ˆ (j) y\n∈arg max P(Y = y˜)\nY P(X(j) NB\n= xtest\ny˜\nj=1\n|Y = y˜).\n\nThere could potentially be a problem that most of the conditional probabilities are 0 because the dimensionality of the data is very high compared to the amount ˆ (j)\nof data. This causes a problem because if even one P(X(j) = xtest|Y = y˜) is zero\nt",
  "218": "the amount ˆ (j)\nof data. This causes a problem because if even one P(X(j) = xtest|Y = y˜) is zero\nthen the whole right side is zero. In other words, if no training examples from class “spam” have the word “tomato,” we’d never classify a test example containing the word “tomato” as spam!\n\n3",
  "219": "To avoid this, we (sort of) set the probabilities to a small positive value when there are no data. In particular, we use a “Bayesian shrinkage estimate” of P(X(j) (j)\n= xtest|Y = y˜) where we add some hallucinated examples. There are K\nhallucinated examples spread evenly over the possible values of X(j). K is the\nnumber of distinct values of X(j). The probabilities are pulled toward 1/K. So,\nnow we replace:\n\n1 i (j) (j)\n+ 1\n=\n) [ ˆ x x\n,y =y˜\nP(X(j) (j ]\n= xtest|Y = y˜) =\n\nP\n\nP i test i 1 i\n[yi=y˜] + K\n\nˆP",
  "220": "\n1 i (j) (j)\n+ 1\n=\n) [ ˆ x x\n,y =y˜\nP(X(j) (j ]\n= xtest|Y = y˜) =\n\nP\n\nP i test i 1 i\n[yi=y˜] + K\n\nˆP(Y = y˜) =\nP 1 i\n[yi=y˜] + 1\nm + K\nˆ\nThis is called Laplace smoothing. The smoothing for P(Y = y˜) is probably unnecessary and has little to no effect.\n\nNa¨ıve Bayes is not necessarily the best algorithm, but is a good first thing to try, and performs surprisingly well given its simplicity!\n\nThere are extensions to continuous data and other variations too.\n\nPPT Slides\n\n4",
  "221": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "523": "http://www.r-project.org/. In the menu on the left, click on CRAN under “Download,\nPackages.” Choose a location close to you. At MIT, you can go with University of Toronto under Canada. This leads you to instructions on how to download R for Linux, Mac, or",
  "222": "Decision Trees\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nCredit: Russell & Norvig, Mitchell, Kohavi & Quinlan, Carter, Vanden Berghen\n\nWhy trees?\n\n• interpretable/intuitive, popular in medical applications because they mimic\nthe way a doctor thinks\n\n• model discrete outcomes nicely\n\n• can be very powerful, can be as complex as you need them\n\n• C4.5 and CART - from “top 10” - decision trees are very popular\n\nSome real examples (from Russell & Norvig, Mitchell)\n\n• BP’s GasOIL system for separating gas and oil o",
  "223": "e real examples (from Russell & Norvig, Mitchell)\n\n• BP’s GasOIL system for separating gas and oil on offshore platforms - decision trees replaced a hand-designed rules system with 2500 rules. C4.5-based\nsystem outperformed human experts and saved BP millions. (1986)\n\n• learning to fly a Cessna on a flight simulator by watching human experts fly the simulator (1992)\n\n• can also learn to play tennis, analyze C-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “s",
  "224": "-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “splitting” attributes one by one. To determine which attribute to split, look at “node impurity.”\n\n• Assign leaf nodes the majority vote in the leaf.\n\n• When we get to the bottom, prune the tree to prevent overfitting\n\nWhy is this a good way to build a tree?\n\n1",
  "225": "I have to warn you that C4.5 and CART are not elegant by any means that I can define elegant. But the resulting trees can be very elegant. Plus there are 2 of the top 10 algorithms in data mining that are decision tree algorithms! So it’s worth it for us to know what’s under the hood... even though, well... let’s just say it ain’t pretty.\n\nExample: Will the customer wait for a table? (from Russell & Norvig)\n\nHere are the attributes:\n\nList of ten attributes that contribute to whether the customer will wait f",
  "226": "Here are the attributes:\n\nList of ten attributes that contribute to whether the customer will wait for a table.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are the examples:\n\nTab le of twe lv e ex am ples , t he att ribu\n\ntes fo r ea ch , an d pred i ctio n of\n\nwhe the r th e cu st om er w i ll wai t.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are two options for the first feature to split at the top of the tree. Which one should we choose? Which one gives me the most information?\n\n2",
  "227": "Two examples of de cision trees. Opti on\n\nfir\n\non B is\n\nA i s sp lit\n\nsplit first by Ty pe.\n\nwh ile\n\nO pti\n\nst by\n\nP atr\n\non s,\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWhat we need is a formula to compute “information.” Before we do that, here’s another example. Let’s say we pick one of them (Patrons). Maybe then we’ll pick Hungry next, because it has a lot of “information”:\n\nA decision tree sp lit first by Patron s,\n\ngry\n\nthe n by Hun\n\n.",
  "228": "ause it has a lot of “information”:\n\nA decision tree sp lit first by Patron s,\n\ngry\n\nthe n by Hun\n\n.\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWe’ll build up to the derivation of C4.5. Origins: Hunt 1962, ID3 of Quinlan 1979 (600 lines of Pascal), C4 (Quinlan 1987). C4.5 is 9000 lines of C (Quinlan 1993). We start with some basic information theory.\n\n3",
  "229": "Information Theory (from slides of Tom Carter, June 2011)\n\n“Information” from observing the occurrence of an event\n:= #bits needed to encode the probability of the event p = −log2 p.\n\nE.g., a coin flip from a fair coin contains 1 bit of information. If the event has probability 1, we get no information from the occurrence of the event.\n\nWhere did this definition of information come from? Turns out it’s pretty cool. We want to define I so that it obeys all these things:\n\n• I(p) ≥0, I(1) = 0; the information ",
  "230": " cool. We want to define I so that it obeys all these things:\n\n• I(p) ≥0, I(1) = 0; the information of any event is non-negative, no information from events with prob 1\n\n• I(p1 · p2) = I(p1) + I(p2); the information from two independent events\nshould be the sum of their informations\n\n• I(p) is continuous, slight changes in probability correspond to slight changes in information\n\nTogether these lead to:\n\nI(p2) = 2I(p) or generally I(pn) = nI(p),\n\nthis means that\n\n1\nI(p) = I\n\n(p1/m)m\n= mI\n\np1/m\nso\nI(p) = I\n\np",
  "231": " = 2I(p) or generally I(pn) = nI(p),\n\nthis means that\n\n1\nI(p) = I\n\n(p1/m)m\n= mI\n\np1/m\nso\nI(p) = I\n\np1/m\nm\n\nand more generally,\n\nI\n\npn/m\nn\n=\nI(p). m\nThis is true for any fraction n/m, which includes rationals, so just define it for\nall positive reals:\nI(pa) = aI(p).\n\nThe functions that do this are I(p) = −logb(p) for some b. Choose b = 2 for\n“bits.”\n\n4",
  "232": "Flipping a fair coin gives −log2(1/2) = 1 bit of information if it comes up either\nheads or tails.\n\nA biased coin landing on heads with p = .99 gives −log2(.99) = .0145 bits of\ninformation.\n\nA biased coin landing on heads with p = .01 gives −log2(.01) = 6.643 bits of\ninformation.\n\nNow, if we had lots of events, what’s the mean information of those events? Assume the events v1, ..., vJ occur with probabilities p1, ..., pJ, where [p1, .",
  "233": "on of those events? Assume the events v1, ..., vJ occur with probabilities p1, ..., pJ, where [p1, ..., pJ] is a discrete probability distribution.\n\nX J\nEp [p1,...,pJ]I(p) =\npjI(p ∼\nj) =\n\nj=1\n− X\npj log2 pj =: H(p)\n\nj\n\nwhere p is the vector [p1, ..., pJ]. H(p) is called the entropy of discrete distribution p.\n\nSo if there are only 2 events (binary), with probabilities p = [p, 1 −p],\n\nH(p) = −p log2(p) −(1 −p) log2(1 −p).\n\nIf the probabilities were [1/2, 1/2],\n\n1 1\nH(p) = −2\nlog2\n= 1\n(Yes, we knew that.) 2 2",
  "524": "structions on how to download R for Linux, Mac, or Windows.\n\nOnce you open R, to figure out your current directory, type getwd(). To change directory, use setwd (note that the “C:” notation is for Windows and would be different on a Mac):\n\n> setwd(\"C:\\\\Dat",
  "234": " log2(1 −p).\n\nIf the probabilities were [1/2, 1/2],\n\n1 1\nH(p) = −2\nlog2\n= 1\n(Yes, we knew that.) 2 2\n\nOr if the probabilities were [0.99, 0.01],\n\nH(p) = 0.08 bits.\n\nAs one of the probabilities in the vector p goes to 1, H(p) →0, which is what we want.\n\nBack to C4.5, which uses Information Gain as the splitting criteria.\n\n5",
  "235": "Back to C4.5 (source material: Russell & Norvig, Mitchell, Quinlan) We consider a “test” split on attribute A at a branch.\n\nIn S we have #pos positives and #neg negatives. For each branch j, we have #posj positives and #negj negatives.\n\nThe training probabilities\n\nin branch j are:\n\n#posj #negj ,\n#posj + #negj #posj + #negj\n\n.",
  "236": "gatives.\n\nThe training probabilities\n\nin branch j are:\n\n#posj #negj ,\n#posj + #negj #posj + #negj\n\n.\n\nThe Information Gain is calculated like this:\n\nGain(S, A) = expected reduction in entropy due to branching on attribute A\n= original\n\nentropy −entropy after branching #pos #neg\n= H\n,\n#pos + #neg #pos + #neg\n\nJ\n\n−\nX #posj + #negj\n#posj #negj H , .",
  "237": "ropy after branching #pos #neg\n= H\n,\n#pos + #neg #pos + #neg\n\nJ\n\n−\nX #posj + #negj\n#posj #negj H , .\n#pos + #neg\n\n#posj + #negj #posj + #negj\nj=1\n\nBack to the example with\n\nthe restauran\n\nts.\n\n1 1 2 4 6 2 4\nGain(S, Patrons) = H\n, −\nH([0, 1]) +\nH([1, 0]) +\nH , 2 2 12 12 12\n\n6 6\n\n≈0.541 bits.\n\n2 1 1 2 1 1\nGain(S, Type) = 1 −\nH ,\n+\nH , 12 2 2 12 2 2 4\n\n2 2 4 2 2 ,\n+\nH , 0 bits. 4 4\n\n≈\n\n+\nH 12\n\n12\n\n4 4\n\n6",
  "238": "Actually Patrons has the highest gain among the attributes, and is chosen to be the root of the tree. In general, we want to choose the feature A that maximizes Gain(S, A).\n\nOne problem with Gain is that it likes to partition too much, and favors numerous splits: e.g., if each branch contains 1 example:\n\nThen,\n\nH\n\n#posj #negj ,\n= 0 for all j,\n#posj + #negj #posj + #negj\nso all those negative terms would be zero and we’d\n\nchoose that attribute over all the others.\n\nAn alternative to Gain is the Gain Ratio. W",
  "239": "ro and we’d\n\nchoose that attribute over all the others.\n\nAn alternative to Gain is the Gain Ratio. We want to have a large Gain, but also we want small partitions. We’ll choose our attribute according to:\n\nGain(S, A) ←want large SplitInfo(S, A) ←want small\n\nwhere SplitInfo(S, A) comes from the partition:\n\nJ Sj Sj\nSplitInfo(S, A) =\nX | | − log |S|\n\n| |\n\nj=1\n|S|\n\nwhere |Sj| is the number of examples in branch j. We want each term in the S sum to be large. That means we want | j| to be large, meaning that we w",
  "240": ". We want each term in the S sum to be large. That means we want | j| to be large, meaning that we want |S| lots of examples in each branch.\n\nKeep splitting until:\n\n7\n\n[IMAGE_OCR 1] S$ taining data going to this branch\nA=[0,0,0,0,0\nA=[1,1,1,1,1]\nA=[00901]|\n#pos=1 I ] #pos=0\n#neg=0 A=[0,00,1,1] #neg=1\na #pos=1\nmer #neg=0",
  "241": "• no more examples left (no point trying to split)\n\n• all examples have the same class\n\n• no more attributes to split\n\nFor the restaurant example, we get this:\n\nA decisi\n\nee s plit by\n\n, Type,\n\non tr\n\nPa tro\n\nand Fr\n\ni/Sat.\n\nun gry\n\nns, H\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nA wrinkle: actually, it turns out that the class labels for the data were themselves generated from a tree. So to get the label for an example, they fed it into a tree, and got the label from the leaf. That",
  "242": ". So to get the label for an example, they fed it into a tree, and got the label from the leaf. That tree is here:\n\n8",
  "243": "A decisi\n\non tree for t\n\nant exampl e, usin\n\nhe restaur\n\ng most of th e attrib\n\nutes.\n\nA Modern Approach, Prentice Hall, 2009.\n\nBut the one we found is simpler! Does that mean our algorithm isn’t doing a good job?\n\nThere are possibilities to replace H([p, 1 −p]), it is not the only thing we can use! One example is the Gini index 2p(1 −p) used by CART. Another example is the value 1 −max(p, 1 −p).1\n\n1If an event has prob p of success, this value is the proportion of time you guess incorrectly if you classify",
  "244": "nt has prob p of success, this value is the proportion of time you guess incorrectly if you classify the\nevent to happen when p > 1/2 (and classify the event not to happen when p ≤1/2).\n\n9",
  "245": "Gin\n\nf p bet\n\nclassifica\n\ny,\n\nrop\n\ntion error for values o\n\nent\n\nof\n\nGra ph\n\ni i nde x, and mi s\n\nween 0 and 1\n\n.\n\nLearning, Springer, 2009.\n\nC4.5 uses information gain for splitting, and CART uses the Gini index. (CART only has binary splits.)\n\nPruning Let’s start with C4.5’s pruning. C4.5 recursively makes choices as to whether to prune on an attribute:\n\n• Option 1: leaving the tree as is\n\n• Option 2: replace that part of the tree with a leaf corresponding to the most frequent label in the data S going to",
  "246": "ce that part of the tree with a leaf corresponding to the most frequent label in the data S going to that part of the tree.\n\n• Option 3: replace that part of the tree with one of its subtrees, corresponding to the most common branch in the split\n\nDemo To figure out which decision to make, C4.5 computes upper bounds on the probability of error for each option. I’ll show you how to do that shortly.\n\n• Prob of error for Option 1 ≤UpperBound1\n\n• Prob of error for Option 2 ≤UpperBound2\n\n10",
  "247": "• Prob of error for Option 3 ≤UpperBound3\n\nC4.5 chooses the option that has the lowest of these three upper bounds. This ensures that (w.h.p.) the error rate is fairly low.\n\nE.g., which has the smallest upper bound:\n\n• 1 incorrect out of 3\n\n• 5 incorrect out of 17, or\n\n• 9 incorrect out of 32?\n\nFor each option, we count the number correct and the number incorrect. We need upper confidence intervals on the proportion that are incorrect. To calculate the upper bounds, calculate confidence intervals on proport",
  "248": "portion that are incorrect. To calculate the upper bounds, calculate confidence intervals on proportions.\n\nThe abstract problem is: say you flip a coin N times, with M heads. (Here N is the number of examples in the leaf, M is the number incorrectly classified.) What is an upper bound for the probability p of heads for the coin?\n\nThink visually about the binomial distribution, where we have N coin flips, and how it changes as p changes:\n\np=0.5\np=0.1\np=0.9\n\nWe want the upper bound to be as large as possible ",
  "249": "how it changes as p changes:\n\np=0.5\np=0.1\np=0.9\n\nWe want the upper bound to be as large as possible (largest possible p, it’s an upper bound), but still there needs to be a probability α to get as few errors as we got. In other words, we want:\n\nPM ( ∼Bin(N,preasonable upper bound) M or fewer errors) ≥α\n\n11\n\n[IMAGE_OCR 1] ae O a4 aż m 0.03 0.05 004 002 % 5 10 15 20 25\n\n[IMAGE_OCR 2] 0.35 03 025 02 as a 05 % 5 10 15 20\n\n[IMAGE_OCR 3] 0.35 03 025 02 as a 05 % 5 10 15 20",
  "250": "which means we want to choose our upper bound, call it pα), so that it’s the largest possible value of preasonable upper bound that still satisfies that inequality.\n\nThat is,\n\nPM∼Bin(N,pα)(M or fewer errors) ≈α\n\nX M Bin(z, N, pα)\n\nz=0\n≈α\n\nX M N\npz (1 −p )N−z ≈α for M > 0\n(for M = 0 it’s (1 −p )N\nα z α α\nz=0\n≈α)\n\nWe can calculate this numerically without a problem. So now if you give me α\nM and N, I can give you pα. C4.5 uses α = .",
  "251": "s numerically without a problem. So now if you give me α\nM and N, I can give you pα. C4.5 uses α = .25 by default. M for a given branch\nis how many misclassified examples are in the branch. N for a given branch is just the number of examples in the branch, |Sj|.\n\nSo we can calculate the upper bound on a branch, but it’s still not clear how to calculate the upper bound on a tree. Actually, we calculate an upper confidence bound on each branch on the tree and average it over the relative frequencies of landin",
  "252": "r confidence bound on each branch on the tree and average it over the relative frequencies of landing in each branch of the tree. It’s best explained by example:\n\nLet’s consider a dataset of 16 examples describing toys (from the Kranf Site). We want to know if the toy is fun or not.\n\n12\n\n[IMAGE_OCR 1]\nl-a\na ‘Fash diiuton this way",
  "253": "Color Max number of players Fun? red 2 yes red 3 yes green 2 yes red 2 yes green 2 yes green 4 yes green 2 yes green 1 yes red 2 yes green 2 yes red 1 yes blue 2 no green 2 yes green 1 yes red 3 yes green 1 yes Think of a split on color.\n\nTo calculate the upper bound on the tree for Option 1, calculate p.25 for each branch, which are respectively .206, .143, and .75. Then the average is: 1 1\nAve of the upper bounds for tree =\n(6 .206 + 9\n16 ·\n· .143 + 1 · .",
  "254": "and .75. Then the average is: 1 1\nAve of the upper bounds for tree =\n(6 .206 + 9\n16 ·\n· .143 + 1 · .75) = 3.273 × 16\nLet’s compare that to the error rate of Option 2, where we’d replace the tree\nwith a leaf with 6+9+1 = 16 examples in it, where 15 are positive, and 1 is\n1\nnegative. Calculate pα that solves α = P\nz=0 Bin(z, 16, pα), which is .157. The\naverage is: 1 1\nAve of the upper bounds for leaf =\n16 · .157 = 2.512\n16 × .",
  "255": " pα), which is .157. The\naverage is: 1 1\nAve of the upper bounds for leaf =\n16 · .157 = 2.512\n16 × . 16\nSay we had to make the decision amongst only Options 1 and 2. Since 2.512 <\n3.273, the upper bound on the error for Option 2 is lower, so we’ll prune the tree\nto a leaf. Look at the data - does it make sense to do this?\n\n13",
  "256": "CART - Classification and Regression Trees (Breiman, Freedman, Olshen, Stone,\n1984)\n\nDoes only binary splits, not multiway splits (less interpretable, but simplifies splitting criteria).\n\nLet’s do classification first, and regression later. For splitting, CART uses the Gini index. The Gini index again is\n\nvariance of Bin(n, p)\np(1 −p) =\n= variance of Bernoulli(p).\nn For pruning, CART uses “minimal cost complexity.”\n\nEach subtree is assigned a cost. The first term in the cost is a misclassification error. Th",
  "257": "xity.”\n\nEach subtree is assigned a cost. The first term in the cost is a misclassification error. The second term is a regularization term. If we choose C to be large, the tree that minimizes the cost will be sparser. If C is small, the tree that minimizes the cost will have better training accuracy.\n\nX\n1[yi=leaf’s class] + C [#leaves in subtree] .",
  "258": "imizes the cost will have better training accuracy.\n\nX\n1[yi=leaf’s class] + C [#leaves in subtree] .\n\nX\n\ncost(subtree) =\n\n̸\n\nves j xi∈\n\nlea\n\nleaf j\n\nWe could create a sequence of nested subtrees by gradually increasing C.\n\nDraw a picture\n\nNow we need to choose C. Here’s one way to do this:\n\n• Step 1: For each C, hold out some data, split, then prune, producing a tree for each C.\n\n• Step 2: see which tree is the best on the holdout data, choose C.\n\n• Step 3: use all data, use chosen C, run split, then prune ",
  "259": "he best on the holdout data, choose C.\n\n• Step 3: use all data, use chosen C, run split, then prune to produce the final tree.\n\nThere are other ways to do this! You’ll use cross-validation in the homework.\n\nReview C4.5 and CART\n\n14",
  "260": "CART’s Regression Trees\n\nFour-part figure in\n\ncl\n\nud ing partitions, a corresponding tree , and a perspective plot of the predicti on surface .\n\nLearning, Springer, 2009.\n\nCART decides which attributes to split and where to split them. In each leaf, we’re going to assign f(x) to be a constant.\n\nCan you guess what value to assign?\n\nConsider the empirical error, using the least squares loss:\n\nRtrain(f) =\nX (yi i −f(xi))2\n\n15",
  "261": "Break it up by leaves. Call the value of f(x) in leaf j by fj since it’s a constant.\n\nX (y f(x 2 i i))\n\nX\n\nRtrain(f) =\n\nX ves j i −\n\n∈\n\nlea\n\nleaf j\n\n=\n(y\n2 =:\nRtrain i fj) j (fj).\n\nX\n\nX\n\nleaf j −\n\nleaves j i∈\n\nlea\n\nves j\n\nTo choose the value of the fj’s so that they minimize Rtrain j , take the derivative, set it to 0. Let |Sj| be the number of examples in leaf j.\n\nd\n0 =\n˜ df i∈\n\nX ˜ (yi f)2\n˜f=fj\nleaf j −\n\n!\n\nX\n\nyi i −|Sj| ˜f\n\n!",
  "262": "the number of examples in leaf j.\n\nd\n0 =\n˜ df i∈\n\nX ˜ (yi f)2\n˜f=fj\nleaf j −\n\n!\n\nX\n\nyi i −|Sj| ˜f\n\n!\n\n= −2\nX (yi i −˜f)\n\n= −2\n˜f=fj\n\n˜f=fj\n1\nfj =\nX\nyi = y¯\n\n|Sj| Sj,\n\ni∈leaf j\n\nwhere y¯Sj is the sample average of the labels for leaf j’s examples.\n\nSo now we know what value to assign for f in each leaf. How to split? Greedily want attribute A and split point s solving the following.\n\n\n\n\n\nmin (yi\nC1)2 + min\n(yi C2)2 .",
  "263": "dily want attribute A and split point s solving the following.\n\n\n\n\n\nmin (yi\nC1)2 + min\n(yi C2)2 . A, s C1 x\n\nX\n\nX\n\nC2 leaf| (A) − −\n\nmin\n\n\n\n( i leaf for each attribute A ∈{\n|x A)>s\ndo }\n\ni∈{ x ≤s} x\n\na\n\nlinesearch over s\n\nThe first term means that we’ll choose the optimal C1 = y¯{leaf|x(A)≤s . The second\n}\nterm means we’ll choose C2 = y¯{leaf x(A)>s .\n| }\n\nFor pruning, again CART does minimal cost complexity pruning:\n\ni∈S\n−y¯Sj)2 + C[# leaves in tree]\n\nX\n\nX yi\n\ncost =\n(\n\nves j x\n\nlea\n\nj\n\n16",
  "264": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "265": "Logistic Regression\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nThanks to Ashia Wilson Credit: J.S. Cramer’s “The Origin of Logistic Regression”\n\nOrigins: 19th Century.\n\n• Studying growth of populations and the course of chemical reactions using\n\nd W(t) = βW(t)\ndt ⇒\nW(t) = Aeβt\n\nwhich is a good model for unopposed growth, like the US population’s growth at the time.\n\n• Adolphe Quetelet (1796 - 1874), Belgian astronomer turned statistician,\nknew it produced impossible values and asked his pupil Pierre-Fran¸cois ",
  "266": "onomer turned statistician,\nknew it produced impossible values and asked his pupil Pierre-Fran¸cois Verhulst (1804-1849) to help him work on a more realistic model. They chose\n\nd W(t) = βW(t) −Φ(W(t))\ndt\n\nto resist further growth, and with the choice of Φ to be a quadratic function, they got:\n\nd W(t) = βW(t)(Ω\ndt −W(t)),\n\nW(t)\nwhere Ωis the saturation limit of W. Writing P(t) =\nas the proportion of Ω saturation limit:\nd P(t) = βP(t)(1 −P(t))\ndt\ne(α+βt)\nP(t) = 1 + e(α+βt)\n\nwhere α is the constant from the in",
  "267": "on limit:\nd P(t) = βP(t)(1 −P(t))\ndt\ne(α+βt)\nP(t) = 1 + e(α+βt)\n\nwhere α is the constant from the integration. Verhulst’s “logistic” function has values between 0 and 1.\n\nDraw the logistic function\n\n1",
  "268": "He published in 3 papers between 1838 and 1847. The first paper demonstrated that the curve agrees very well with the actual course of the population in France, Belgium, Essex, and Russia for periods up to 1833. He did not say how he fitted\nthe curves. In the second paper he tried to fit the logistic to 3 points using 20-30\nyears of data (which in general is a not a great way to get a good model). His estimates of the limiting population Ωof 6.6 million for Belgium and 40 million\nfor France were a little of",
  "269": "es of the limiting population Ωof 6.6 million for Belgium and 40 million\nfor France were a little off- these populations are now 11 million for Belgium\nand 65 million for France. In another paper, the estimate was corrected, and they estimated 9.5 million for Belgium (pretty close!).\n\nVerhulst died young in poor heath, and his work was forgotten about, but reinvented in 1920 by Raymond Pearl and Lowell Reed who were studying population growth of the US. They also tried to fit the logistic function to popula",
  "270": "ho were studying population growth of the US. They also tried to fit the logistic function to population growth, and estimated Ωfor the US to be 197 million (the current population is 312 million). Actually, Pearl and collaborators spent 20 years applying the logistic growth curve to almost any living population (fruit flies, humans in North Africa, cantaloupes).\n\nVerhulst’s work was rediscovered just after Pearl and Reed’s first paper in 1920, but they didn’t acknowledge him in their second paper, and only",
  "271": "earl and Reed’s first paper in 1920, but they didn’t acknowledge him in their second paper, and only in a footnote in a third paper (by Pearl) in 1922. They cited him in 1923, but didn’t use his terminology and called his papers “long since forgotten.” The name logistic was revived by Yule, in a presidential address to the Royal Statistical Society in 1925.\n\nThere was a lot of debate over whether the logistic function could replace the cdf of the normal distribution. The person who really showed this could ",
  "272": " function could replace the cdf of the normal distribution. The person who really showed this could happen was\nJoseph Berkson (1899-1982), who was the chief statistician at the Mayo Clinic,\nand who was a collaborator of Reed’s. But because of his personality and attacks on the method of maximum likelihood, controversy ensued... it took until around the 1960’s to resolve it!\n\nLet’s start with a probabilistic model. Assume x ∈Rn, and that x is deterministic (chosen, not random):\n\nY ∼Bernoulli(P(Y = 1|x)).\n\n2",
  "497": "ansactions containing it,\n\nm\nSupp(bananas, cherries, elderberries) =\nX Mi,2 · Mi,3\ni=1\n· Mi,5.\n\n• Confidence of rule a →b: the fraction of times itemset b is purchased when itemset a is purchased.\n\nSupp(a ∪b) #times a and b are purchased\nConf(a →b) =\n=\nSup",
  "273": "Here Y takes either 0 or 1, but we need a ±1 for classification. I’ll write ˜0 for\nnow to stand for either 0 or −1. The model is:\n\nP(Y = 1\nln\n|x, λ)\n= λTx\nP(Y = ˜0|x, λ)\n\n|\n\n{z\n\n}\n\n“odds ratio” Why does this model make an\n\nWe want to make a linear combination of feature values, like in regular regression, which explains the right hand side. But that can take any real values. We need to turn it into a model for\nP(Y = 1|x, λ), which takes values only between 0 and 1. The odds ratio turns\nthose probabilities i",
  "274": "r\nP(Y = 1|x, λ), which takes values only between 0 and 1. The odds ratio turns\nthose probabilities into positive real numbers, then the log turns it into any real numbers.\n\ny sense\n\nat all?\n\nP(Y = 1|x, λ) = eλT x\nP(Y = ˜0|x, λ)\n\nT\n(Y = 1|\nT P\nx, λ) = eλ xP(Y = ˜0|x, λ) = eλ x(1 −P(Y = 1|x, λ))\n\nP(Y = 1|\nT\nx, λ)\nh\n1 + eλ xi\n= eλT x\n\neλT x\nP(Y = 1|x, λ) = 1 + eλT\nlogistic x ← function\n\nWe’ll use maximum likelihood estimation. We choose parameters λ to maximize the likelihood of the data given the model.\n\nm ii",
  "275": "ood estimation. We choose parameters λ to maximize the likelihood of the data given the model.\n\nm iid\nL(λ) := P(Y1 = y1, . . . , Ym = ym|λ, x1, . . . , xm) =\nY\nP(Yi = yi\ni=1\n|λ, xi).\n\nChoose\nλ∗∈argmax L(λ) = argmax log L(λ)\nλ\nλ\nlog-likelihood\nwhere we take the log for convenience since it doesn’t\n\n| {z\n\n}\n\neffect\n\nthe argmax.\n\nBut first, we’ll simplify. We need P(Y = yi|λ, xi) for both yi = 1 and yi = −1.\n \n\nT \n\nλ\nx\nyi = 1,\nneed P(Y = 1|λ, x\n1\ni) := pi =\ne i 1 T T T\n1+eλ\nx =\n=\ni\n1+e−λ\nxi\n1+e−y λ\nx i i T",
  "276": "\n\nT \n\nλ\nx\nyi = 1,\nneed P(Y = 1|λ, x\n1\ni) := pi =\ne i 1 T T T\n1+eλ\nx =\n=\ni\n1+e−λ\nxi\n1+e−y λ\nx i i T T If  \nyi = −\nλ\nx\nλ\nx 1 (i.e., ˜0),\nneed P(Y = ˜0|λ, xi) = 1 −p = 1+e\ni T T\nλ\nx − e i i\nλ\nx =\n1 T\n1+e\ni\n1+e\ni\n1+eλ\nxi\n=\n1\n1+e−\nT\ny λ\nx . i i \n\n3",
  "277": "So, we can just write\n\n1\nP(Y = yi|λ, xi) = 1 + e−y λT\n. i xi\n\nThen,\n\nλ∗∈argmax log L(λ)\nλ\nm\n= argmax\nX 1\nlog 1 + e−yiλT x\nλ\ni\ni=1\nm\n= argmin\nλ\n\nX\nlog(1 + e−yiλT xi).\n\ni=1\n\nThis agrees with the “frequentist” derivation we had before.\n\nThe loss is convex in λ so we can minimize by gradient descent.\n\n4",
  "278": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "279": "Rule Mining and the Apriori Algorithm\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nThe Apriori algorithm - often called the “first thing data miners try,” but somehow doesn’t appear in most data mining textbooks or courses!\n\nStart with market basket data:\n\nSome important definitions:\n\n• Itemset: a subset of items, e.g., (bananas, cherries, elderberries), indexed by {2, 3, 5}.\n\n• Support of an itemset: number of transactions containing it,\n\nm\nSupp(bananas, cherries, elderberries) =\nX Mi,2 · Mi,3\ni=1\n· Mi,5.\n\n• Confidence of rule a →b: the fraction of times itemset b is purchased when itemset a is purchased.\n\nSupp(a ∪b) #times a and b are purchased\nConf(a →b) =\n=\nSupp(a) #times a is purchased ˆ\n= P(b|a).\n\n1",
  "280": "We want to find all strong rules. These are rules a →b such that:\n\nSupp(a ∪b) ≥θ, and Conf(a →b) ≥minconf.\n\nHere θ is called the minimum support threshold.\n\nThe support has a monotonicity property called downward closure:\n\nIf Supp(a ∪b) ≥θ then Supp(a) ≥θ and Supp(b) ≥θ.\n\nThat is, if a ∪b is a frequent item set, then so are a and b.\n\nSupp(a ∪b) = #times a and b are purchased\n≤#times a is purchased = Supp(a).\n\nApriori finds all frequent itemsets (a such that Supp(a) ≥θ).\nWe can use Apriori’s result to get all strong rules a →b as follows:\n\n• For each frequent itemset ℓ:\n\n– Find all nonempty subsets of ℓ\n\n– For each subset a, output a →{ℓ\\ a} whenever\n\nSupp(ℓ) Supp(a) ≥minconf.\n\nNow for Apriori. Use the downward closure property: generate all k-itemsets\n(itemsets of size k) from (k −1)-itemsets. It’s a breadth-first-search.\n\n2",
  "281": "Example:\nθ = 10\n\nerries\n\nbananas\n\ncherries\n\nes grap\n\napples\n\nelderb\n\n1-itemsets:\na b c d e\n/f\ng supp: 25 20 30 45 29 5 17\n\n2-itemsets:\n\n{ } {\n\na,b a,c} { } {a,e}\n\na,d ... { e,g} supp: 7 25 15 23 3\n\n3-itemsets: {a,c,d}\n{a,c,e} {b,d,g} ... supp: 15 22 15\n\n4-itemsets: {a,c,d,e}\nsupp: 12\n\nApriori Algorithm:\n\nInput: Matrix M\n\nL1={frequent 1-itemsets; i such that Supp(i) ≥θ}.\n\nFor k = 2, while Lk−1 = ∅(while there are large k −1-itemsets), k + +\n\n̸\n\n• Ck = apriori gen(Lk\n1) generate candidate itemsets of size k −\n• Lk = {c : c ∈Ck, Supp(c) ≥θ} frequent itemsets of size k (loop over\ntransactions, scan the database)\n\nend\n\nS\n\nOutput: k Lk.\n\n3",
  "282": "The subroutine apriori gen joins Lk−1 to Lk−1.\n\napriori gen Subroutine:\n\nInput: Lk−1\n\nFind all pairs of itemsets in Lk−1 where the first k −2 items are identical.\n\ntoo big Union them (lexicographically) to get Ck ,\n\ne.g.,{a, b, c, d, e, f}, {a, b, c, d, e, g} →{a, b, c, d, e, f, g}\n\nPrune: Ck = {\ntoo big c ∈Ck\n, all (k −1)-subsets cs of c obey cs ∈Lk−1}.\n\nOutput: Ck.\n\nExample of Prune step: consider {a, b, c, d, e, f, g} too big which is in Ck , and I want to know whether it’s in Ck. Look at {a, b, c, d, e, f, g}, {a, b, c, d, e, f, g},{a, b, c, d, e, f, g}, {a, b, c, d, e, f, g}, etc. If any are not in L6, then prune {a, b, c, d, e, f, g} from L7.\n\nExample of the aprior\n\ni a lgori\n\nthm.\n\n4",
  "283": "• Apriori scans the database at most how many times?\n\n• Huge number of candidate sets.\n\n/\n\n•\nwned huge number of apriori-like papers.\n\nSpa\n\nWhat do you do with the rules after they’re generated?\n\n• Information overload (give up)\n\n• Order rules by “interestingness”\n\n– Confidence Supp(a\nˆP(b|a) =\n∪b) Supp(a)\n\n– “Lift”/“Interest”\nˆP(b|a) Supp(b)\n=\nˆP(b) −Supp(a b 1 ∪) Supp(a) :\n\n– Hundreds!\n\nResearch questions:\n\n• mining more than just itemsets (e.g, sequences, trees, graphs)\n\n• incorporating taxonomy in items\n\n• boolean logic and “logical analysis of data”\n\n• Cynthia’s questions: Can we use rules within ML to get good predictive models?\n\n5",
  "285": "R for Machine Learning\n\nAllison Chang\n\n1 Introduction\n\nIt is common for today’s scientific and business industries to collect large amounts of data, and the ability to analyze the data and learn from it is critical to making informed decisions. Familiarity with software such as R allows users to visualize data, run statistical tests, and apply machine learning algorithms. Even if you already know other software, there are still good reasons to learn R:\n\n1. R is free. If your future employer does not already have R installed, you can always download it for free, unlike other proprietary software packages that require expensive licenses. No matter where you travel, you can have access to R on your computer.\n\n2. R gives you access to cutting-edge technology. Top researchers develop statistical learning methods\nin R, and new algorithms are constantly added to the list of packages you can download.\n\n3. R is a useful skill. Employers that value analytics recognize R as useful and important. ",
  "286": "l learning methods\nin R, and new algorithms are constantly added to the list of packages you can download.\n\n3. R is a useful skill. Employers that value analytics recognize R as useful and important. If for no other reason, learning R is worthwhile to help boost your r´esum´e.\n\nNote that R is a programming language, and there is no intuitive graphical user interface with buttons you can click to run different methods. However, with some practice, this kind of environment makes it easy to quickly code scripts and functions for various statistical purposes. To get the most out of this tutorial, follow the examples\nby typing them out in R on your own computer. A line that begins with > is input at the command prompt. We\ndo not include the output in most cases, but you should try out the commands yourself and see what happens. If you type something at the command line and decide not to execute, press the down arrow to clear the line; pressing the up arrow gives you the previous executed co",
  "287": "nds yourself and see what happens. If you type something at the command line and decide not to execute, press the down arrow to clear the line; pressing the up arrow gives you the previous executed command.\n\n1.1 Getting Started\n\nThe R Project website is http://www.r-project.org/. In the menu on the left, click on CRAN under “Download,\nPackages.” Choose a location close to you. At MIT, you can go with University of Toronto under Canada. This leads you to instructions on how to download R for Linux, Mac, or Windows.\n\nOnce you open R, to figure out your current directory, type getwd(). To change directory, use setwd (note that the “C:” notation is for Windows and would be different on a Mac):\n\n> setwd(\"C:\\\\Datasets\")\n\n1.2 Installing and loading packages\n\nFunctions in R are grouped into packages, a number of which are automatically loaded when you start R. These include “base,” “utils,” “graphics,” and “stats.” Many of the most essential and frequently used functions come in these packages",
  "288": "es, a number of which are automatically loaded when you start R. These include “base,” “utils,” “graphics,” and “stats.” Many of the most essential and frequently used functions come in these packages. However, you may need to download additional packages to obtain other useful functions. For example, an important classification method called Support Vector Machines is contained in a package called\n\n1",
  "289": "“e1071.” To install this package, click “Packages” in the top menu, then “Install package(s)...” When asked to select a CRAN mirror, choose a location close to you, such as “Canada (ON).” Finally select “e1071.” To load the package, type library(e1071) at the command prompt. Note that you need to install a package only once, but that if you want to use it, you need to load it each time you start R.\n\n1.3 Running code\n\nYou could use R by simply typing everything at the command prompt, but this does not easily allow you to save, repeat, or share your code. Instead, go to “File” in the top menu and click on “New script.” This opens up a new window that you can save as a .",
  "290": "and prompt, but this does not easily allow you to save, repeat, or share your code. Instead, go to “File” in the top menu and click on “New script.” This opens up a new window that you can save as a .R file. To execute the code you type into this window, highlight the lines you\nwish to run, and press Ctrl-R on a PC or Command-Enter on a Mac. If you want to run an entire script, make\nsure the script window is on top of all others, go to “Edit,” and click “Run all.” Any lines that are run appear in red at the command prompt.\n\n1.4 Help in R\n\nThe functions in R are generally well-documented. To find documentation for a particular function, type ?\nfollowed directly by the function name at the command prompt. For example, if you need help on the “sum” function, type ?",
  "291": "rally well-documented. To find documentation for a particular function, type ?\nfollowed directly by the function name at the command prompt. For example, if you need help on the “sum” function, type ?sum. The help window that pops up typically contains details on both the input and output for the function of interest. If you are getting errors or unexpected output, it is likely that your input is insufficient or invalid, so use the documentation to figure out the proper way to call the function.\n\nIf you want to run a certain algorithm but do not know the name of the function in R, doing a Google search of R plus the algorithm name usually brings up information on which function to use.\n\n2 Datasets\n\nWhen you test any machine learning algorithm, you should use a variety of datasets. R conveniently comes with its own datasets, and you can view a list of their names by typing data() at the command prompt. For instance, you may see a dataset called “cars.” Load the data by typing data(cars)",
  "292": "iently comes with its own datasets, and you can view a list of their names by typing data() at the command prompt. For instance, you may see a dataset called “cars.” Load the data by typing data(cars), and view the data by typing cars.\n\nAnother useful source of available data is the UCI Machine Learning Repository, which contains a couple hundred datasets, mostly from a variety of real applications in science and business. The repository is located at\nhttp://archive.ics.uci.edu/ml/datasets.html. These data are often used by machine learning researchers\nto develop and compare algorithms. We have downloaded a number of datasets for your use, and you can find the text files\n\n.",
  "293": "/ml/datasets.html. These data are often used by machine learning researchers\nto develop and compare algorithms. We have downloaded a number of datasets for your use, and you can find the text files\n\n. These\n\ninclude:\n\nin the Datasets section\n\nName Rows Cols Data Iris 150 4 Real Wine 178 13 Integer, Real Haberman’s Survival 306 3 Integer Housing 506 14 Categorical, Integer, Real Blood Transfusion Service Center 748 4 Integer Car Evaluation 1728 6 Categorical Mushroom 8124 119 Binary\nPen-based Recognition of Handwritten Digits\n10992 16 Integer\n\n2",
  "294": "You are encouraged to download your own datasets from the UCI site or other sources, and to use R to study the data. Note that for all except the Housing and Mushroom datasets, there is an additional class attribute column that is not included in the column counts. Also note that if you download the Mushroom dataset from the UCI site, it has 22 categorical features; in our version, these have been transformed into 119 binary features.\n\n3 Basic Functions\n\nIn this section, we cover how to create data tables, and analyze and plot data. We demonstrate by example how to use various functions. To see the value(s) of any variable, vector, or matrix at any time, simply enter its name in the command line; you are encouraged to do this often until you feel comfortable with how each data structure\nis being stored. To see all the objects in your workspace, type ls(). Also note that the arrow operator <- sets\nthe left-hand side equal to the right-hand side, and that a comment begins with #.\n\n3.1 Cr",
  "295": " being stored. To see all the objects in your workspace, type ls(). Also note that the arrow operator <- sets\nthe left-hand side equal to the right-hand side, and that a comment begins with #.\n\n3.1 Creating data\n\nTo create a variable x and set it equal to 1, type x <- 1. Now suppose we want to generate the vector [1, 2, 3, 4, 5],\nand call the vector v. There are a couple different ways to accomplish this:\n\n> v <- 1:5\n> v <- c(1,2,3,4,5)\n# c can be used to concatenate multiple vectors\n> v <- seq(from=1,to=5,by=1)\n\nThese can be row vectors or column vectors. To generate a vector v0 of six zeros, use either of the following. Clearly the second choice is better if you are generating a long vector.\n\n> v0 <- c(0,0,0,0,0,0)\n> v0 <- seq(from=0,to=0,length.out=6)\n\nWe can combine vectors into matrices using cbind and rbind. For instance, if v1, v2, v3, and v4 are vectors of the same length, we can combine them into matrices, using them either as columns or as rows:\n\n> v1 <- c(1,2,3,4,5)\n> v2 <- ",
  "296": "es using cbind and rbind. For instance, if v1, v2, v3, and v4 are vectors of the same length, we can combine them into matrices, using them either as columns or as rows:\n\n> v1 <- c(1,2,3,4,5)\n> v2 <- c(6,7,8,9,10)\n> v3 <- c(11,12,13,14,15)\n> v4 <- c(16,17,18,19,20)\n> cbind(v1,v2,v3,v4)\n> rbind(v1,v2,v3,v4)\n\nAnother way to create the second matrix is to use the matrix function to reshape a vector into a matrix of the right dimensions.\n\n> v <- seq(from=1,to=20,by=1)\n> matrix(v, nrow=4, ncol=5)\n\nNotice that this is not exactly right—we need to specify that we want to fill in the matrix by row.\n\n> matrix(v, nrow=4, ncol=5, byrow=TRUE)\n\nIt is often helpful to name the columns and rows of a matrix using colnames and rownames. In the following, first we save the matrix as matrix20, and then we name the columns and rows.\n\n3",
  "297": "> matrix20 <- matrix(v, nrow=4, ncol=5, byrow=TRUE)\n> colnames(matrix20) <- c(\"Col1\",\"Col2\",\"Col3\",\"Col4\",\"Col5\")\n> rownames(matrix20) <- c(\"Row1\",\"Row2\",\"Row3\",\"Row4\")\n\nYou can type colnames(matrix20)/rownames(matrix20) at any point to see the column/row names for matrix20.\nTo access a particular element in a vector or matrix, index it by number or by name with square braces:\n\n> v[3]\n# third element of v\n> matrix20[,\"Col2\"]\n# second column of matrix20\n> matrix20[\"Row4\",]\n# fourth row of matrix20\n> matrix20[\"Row3\",\"Col1\"]\n# element in third row and first column of matrix20\n> matrix20[3,1]\n# element in third row and first column of matrix20\n\nYou can find the length of a vector or number of rows or columns in a matrix using length, nrow, and ncol.\n\n> length(v1)\n> nrow(matrix20)\n> ncol(matrix20)\n\nSince you will be working with external datasets, you will need functions to read in data tables from text files. For instance, suppose you wanted to read in the Haberman’s Survival dataset (from",
  "298": "20)\n\nSince you will be working with external datasets, you will need functions to read in data tables from text files. For instance, suppose you wanted to read in the Haberman’s Survival dataset (from the UCI Repository). Use the read.table function:\n\ndataset <- read.table(\"C:\\\\Datasets\\\\haberman.csv\", header=FALSE, sep=\",\")\n\nThe first argument is the location (full path) of the file. If the first row of data contains column names, then the\nsecond argument should be header = TRUE, and otherwise it is header = FALSE. The third argument contains\nthe delimiter. If the data are separated by spaces or tabs, then the argument is sep = \" \" and sep = \"\\t\"\nrespectively. The default delimiter (if you do not include this argument at all) is “white space” (one or more spaces, tabs, etc.). Alternatively, you can use setwd to change directory and use only the file name in the read.table function. If the delimiter is a comma, you can also use read.csv and leave off the sep argument:\n\ndataset <- read.",
  "299": "y, you can use setwd to change directory and use only the file name in the read.table function. If the delimiter is a comma, you can also use read.csv and leave off the sep argument:\n\ndataset <- read.csv(\"C:\\\\Datasets\\\\haberman.csv\", header=FALSE)\n\nUse write.table to write a table to a file. Type ?write.table to see details about this function. If you need to write text to a file, use the cat function.\n\nA note about matrices versus data frames: A data frame is similar to a matrix, except it can also include\nnon-numeric attributes. For example, there may be a column of characters. Some functions require the data\npassed in to be in the form of a data frame, which would be stated in the documentation. You can always coerce a matrix into a data frame using as.data.frame. See Section 3.6 for an example.\n\nA note about factors: A factor is essentially a vector of categorical variables, encoded using integers. For instance, if each example in our dataset has a binary class attribute, say 0 or ",
  "498": "b) #times a and b are purchased\nConf(a →b) =\n=\nSupp(a) #times a is purchased ˆ\n= P(b|a).\n\n1",
  "642": "j). We choose Rreg(f) to be either:\n\nP\n\n2 =\nj λj\n\n2 ⇐= ridge regression, SVM\n\nP\n\n• IλI2\n\n• IλI1 =\n|λj| ⇐= LASSO, approximately AdaBoost\nj\n\nP\n\n7",
  "300": " example.\n\nA note about factors: A factor is essentially a vector of categorical variables, encoded using integers. For instance, if each example in our dataset has a binary class attribute, say 0 or 1, then that attribute can be represented as a factor. Certain functions require one of their arguments to be a factor. Use as.factor to encode a vector as a factor. See Sections 4.5 and 4.9 for examples.\n\n3.2 Sampling from probability distributions\n\nThere are a number of functions for sampling from probability distributions. For example, the following commands\ngenerate random vectors of the user-specified length n from distributions (normal, exponential, poisson, uniform,\nbinomial) with user-specified parameters. There are other distributions as well.\n\n4",
  "301": "> norm_vec <- rnorm(n=10, mean=5, sd=2)\n> exp_vec <- rexp(n=100, rate=3)\n> pois_vec <- rpois(n=50, lambda=6)\n> unif_vec <- runif(n=20, min=1, max=9)\n> bin_vec <- rbinom(n=20, size=1000, prob=0.7)\n\nSuppose you have a vector v of numbers. To randomly sample, say, 25 of the numbers, use the sample function:\n\n> sample(v, size=25, replace=FALSE)\n\nIf you want to sample with replacement, set the replace argument to TRUE. If you want to generate the same random vector each time you call one of the random functions listed above, pick a “seed” for the random number generator using set.seed, for example set.seed(100).\n\n3.3 Analyzing data\n\nTo compute the mean, variance, standard deviation, minimum, maximum, and sum of a set of numbers, use mean, var, sd, min, max, and sum. There are also rowSum and colSum to find the row and column sums for a matrix.\nTo find the component-wise absolute value and square root of a set of numbers, use abs and sqrt. Correlation\nand covariance for two vectors are compu",
  "302": "lSum to find the row and column sums for a matrix.\nTo find the component-wise absolute value and square root of a set of numbers, use abs and sqrt. Correlation\nand covariance for two vectors are computed with cor and cov respectively.\n\nLike other programming languages, you can write if statements, and for and while loops. For instance, here is a simple loop that prints out even numbers between 1 and 10 (%% is the modulo operation):\n\n> for (i in 1:10){\n+ if (i %% 2 == 0){\n+\ncat(paste(i, \"is even.\\n\", sep=\" \"))\n# use paste to concatenate strings\n+\n}\n+ }\n\nThe 1:10 part of the for loop can be specified as a vector. For instance, if you wanted to loop over indices 1, 2, 3, 5, 6, and 7, you could type for (i in c(1:3,5:7)). To pick out the indices of elements in a vector that satisfy a certain property, use which, for example:\n\n> which(v >= 0)\n# indices of nonnegative elements of v\n> v[which(v >= 0)]\n# nonnegative elements of v\n\n3.4 Plotting data\n\nWe use the Haberman’s Survival data (read in",
  "303": "roperty, use which, for example:\n\n> which(v >= 0)\n# indices of nonnegative elements of v\n> v[which(v >= 0)]\n# nonnegative elements of v\n\n3.4 Plotting data\n\nWe use the Haberman’s Survival data (read into data frame dataset) to demonstrate plotting functions. Each row of data represents a patient who had surgery for breast cancer. The three features are: the age of the patient at the time of surgery, the year of the surgery, and the number of positive axillary nodes detected. Here we plot:\n\n1. Scatterplot of the first and third features,\n\n2. Histogram of the second feature,\n\n3. Boxplot of the first feature.\n\nTo put all three plots in a 1 × 3 matrix, use par(mfrow=c(1,3)). To put each plot in its own window, use\nwin.graph() to create new windows.\n\n> plot(dataset[,1], dataset[,3], main=\"Scatterplot\", xlab=\"Age\", ylab=\"Number of Nodes\", pch=20)\n> hist(dataset[,2], main=\"Histogram\", xlab=\"Year\", ylab=\"Count\")\n> boxplot(dataset[,1], main=\"Boxplot\", xlab=\"Age\")\n\n5",
  "304": "Scatterplot Histogram Boxplot\n\n0 10 20 30 40 50\n\n0 10 20 30 40 50 60\n\n30 40 50 60 70 80\n\nNumber of Nodes\n\nCount\n\n30 40 50 60 70 80 58 60 62 64 66 68\n\nAge Year Age\n\nFigure 1: Plotting examples.\n\nThe pch argument in the plot function can be varied to change the marker. Use points and lines to add extra points and lines to an existing plot. You can save the plots in a number of different formats; make sure the plot window is on top, and go to “File” then “Save as.”\n\n3.5 Formulas\n\nCertain functions have a “formula” as one of their arguments. Usually this is a way to express the form of a model. Here is a simple example. Suppose you have a response variable y and independent variables x1, x2, and\nx3. To express that y depends linearly on x1, x2, and x3, you would use the formula y ∼ x1 + x2 + x3, where\ny, x1, x2, and x3 are also column names in your data matrix. See Section 3.6 for an example. Type ?",
  "305": "express that y depends linearly on x1, x2, and x3, you would use the formula y ∼ x1 + x2 + x3, where\ny, x1, x2, and x3 are also column names in your data matrix. See Section 3.6 for an example. Type ?formula for details on how to capture nonlinear models.\n\n3.6 Linear regression\n\nOne of the most common modeling approaches in statistical learning is linear regression. In R, use the lm function to generate these models. The general form of a linear regression model is\n\nY = β0 + β1X1 + β2X2 + · · · + βkXk + ε,\n\nwhere ε is normally distributed with mean 0 and some variance σ2 .",
  "306": "the lm function to generate these models. The general form of a linear regression model is\n\nY = β0 + β1X1 + β2X2 + · · · + βkXk + ε,\n\nwhere ε is normally distributed with mean 0 and some variance σ2 .\n\nLet y be a vector of dependent variables, and x1 and x2 be vectors of independent variables. We want to find\nthe coefficients of the linear regression model Y = β0 + β1X1 + β2X2 + ε. The following commands generate the\nlinear regression model and give a summary of it.\n\n> lm_model <- lm(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)))\n> summary(lm_model)\n\nThe vector of coefficients for the model is contained in lm model$coefficients.\n\n4 Machine Learning Algorithms\n\nWe give the functions corresponding to the algorithms covered in class. Look over the documentation for each function on your own as only the most basic details are given in this tutorial.\n\n6",
  "307": "4.1 Prediction\n\nFor most of the following algorithms (as well as linear regression), we would in practice first generate the model using training data, and then predict values for test data. To make predictions, we use the predict function. To see documentation, go to the help page for the algorithm, and then scroll through the Contents menu on the left side to find the corresponding predict function. Or simply type ?",
  "308": "e predict function. To see documentation, go to the help page for the algorithm, and then scroll through the Contents menu on the left side to find the corresponding predict function. Or simply type ?predict.name, where name is the function corresponding to the algorithm. Typically, the first argument is the variable in which you saved the model, and the second argument is a matrix or data frame of test data. Note that when you call the function, you can just type predict instead of predict.name. For instance, if we were to predict for the linear regression model above, and x1 test and x2 test are vectors containing test data, we can use the command\n\n> predicted_values <- predict(lm_model, newdata=as.data.frame(cbind(x1_test, x2_test)))\n\n4.2 Apriori\n\nTo run the Apriori algorithm, first install the arules package and load it. See Section 1.2 for installing and loading new packages. Here is an example of how to run the Apriori algorithm using the Mushroom dataset. Note that the dataset m",
  "309": "tall the arules package and load it. See Section 1.2 for installing and loading new packages. Here is an example of how to run the Apriori algorithm using the Mushroom dataset. Note that the dataset must be a binary incidence matrix; the column names should correspond to the “items” that make up the “transactions.” The following commands print out a summary of the results and a list of the generated rules.\n\n> dataset <- read.csv(\"C:\\\\Datasets\\\\mushroom.csv\", header = TRUE)\n> mushroom_rules <- apriori(as.matrix(dataset), parameter = list(supp = 0.8, conf = 0.9))\n> summary(mushroom_rules)\n> inspect(mushroom_rules)\n\nYou can modify the parameter settings depending on your desired support and confidence thresholds.\n\n4.3 Logistic Regression\n\nYou do not need to install an extra package for logistic regression. Using the same notation as in Section 3.6, the command is:\n\n> glm_mod <-glm(y ∼ x1+x2, family=binomial(link=\"logit\"), data=as.data.frame(cbind(y,x1,x2)))\n\n4.4\nK-Means Clustering\n\nYou do",
  "310": "ic regression. Using the same notation as in Section 3.6, the command is:\n\n> glm_mod <-glm(y ∼ x1+x2, family=binomial(link=\"logit\"), data=as.data.frame(cbind(y,x1,x2)))\n\n4.4\nK-Means Clustering\n\nYou do not need an extra package. If X is the data matrix and m is the number of clusters, then the command is:\n\n> kmeans_model <- kmeans(x=X, centers=m)\n\n4.5\nk-Nearest Neighbor Classification\n\nInstall and load the class package. Let X train and X test be matrices of the training and test data respectively, and labels be a binary vector of class attributes for the training examples. For k equal to K, the command is:\n\n> knn_model <- knn(train=X_train, test=X_test, cl=as.factor(labels), k=K)\n\nThen knn model is a factor vector of class attributes for the test set.\n\n7",
  "311": "4.6 Na¨ıve Bayes\n\nInstall and load the e1071 package. Using the same notation as in Section 3.6, the command is:\n\n> nB_model <- naiveBayes(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)))\n\n4.7 Decision Trees (CART)\n\nCART is implemented in the rpart package. Again using the formula, the command is\n\n> cart_model <- rpart(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)), method=\"class\")\n\nYou can use plot.rpart and text.rpart to plot the decision tree.\n\n4.8 AdaBoost\n\nThere are a number of different boosting functions in R. We show here one implementation that uses decision trees as base classifiers. Thus the rpart package should be loaded. Also, the boosting function ada is in the ada\npackage. Let X be the matrix of features, and labels be a vector of 0-1 class labels. The command is\n\n> boost_model <- ada(x=X, y=labels)\n\n4.9 Support Vector Machines (SVM)\n\nThe SVM algorithm is in the e1071 package. Let X be the matrix of features, and labels be a vector of 0-1 class\nlabels. Let the regulariz",
  "312": "odel <- ada(x=X, y=labels)\n\n4.9 Support Vector Machines (SVM)\n\nThe SVM algorithm is in the e1071 package. Let X be the matrix of features, and labels be a vector of 0-1 class\nlabels. Let the regularization parameter be C. Use the following commands to generate the SVM model and view details:\n\n> svm_model <- svm(x=X, y=as.factor(labels), kernel =\"radial\", cost=C)\n> summary(svm_model)\n\nThere are a variety of parameters such as the kernel type and value of the C parameter, so check the documentation on how to alter them.\n\n8",
  "313": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "314": "Fundamentals of Learning\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nImportant Problems in Data Mining\n\n1. Finding patterns (correlations) in large datasets\n-e.g. (Diapers → Beer). Use Apriori!\n\n2. Clustering - grouping data into clusters that “belong” together - objects\nwithin a cluster are more similar to each other than to those in other clusters.\n\n• Kmeans, Kmedians\n\ni=1, xi ∈X ⊂ Rn\n\n• Input: {xi}m\n\n• Output: f : X →{1, . . . , K} (K clusters)\n\n• clustering consumers for market research, clustering genes into families, image segmentation (medical imaging)\n\n3. Classification\n\n• Input: {(xi, yi)}m “examples,” “instances with labels,” “observations”\ni=1\n• xi ∈X , yi ∈ {−1, 1} “binary”\n\n• Output: f : X → R and use sign(f) to classify.\n\n• automatic handwriting recognition, speech recognition, biometrics, doc­ ument classification\n\n• “LeNet”\n\n4. Regression\n\n1\n\n[IMAGE_OCR 1]\n3 +\n\n+ + o\n\nre -\n. SE",
  "315": "• Input: {(xi, yi)}m\n\ni=1, xi ∈X , yi ∈ R\n\n• Output: f : X → R\n\n• predicting an individual’s income, predict house prices, predict stock prices, predict test scores\n\n5. Ranking (later) - in between classification and regression. Search engines\nuse ranking methods\n\n6. Density Estimation - predict conditional probabilities\n\ni=1, xi ∈X , yi ∈ {−1, 1}\n\n• {(xi, yi)}m\n\n• Output: f : X → [0, 1] as “close” to P (y = 1|x) as possible.\n\n• estimate probability of failure, probability to default on loan\n\nRule mining and clustering are unsupervised methods (no ground truth), and classification, ranking, and density estimation are supervised methods (there is ground truth). In all of these problems, we do not assume we know the distribution that the data are drawn from!\n\nTraining and Testing (in-sample and out-of-sample) for supervised learning\n\nTraining: training data are input, and model f is the output.\n\ni=1 =⇒ Algorithm =⇒ f.\n\n{(xi, yi)}m\n\nTesting: You want to predict y for a new x, where (x, y)",
  "316": "and out-of-sample) for supervised learning\n\nTraining: training data are input, and model f is the output.\n\ni=1 =⇒ Algorithm =⇒ f.\n\n{(xi, yi)}m\n\nTesting: You want to predict y for a new x, where (x, y) comes from the same distribution as {(xi, yi)}m\n\ni=1.\n\nThat is, (x, y) ∼ D(X , Y) and each (xi, yi) ∼ D(X , Y).\n\nCompute f(x) and compare it to y. How well does f(x) match y? Measure goodness of f using a loss function R : Y × Y → R:\n\nRtest(f) = E(x,y)∼DR(f(x), y)\n\n=\nR(f(x), y)dD(x, y). (x,y)∼D\n\n2",
  "317": "Rtest is also called the true risk or the test error.\n\nCan we calculate Rtest?\n\nWe want Rtest to be small, to indicate that f(x) would be a good predictor (“es­ timator”) of y.\n\nFor instance\n\nR(f(x), y) = (f(x) − y)2\nleast squares loss, or\nR(f(x), y) = 1[sign(f(x))\n(mis)classification error\n=y]\n\nWhich problems might these loss functions correspond to?\n\nHow can we ensure Rtest(f) is small?\n\nLook at how well f performs (on average) on {(xi, yi)}i.\n\nm m 1\nRtrain(f) =\nR(f(xi), yi).\nm i=1\nRtrain is also called the empirical risk or training error. For example,\n\nm m 1\nRtrain(f) =\n=yi].\n1[sign(f(xi))\nm i=1\n\n(How many handwritten digits did f classify incorrectly?)\n\nSay our algorithm constructs f so that Rtrain(f) is small. If Rtrain(f) is small, hopefully Rtest(f) is too.\n\nWe would like a guarantee on how close Rtrain is to Rtest . When would it be close to Rtest?\n\n• If m is large.\n\n• If f is “simple.”\n\n3",
  "318": "Illustration\n\nIn one of the figures in the illustration, f:\n\n• was overfitted to the data\n\n• modeled the noise\n\n• “memorized” the examples, but didn’t give us much other useful information\n\n• doesn’t “generalize,” i.e., predict. We didn’t “learn” anything!\n\nComputational Learning Theory, a.k.a. Statistical Learning Theory, a.k.a., learning theory, and in particular, Vapnik’s Structural Risk Minimization (SRM) addresses generalization. Here’s SRM’s classic picture:\n\nWhich is harder to check for, overfitting or underfitting?\n\n4",
  "319": "Computational learning theory addresses how to construct probabilistic guaran­ tees on the true risk. In order to do this, it quantifies the class of “simple models.”\n\nBias/Variance Tradeoff is related to learning theory (actually, bias is related to\nlearning theory).\n\nInference Notes - Bias/Variance Tradeoff\n\n5",
  "320": "Regularized Learning Expression\n\nStructural Risk Minimization says that we need some bias in order to learn/generalize\n(avoid overfitting). Bias can take many forms:\n\n• “simple” models f(x) =\nλjx(j) where IλI2 =\nλ2 < C\nj 2 j j\n\n• “prior” in Bayesian statistics\n\n• connectivity of neurons in the brain\n\nRegularized Learning Expression:\n\nm\n\nR(f(xi), yi) + CRreg(f)\n\ni This expression is kind of omnipresent. This form captures many algorithms: SVM, boosting, ridge regression, LASSO, and logistic regression.\n\nIn the regularized learning expression, the loss R(f(xi), yi) could be:\n\n• “least squares loss” (f(xi) − yi)2\n\n• “misclassification error” 1[yi\nf(xi))] = 1[yif(xi)≤0]\n\n– Note that minimizing 1[yif(xi)≤0] is computationally hard. i\n\n̸=sign(\n\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\n−yif(xi)\n• “logistic loss” log2 1 + e\n⇐= logistic regression\n\n6\n\n[IMAGE_OCR 1]\n35 — tj <0)\nexp)\n3 04,1 +exp yo)\n—— max(0,1-¥\"09)\n25 2 15 1\nu = SS\n==)\no\n-2 AS A -0.5 o 0.5 1 15 2\ny",
  "321": "cw.mit.edu/fairuse.\n\n−yif(xi)\n• “logistic loss” log2 1 + e\n⇐= logistic regression\n\n6\n\n[IMAGE_OCR 1]\n35 — tj <0)\nexp)\n3 04,1 +exp yo)\n—— max(0,1-¥\"09)\n25 2 15 1\nu = SS\n==)\no\n-2 AS A -0.5 o 0.5 1 15 2\ny* F090.",
  "322": "• “hinge loss” max(0, 1 − yif(xi)) ⇐= SVM\n\n−yif(xi) ⇐ • “exponential loss” e\n= AdaBoost\n\nIn the regularized learning expression, we define a couple of options for Rreg(f).\nUsually f is linear, f(x) =\nj λjx(j). We choose Rreg(f) to be either:\n\nP\n\n2 =\nj λj\n\n2 ⇐= ridge regression, SVM\n\nP\n\n• IλI2\n\n• IλI1 =\n|λj| ⇐= LASSO, approximately AdaBoost\nj\n\nP\n\n7",
  "323": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "324": "Bias/Variance Tradeoff\n\nA parameter is some quantity about a distribution that we would like to know.\nWe’ll estimate the parameter θ using an estimator θˆ. The bias of estimator θˆ for\nparameter θ is defined as:\n\n• Bias(θˆ,θ) := E(θˆ) − θ, where the expectation is with respect to the distribution\nthat θˆ is constructed from.\n\nAn estimator whose bias is 0 is called unbiased. Contrast bias with:\n\n• Var(θˆ) = E(θˆ− E(θˆ))2 .",
  "325": " := E(θˆ) − θ, where the expectation is with respect to the distribution\nthat θˆ is constructed from.\n\nAn estimator whose bias is 0 is called unbiased. Contrast bias with:\n\n• Var(θˆ) = E(θˆ− E(θˆ))2 .\n\nFigure s howing the f our comb inations of l ow and hi gh bias, and low and high variance.\n\nOf course, we’d like an estimator with low bias and low variance.\n\nA little bit of decision theory\n\n(The following is based on notes of David McAllester.)\n\nLet’s say our data come from some distribution D on X × Y, where Y ⊂ R. Usually we don’t know D (we instead only have data) but for the moment, let’s say we know it. We want to learn a function f : X →Y.\n\nThen if we could choose any f we want, what would we choose? Maybe we’d choose f to minimize the least squares error:\n\nEx,y∼D[(y − f(x))2].\n\n1",
  "343": "eration assigns each data point\nto its closest center, therefore, the cluster assignment is better:\n\n(t+1)\n(t+1)\n(t) (t) (t) (t) (t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\nOn the second step, each cluster is re-centered at its mean, so the representatives\nare better. By Lemma 1,\n\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\n•\n\nSo does the cost converge?\n\n4",
  "326": "It turns out that the f ∗ that minimizes the above error is the conditional expec­ tation!\n\nDraw a picture\n\nProposition.\n\nf ∗ (x) = Ey[y|x].\n\nProof. Consider each x separately. For each x there’s a marginal distribution on y. In other words, look at Ey[(y − f(x))2|x] for each x. So, pick an x. For this x, define ¯y to be Ey[y|x]. Now,\n\nEy[(y − f(x))2|x]\n= Ey[(y − y¯ + ¯y − f(x))2|x]\n= Ey[(y − y¯)2|x] + Ey[(¯y − f(x))2|x] + 2Ey[(y − y¯)(¯y − f(x))|x]\n= Ey[(y − y¯)2|x] + (¯y − f(x))2 + 2(¯y − f(x))Ey[(y − y¯)|x]\n= Ey[(y − y¯)2|x] + (¯y − f(x))2\n\nwhere the last step follows from the definition of ¯y.\n\nSo how do we pick f(x)? Well, we can’t do anything about the first term, it doesn’t depend on f(x). The best choice of f(x) minimizes the second term,\nwhich happens at f(x) = y¯, where remember ¯y = Ey[y|x].\n\nSo we know for each x what to choose in order to minimize Ey[(y − f(x))2|x]. To complete the argument, note that:\n\nEx,y[(y − f(x))2] = Ex[Ey[(y − f(x))2|x]]\n\nand we have found the minim",
  "327": "¯y = Ey[y|x].\n\nSo we know for each x what to choose in order to minimize Ey[(y − f(x))2|x]. To complete the argument, note that:\n\nEx,y[(y − f(x))2] = Ex[Ey[(y − f(x))2|x]]\n\nand we have found the minima of the inside term for each x. •\n\nNote that if we’re interested instead in the absolute loss Ex,y[|y − f(x)|], it is possible to show that the best predictor is the conditional median, that is,\nf(x) = median[y|x].\n\n2",
  "328": "Back to Bias/Variance Decomposition\n\nLet’s think about a situation where we created our function f using data S. Why would we do that of course?\n\nWe have training set S = (x1, y1), . . . , (xm, ym), where each example is drawn iid\nfrom D. We want to use S to learn a function fS : X →Y.\n\nWe want to know what the error of fS is on average. In other words, we want to know what Ex,y,S[(y−fS(x))2] is. That will help us figure out how to minimize it.\nThis is going to be a neat result - it’s going to decompose into bias and variance\nterms!\n\nFirst, let’s consider some learning algorithm (which produced fS) and its ex­ pected prediction error:\n\nEx,y,S[(y − fS(x))2].\n\nRemember that the estimator fS is random, since it depends on the randomly drawn training data. Here, the expectation is taken with respect to a new ran­ domly drawn point x, y ∼ D and training data S ∼ Dm .",
  "329": "r that the estimator fS is random, since it depends on the randomly drawn training data. Here, the expectation is taken with respect to a new ran­ domly drawn point x, y ∼ D and training data S ∼ Dm .\n\nLet us define the mean prediction of the algorithm at point x to be:\n\n¯f(x) = ES[fS(x)].\n\nIn other words, to get this value, we’d get infinitely many training sets, run the learning algorithm on all of them to get infinite predictions fS(x) for each x.\n\n¯ Then for each x we’d average the predictions to get f(x).\n\nWe can now decompose the error, at a fixed x, as follows:\n\nEy,S[(y − fS(x))2]\n\n= Ey,S[(y − y¯ + ¯y − fS(x))2]\n= Ey(y − y¯)2 + ES(¯y − fS(x))2 + 2Ey,S[(y − y¯)(¯y − fS(x))].\n\nThe third term here is zero, since Ey,S[(y−y¯)(¯y−fS(x))] = Ey(y−y¯)ES(¯y−fS(x)),\nand the first part of that is Ey(y − y¯) = 0.\n\n3",
  "330": "The first term is the variance of y around its mean. We don’t have control over that when we choose fS. This term is zero if y is deterministically related to x.\n\nLet’s look at the second term:\n\nES(¯y − fS(x))2\n\n¯ ¯\n= ES(¯y − f(x) + f(x) − fS(x))2\n\n¯ ¯\n= ES(¯y − f(x))2 + ES(f¯(x) − fS(x))2 + 2ES[(¯y − f(x))(f¯(x) − fS(x))]\n\n¯ ¯ The last term is zero, since (¯y − f(x)) is a constant, and f(x) is the mean of ¯ fS(x) with respect to S. Also the first term isn’t random. It’s (¯y − f(x))2 .\n\nPutting things together, what we have is this (reversing some terms):\n\nEy,S[(y − fS(x))2] = Ey(y − y¯)2 + ES(f¯(x) − fS(x))2 + (¯y − f¯(x))2 .",
  "331": "S. Also the first term isn’t random. It’s (¯y − f(x))2 .\n\nPutting things together, what we have is this (reversing some terms):\n\nEy,S[(y − fS(x))2] = Ey(y − y¯)2 + ES(f¯(x) − fS(x))2 + (¯y − f¯(x))2 .\n\nIn this expression, the second term is the variance of our estimator around its mean. It controls how our predictions vary around its average prediction. The third term is the bias squared, where the bias is the difference between the av­ erage prediction and the true conditional mean.\n\nWe’ve just proved the following:\n\nTheorem.\n\nFor each fixed x, Ey,S[(y − fS(x))2] = vary|x(y) + varS(fS(x)) + bias(fS(x))2 .",
  "332": " difference between the av­ erage prediction and the true conditional mean.\n\nWe’ve just proved the following:\n\nTheorem.\n\nFor each fixed x, Ey,S[(y − fS(x))2] = vary|x(y) + varS(fS(x)) + bias(fS(x))2 .\n\nSo\nEx,y,S[(y − fS(x))2] = Ex[vary|x(y) + varS(fS(x)) + bias(fS(x))2].\n\nThat is the bias-variance decomposition.\n\nThe “Bias-Variance” tradeoff: want to choose fS to balance between reducing the\nsecond and third terms in order to make the lowest MSE. We can’t just minimize one or the other, it needs to be a balance. Sometimes, if you are willing to inject some bias, this can allow you to substantially reduce the variance. E.g., modeling with lower degree polynomials, rather than higher degree polynomials.\n\n4",
  "333": "Question: Intuitively, what happens to the second term if fS fits the data per­ fectly every time (overfitting)?\n\nQuestion: Intuitively, what happens to the last two terms if fS is a flat line every time?\n\nThe bottom line: In order to predict well, you need to strike a balance between bias and variance.\n\n• The variance term controls wiggliness, so you’ll want to choose simple func­ tions that can’t yield predictions that are too varied.\n\n• The bias term controls how close the average model prediction is close to the truth, ¯y. You’ll need to pay attention to the data in order to reduce the bias term.\n\n• Since you can’t calculate either the bias or the variance term, what we usually do is just impose some “structure” into the functions we’re fitting with, so the class of functions we are working with is small (e.g., low degree polynomials). We then try to fit the data well using those functions. Hope­ fully this strikes the right balance of wiggliness (variance) and capturing the mean o",
  "499": "We want to find all strong rules. These are rules a →b such that:\n\nSupp(a ∪b) ≥θ, and Conf(a →b) ≥minconf.\n\nHere θ is called the minimum support threshold.\n\nThe support has a monotonicity property called downward closure:\n\nIf Supp(a ∪b) ≥θ then Supp(a) ≥θ ",
  "334": "rking with is small (e.g., low degree polynomials). We then try to fit the data well using those functions. Hope­ fully this strikes the right balance of wiggliness (variance) and capturing the mean of the data (bias).\n\n• One thing we like to do is make assumptions on the distribution D, or at least on the class of functions that might be able to fit well. Those assumptions each lead to a different algorithm (i.e. model). How well the algorithm works or not depends on how true the assumption is.\n\n• Even when we’re not working with least squares error, we hope a similar idea holds (and will work on proving that later in the course). We’ll use the same type of idea, where we impose some structure, and hope it reduces wiggliness and will still give accurate predictions.\n\nGo back to the other notes!\n\n5",
  "335": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "336": "Clustering\n\nMIT 15.097 Course Notes Cynthia Rudin and S¸eyda Ertekin\n\nCredit: Dasgupta, Hastie, Tibshirani, Friedman\n\nClustering (a.k.a. data segmentation) Let’s segment a collection of examples into “clusters” so that objects within a cluster are more closely related to one another than objects assigned to different clusters. We want to assign each ex­ ample xi to a cluster k ∈{1, ...., K}.\n\nThe K-Means algorithm is a very popular way to do this. It assumes points lie\nin Euclidean space.\n\nn Input: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, .",
  "337": "e K-Means algorithm is a very popular way to do this. It assumes points lie\nin Euclidean space.\n\nn Input: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, ..., zK ) :=\nmin Ixi − zkI2\n\n2. k i The choice of the squared norm is fortuitous, it really helps simplify the math!\n\nn If we’re given points {zk}k, they can induce a Voronoi p artition of R : they break the space into cells where each cell corresponds to one of the zk’s. That is, each cell contains the region of space whose nearest representative is zk.\n\nDraw a picture\n\nWe can look at the examples in each of these regions of space, which are the clusters. Specifically,\n\nCk := {xi : the closest representative to xi is zk}.\n\nLet’s compute the cost another way. Before, we summed over examples, and then picked the right representative zk for each example. This time, we’ll sum over clusters, and look at all the examples in that cluster:\n\ncost(z1, ..., zK ) =\nIxi − zkI2\n\n2.\n\nk {i:xi∈Ck}\n\n1",
  "338": "While we’re analyzing, we’ll need to consider suboptimal partitions of the data, where an example might not be assigned to the nearest representative. So we redefine the cost:\n\nX X\n\nk ∥xi −zk∥2 2. (1)\n\ncost(C1, ..., CK; z1, ..., zK) =\n\n{i:xi∈Ck}\n\nLet’s say we only have one cluster to deal with. Call it C. The representative is z. The cost is then:\ncost(C; z) =\n∥xi −z∥2 2.\n\nX\n\n{ ∈} Where should we place z?\n\ni:xi C\n\nAs you probably guessed, we would put it at the mean of the examples in C. But\nalso, the additional cost incurred by picking z = mean(C) can be characterized\nvery simply:\n\n̸\n\nLemma 1. For any set C ⊂Rn and any z ∈Rn,\n\ncost(C; z) = cost(C, mean(C)) + C\nz mean(C) 2 2. | | · ∥− ∥\n\nLet’s go ahead and prove it. In order to do that, we need to do another biasvariance decomposition (this one’s pretty much identical to one of the ones we did before).\n\nLemma 2. Let X Rn be any random variable. For any z Rn, we have: ∈ ∈\n\nEX∥X −z∥2\n2 = EX∥X −EXX∥2\n2 + ∥z −EXX∥2\n2.\n\nProof. Let x¯ := EXX",
  "339": "s one’s pretty much identical to one of the ones we did before).\n\nLemma 2. Let X Rn be any random variable. For any z Rn, we have: ∈ ∈\n\nEX∥X −z∥2\n2 = EX∥X −EXX∥2\n2 + ∥z −EXX∥2\n2.\n\nProof. Let x¯ := EXX.\n\nj −z(j))2 X\n\nEX∥X −z∥2\n2 = EX\n(X(j)\n\nj\n−x¯(j) + x¯(j) −zj)2\nX\n\n= EX\n(X(j)\n\nj\n−x¯(j))2 + EX\n(x¯(j)\n\nj −zj)2 X X\n\n= EX\n(X(j)\n\nj −x¯(j))(x¯(j) −z(j)) X\n\n+2EX\n(X(j)\n\n= EX∥X −x¯∥2\n2 + EX∥x¯ −z∥2\n2 + 0. ■\n\n2",
  "340": "To prove Lemma 1, pick a specific choice for X, namely X is a uniform random draw from the points xi in set C. So X has a discrete distribution. What will happen with this choice of X is that the expectation will reduce to the cost we already defined above.\n\nX\n\nEX∥X −z∥2\n2 =\n(prob. that point i is chosen)∥xi −z∥2 2 i:xi C\n\n{ ∈} X 1\n=\n\n{i:xi∈C} |C ∥x | i −z∥2 1\n2 =\ncost(C, z) |C| (2)\n\nand if we use Lemma 2 substituting z to be x¯ (a.k.a., EXX, or mean(C)) and simplify as in (2):\n\n1 EX∥X −x¯∥2\n2 =\ncost(C, mean(C)). C | | (3)\n\nWe had already defined cost earlier, and the choice of X was nice because its expectation is just the cost. Let’s recopy Lemma 2’s statement here, using the x¯ notation:\n\nEX∥X −z∥2\n2 = EX∥X −x¯∥2\n2 + ∥z −x¯∥2\n2.\n\nPlugging in (2) and (3),\n\n1 1\ncost(C, z) =\ncost(C, mean(C)) +\n|C| |C ∥z −x¯∥2 | 2. (4)\n\nMultiplying through,\n\ncost(C; z) = cost(C, mean(C)) + |C| · ∥z −mean(C)∥2\n2.\n\nAnd that’s the statement of Lemma 1. ■\n\nTo really minimize the cost (1), you’d need to try ",
  "341": ")) +\n|C| |C ∥z −x¯∥2 | 2. (4)\n\nMultiplying through,\n\ncost(C; z) = cost(C, mean(C)) + |C| · ∥z −mean(C)∥2\n2.\n\nAnd that’s the statement of Lemma 1. ■\n\nTo really minimize the cost (1), you’d need to try all possible assignments of the m data points to K clusters. Uck! The number of distinct assignments is (Jain and Dubes 1988):\n\nK 1\nS(m, K) =\n(\nK! k=1\n−1)K−k K km k\n\nX\n\nS(10, 4) = 34K, S(19, 4)\n1010, ... so not doable. ≈\n\nLet’s try some heuristic gradient-descent-ish method instead.\n\n3",
  "342": "The K-Means Algorithm\n\nChoose the value of K before you start.\n\nn Initialize centers z1, ..., zK ∈ R and clusters C1, ..., CK in any way. Repeat until there is no further change in cost: for each k: Ck ←{xi : the closest representative is zk}\nfor each k: zk = mean(Ck)\n\nThis is simple enough, and takes O(Km) time per iteration.\n\nPPT demo\n\nOf course, it doesn’t always converge to the optimal solution.\n\nBut does the cost converge?\n\nLemma 3. During the course of the K-Means algorithm, the cost monotonically\ndecreases.\n\n(t) (t) (t) (t) Proof. Let z1 , ..., zK , C1 , ..., CK denote the centers and clusters at the start of\nthe tth iterate of K-Means. The first step of the iteration assigns each data point\nto its closest center, therefore, the cluster assignment is better:\n\n(t+1)\n(t+1)\n(t) (t) (t) (t) (t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , .",
  "344": "Example of how K-Means could converge to the wrong thing\n\nHow might you make K-Means more likely to converge to the optimal?\n\nHow might you choose K? (Why can’t you measure test error?)\n\nOther ways to evaluate clusters (“cluster validation”)\n\nThere are loads of cluster validity measures, alternatives to the cost. Draw a picture\n\n• Davies-Baldwin Index - looks at average intracluster distance (within-cluster\ndistance) to the centroid (want it to be small), and intercluster distances between centroids (want it to be large).\n\n• Dunn Index - looks pairwise at minimal intercluster distance (want it to be\nlarge) and maximal intracluster distance (want it to be small).\n\nExample: Microarray data. Have 6830 genes (rows) and 64 patients (columns). The color of each box is a measurement of the expression level of a gene. The expression level of a gene is basically how much of its special protein it is pro­ ducing. The physical chip itself doesn’t actually measure protein levels, but a proxy for t",
  "345": "sion level of a gene. The expression level of a gene is basically how much of its special protein it is pro­ ducing. The physical chip itself doesn’t actually measure protein levels, but a proxy for them (which is RNA, which sticks to the DNA on the chip). If the color is green, it means low expression levels, if the color is red, it means higher expression levels. Each patient is represented by a vector, which is the expression level of their genes. It’s a column vector with values given in color:\n\n5",
  "346": "Commons license. For more information, see http://ocw.mit.edu/fairuse.\n\nEach patient (column) has some type of cancer. Want to cluster patients to see whether patients with the same types of cancers cluster together. So each cluster center is an “average” patient expression level vector for some type of cancer. It’s also a column vector\n\nGraph of sum of squa\n\nst\n\nu\n\ne r s .\n\nres vs. number of cl\n\nHm, there’s no kink in this figure. Compare K = 3 solution with “true” clusters:\n\nT a b l e\n\nc o m p a r\n\ni n g\n\nK\n=\n\nue clust ers.\n\n3\n\ns o l u\n\nt i o n\n\nw\n\ni t h\n\nt r\n\nSpringer, 2009.\n\n6\n\n[IMAGE_OCR 1] oes ge\n= mal = af aa 2\n\naa ee Mery cian A EJ NEA A “ip yall bala\n- ga el \"Ei a\" =\nELE Pizda z. pk el U mu ZAJ li ges hae HOF\n= z mimo cm s s '\nmyk rt) a tz Se c AE FEG 12 ie id» pet",
  "347": "It’s pretty good at keeping the same cancers in the same cluster. The two breast cancers in the 2nd cluster were actually melanomas that metastasized.\n\nGenerally we cluster genes, not patients. Would really like to get something like this in practice:\n\nCourtesy of the Rockefeller University Press. Used with permission.\n\nFigure 7 from Rumfelt, Lynn, et al. \"Lineage Specification and Plasticity in CD19- Early B\ncell Precursors.\" Journal of Experimental Medicine 203 (2006): 675-87.\n\nwhere each row is a gene, and the columns are different immune cell types.\n\nA major issue with K-means: as K changes, cluster membership can change\narbitrarily. A solution is Hierarchical Clustering.\n\n• clusters at the next level of the hierarchy are created by merging clusters at the next lowest level.\n\n– lowest level: each cluster has 1 example\n\n– highest level: there’s only 1 cluster, containing all of the data.\n\n7\n\n[IMAGE_OCR 1] MLP CLP Fr.A Fr. B&C A uma B pw EBFI c D ot",
  "348": "lusterin g.\n\nchical c\n\nlevels o f hierar\n\nSeveral\n\nStatistical Learning, Springer, 2009.\n\nApplication Slides\n\n8",
  "349": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "350": "K-NN\n\n15.097 MIT, Spring 2012, Cynthia Rudin Credit: Seyda Ertekin\n\nK-Nearest Neighbors\n\n• Amongst the simplest of all machine learning algorithms. No eXplicit training or model. • Can be used both for classifcaton and regression.\n• Use XIs K-Nearest Neighbors to vote on what\nXIs label should be.\n\n1",
  "351": "K-Nearest Neighbors\n\n• Classify using the majority vote of the k closest training points\n\nX X X\n\n(a) 1-nearest neighbor\n(b) 2-nearest neighbor\n(c) 3-nearest neighbor\n\nK-Nearest Neighbors\n\n• K-NN algorithm does not explicitly compute decision\nboundaries. The boundaries between distinct classes form a subset of the Voronoi diagram of the training data.\n\nEach line segment is equidistant to neighboring points.\n\n2",
  "352": "K-Nearest Neighbors\n\n• For regression: the value for the test eXample becomes the (weighted) average of the values of the K neighbors.\n\nMaking K-NN More Powerful\n\n• A good value for K can be determined by considering a range of K values.\n\n– K too small: we’ll model the noise\n\n– K too large: neighbors include too many points from other classes\n\n• There are problems when there is a spread of distances among the K­\nNN. Use a distance-based voting scheme, where closer neighbors have\nmore influence.\n\n• The distance measure has to be meaningful – attributes should be scaled\n\n– Eg. Income varies 10,000-1,000,000 while height varies 1.5-1.8 meters\n\n3",
  "353": "Pros/Cons to K-NN\n\nPros:\n\n• Simple and powerful. No need for tuning complex parameters to build a model.\n\n• No training involved (“lazy”). New training examples can be added easily.\n\nPros/Cons to K-NN\n\nCons:\n\n• Expensive and slow: O(md), m= # examples, d= # dimensions\n\n– To determine the nearest neighbor of a new point x, must compute the distance to all m training examples. Runtime performance is slow, but can be improved.\n\n• Pre-sort training examples into fast data structures\n\n• Compute only an approximate distance\n\n• Remove redundant data (condensing)\n\n4",
  "372": "vJ occur with probabilities p1, ..., pJ, where [p1, ..., pJ] is a discrete probability distribution.\n\nX J\nEp [p1,...,pJ]I(p) =\npjI(p ∼\nj) =\n\nj=1\n− X\npj log2 pj =: H(p)\n\nj\n\nwhere p is the vector [p1, ..., pJ]. H(p) is called the entropy of discrete distribution p.\n\nSo if there are only 2 events (binary), with probabilities p = [p, 1 −p],\n\nH(p) = −p log2(p) −(1 −p) log2(1 −p).\n\nIf the probabilities were [1/2, 1/2],\n\n1 1\nH(p) = −2\nlog2\n= 1\n(Yes, we knew that.) 2 2\n\nOr if the probabilities were [0.99, 0.01],\n\nH(p) = 0.08 bits.\n\nAs one of the probabilities in the vector p goes to 1, H(p) →0, which is what we want.\n\nBack to C4.5, which uses Information Gain as the splitting criteria.\n\n5",
  "354": "K-NN Applicatons\n\n• Handwriten character classifcaton using nearest neighbor in large databases. Smith, S.J et. al.; IEEE PAMI, 2004. Classify handwriten characters into numbers.\n\n• Fast content-based image retrieval based on equal-average K-nearest-neighbor\nsearch schemes z. Lu, H. Burkhardt, S. Boehmer; LNCS, 2006. CBIR (Content based image retrieval), return the closest neighbors as the relevant items to a query.\n\n• Use of K-Nearest Neighbor classifer for intrusion detecton\nYihua Liao, V.Rao Vemuri; Computers and Security Journal, 2002 Classify program behavior as normal or intrusive.\n\n• Fault Detecton Using the k-Nearest Neighbor Rule for Semiconductor Manufacturing\nProcesses He, Q.P., Jin Wang; IEEE Transactons in Semiconductor Manufacturing, 2007 Early fault detecton in industrial systems.\n\n5",
  "355": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "356": "Na¨ıve Bayes\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nThanks to S¸eyda Ertekin Credit: Ng, Mitchell\n\nThe Na¨ıve Bayes algorithm comes from a generative model. There is an important distinction between generative and discriminative models. In all cases, we\nwant to predict the label y, given x, that is, we want P(Y = y|X = x). Throughout the paper, we’ll remember that the probability distribution for measure P is\nover an unknown distribution over X × Y.\n\nNa¨ıve Bayes Generative Model\nEstimate P(X = x|Y = y) and P(Y = y)\nand use Bayes rule to get P(Y = y|X = x)\nDiscriminative Model\nDirectly estimate P(Y = y|X = x)\n\nMost of the top 10 classification algorithms are discriminative (K-NN, CART,\nC4.5, SVM, AdaBoost).\n\nFor Na¨ıve Bayes, we make an assumption that if we know the class label y, then we know the mechanism (the random process) of how x is generated.\n\nNa¨ıve Bayes is great for very high dimensional problems because it makes a very strong assumption. Very high dimensional problems suf",
  "357": "e know the mechanism (the random process) of how x is generated.\n\nNa¨ıve Bayes is great for very high dimensional problems because it makes a very strong assumption. Very high dimensional problems suffer from the curse of dimensionality – it’s difficult to understand what’s going on in a high dimensional space without tons of data.\n\nExample: Constructing a spam filter. Each example is an email, each dimension “j” of vector x represents the presence of a word.\n\n1",
  "358": " a\n\n1 0\n\naardvark\n\n\n\n0 .. x . 1 ..\n\n aardwolf  .  .. . 0\n\n=\n\n\n\n buy  ... zyxt\n\nThis x represents an email containing the words “a” and “buy”, but not “aardvark” or “zyxt”. The size of the vocabulary could be ∼50,000 words, so we are in a 50,000 dimensional space.\n\nNa¨ıve Bayes makes the assumption that the x(j)’s are conditionally independent\ngiven y. Say y = 1 means spam email, word 2,087 is “buy”, and word 39,831 is\n“price.” Na¨ıve Bayes assumes that if y = 1 (it’s spam), then knowing x(2,087) = 1\n(email contains “buy”) won’t effect your belief about x(39,381) (email contains “price”).\n\nNote: This does not mean x(2,087) and x(39,831) are independent, that is,\n\nP(X(2,087) = x(2,087)) = P(X(2,087) = x(2,087)|X(39,831) = x(39,831)).\n\nIt only means they are conditionally independent given y. Using the definition of conditional probability recursively,\n\nP(X(1) = x(1), . . .",
  "359": " x(2,087)) = P(X(2,087) = x(2,087)|X(39,831) = x(39,831)).\n\nIt only means they are conditionally independent given y. Using the definition of conditional probability recursively,\n\nP(X(1) = x(1), . . . , X(50,000) = x(50,000)|Y = y) =\nP(X(1) = x(1)|Y = y)P(X(2) = x(2)|Y = y, X(1) = x(1))\nP(X(3) = x(3)|Y = y, X(1) = x(1), X(2) = x(2))\n. . . P(X(50,000) = x(50,000)|Y = y, X(1) = x(1), . . . , X(49,999) = x(49,999)).\n\nThe independence assumption gives:\n\nP(X(1) = x(1), . . . , X(n) = x(n)|Y = y)\n= P(X(1) = x(1)|Y = y)P(X(2) = x(2)|Y = y) . . . P(X(n) = x(n)|Y = y)\nn\n=\nY\nP(X(j) = x(j)\n\nj=1\n|Y = y).\n(1)\n\n2",
  "360": "Bayes rule says\n\n(1) (1)\nP(Y = y)P(X(1) = x(1), ..., X(n) = x(n) Y = y)\nP(Y = y|X\n= x\n, ..., X(n) = x(n)) =\n|\nP(X(1) = x(1), ..., X(n) = x(n))\n\nso plugging in (1), we have\n\nn\nP(Y = y)\nP(X(j) = x(j)\nj=1\nY = y)\nP(Y = y|X(1) = x(1), ..., X(n) = x(n)) =\n\nQ |\n\nP(X(1) = x(1), ..., X(n) = x(n))\n\nFor a new test instance, called xtest, we want to choose the most probable value of y, that is\n\n(1) (n)\nP(Y = y˜) Q\n(1) j P(X\n= xtest, ..., X(n) = x\nyNB ∈ test arg max\n|Y = y˜)\n\n(1) (n) y˜\nP(X(1) = x\n, .",
  "361": " a new test instance, called xtest, we want to choose the most probable value of y, that is\n\n(1) (n)\nP(Y = y˜) Q\n(1) j P(X\n= xtest, ..., X(n) = x\nyNB ∈ test arg max\n|Y = y˜)\n\n(1) (n) y˜\nP(X(1) = x\n, ..., X(n) test\n= xtest)\nn\n= arg max P(Y = y˜)\nY\nP(X(j) = x(j)|Y = y˜).\ny˜\nj=1\n\n(j)\nSo now, we just need P(Y = y˜) for each possible y˜, and P(X(j) = xtest|Y = y˜)\nfor each j and y˜. Of course we can’t compute those. Let’s use the empirical probability estimates:\n\n1 ˆ\n[y =y˜]\nP(Y = y˜) =\nP i i\n= fraction of data where the label is y˜\nm 1 i [ (j) ˆ\n\nP\n\n(j) P\nxi =x\n,y =y˜\n(j) (j) ] P(X\n= x\n|Y = y˜) =\ntest i\n= Conf(Y = y˜ →X(j)\n(j) test\n= xtest).\n1 i\n[yi=y˜]\n\nThat’s the simplest version of Na¨ıve Bayes:\n\nn ˆ ˆ (j) y\n∈arg max P(Y = y˜)\nY P(X(j) NB\n= xtest\ny˜\nj=1\n|Y = y˜).\n\nThere could potentially be a problem that most of the conditional probabilities are 0 because the dimensionality of the data is very high compared to the amount ˆ (j)\nof data. This causes a problem because if even one P(X(j) =",
  "362": " a problem that most of the conditional probabilities are 0 because the dimensionality of the data is very high compared to the amount ˆ (j)\nof data. This causes a problem because if even one P(X(j) = xtest|Y = y˜) is zero\nthen the whole right side is zero. In other words, if no training examples from class “spam” have the word “tomato,” we’d never classify a test example containing the word “tomato” as spam!\n\n3",
  "363": "To avoid this, we (sort of) set the probabilities to a small positive value when there are no data. In particular, we use a “Bayesian shrinkage estimate” of P(X(j) (j)\n= xtest|Y = y˜) where we add some hallucinated examples. There are K\nhallucinated examples spread evenly over the possible values of X(j). K is the\nnumber of distinct values of X(j). The probabilities are pulled toward 1/K. So,\nnow we replace:\n\n1 i (j) (j)\n+ 1\n=\n) [ ˆ x x\n,y =y˜\nP(X(j) (j ]\n= xtest|Y = y˜) =\n\nP\n\nP i test i 1 i\n[yi=y˜] + K\n\nˆP(Y = y˜) =\nP 1 i\n[yi=y˜] + 1\nm + K\nˆ\nThis is called Laplace smoothing. The smoothing for P(Y = y˜) is probably unnecessary and has little to no effect.\n\nNa¨ıve Bayes is not necessarily the best algorithm, but is a good first thing to try, and performs surprisingly well given its simplicity!\n\nThere are extensions to continuous data and other variations too.\n\nPPT Slides\n\n4",
  "364": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "365": "Decision Trees\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nCredit: Russell & Norvig, Mitchell, Kohavi & Quinlan, Carter, Vanden Berghen\n\nWhy trees?\n\n• interpretable/intuitive, popular in medical applications because they mimic\nthe way a doctor thinks\n\n• model discrete outcomes nicely\n\n• can be very powerful, can be as complex as you need them\n\n• C4.5 and CART - from “top 10” - decision trees are very popular\n\nSome real examples (from Russell & Norvig, Mitchell)\n\n• BP’s GasOIL system for separating gas and oil on offshore platforms - decision trees replaced a hand-designed rules system with 2500 rules. C4.5-based\nsystem outperformed human experts and saved BP millions. (1986)\n\n• learning to fly a Cessna on a flight simulator by watching human experts fly the simulator (1992)\n\n• can also learn to play tennis, analyze C-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “splitting” attributes one by one. To determine which attribute to split, look",
  "366": "o play tennis, analyze C-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “splitting” attributes one by one. To determine which attribute to split, look at “node impurity.”\n\n• Assign leaf nodes the majority vote in the leaf.\n\n• When we get to the bottom, prune the tree to prevent overfitting\n\nWhy is this a good way to build a tree?\n\n1",
  "367": "I have to warn you that C4.5 and CART are not elegant by any means that I can define elegant. But the resulting trees can be very elegant. Plus there are 2 of the top 10 algorithms in data mining that are decision tree algorithms! So it’s worth it for us to know what’s under the hood... even though, well... let’s just say it ain’t pretty.\n\nExample: Will the customer wait for a table? (from Russell & Norvig)\n\nHere are the attributes:\n\nList of ten attributes that contribute to whether the customer will wait for a table.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are the examples:\n\nTab le of twe lv e ex am ples , t he att ribu\n\ntes fo r ea ch , an d pred i ctio n of\n\nwhe the r th e cu st om er w i ll wai t.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are two options for the first feature to split at the top of the tree. Which one should we choose? Which one gives me the most information?\n\n2",
  "368": "Two examples of de cision trees. Opti on\n\nfir\n\non B is\n\nA i s sp lit\n\nsplit first by Ty pe.\n\nwh ile\n\nO pti\n\nst by\n\nP atr\n\non s,\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWhat we need is a formula to compute “information.” Before we do that, here’s another example. Let’s say we pick one of them (Patrons). Maybe then we’ll pick Hungry next, because it has a lot of “information”:\n\nA decision tree sp lit first by Patron s,\n\ngry\n\nthe n by Hun\n\n.\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWe’ll build up to the derivation of C4.5. Origins: Hunt 1962, ID3 of Quinlan 1979 (600 lines of Pascal), C4 (Quinlan 1987). C4.5 is 9000 lines of C (Quinlan 1993). We start with some basic information theory.\n\n3",
  "369": "Information Theory (from slides of Tom Carter, June 2011)\n\n“Information” from observing the occurrence of an event\n:= #bits needed to encode the probability of the event p = −log2 p.\n\nE.g., a coin flip from a fair coin contains 1 bit of information. If the event has probability 1, we get no information from the occurrence of the event.\n\nWhere did this definition of information come from? Turns out it’s pretty cool. We want to define I so that it obeys all these things:\n\n• I(p) ≥0, I(1) = 0; the information of any event is non-negative, no information from events with prob 1\n\n• I(p1 · p2) = I(p1) + I(p2); the information from two independent events\nshould be the sum of their informations\n\n• I(p) is continuous, slight changes in probability correspond to slight changes in information\n\nTogether these lead to:\n\nI(p2) = 2I(p) or generally I(pn) = nI(p),\n\nthis means that\n\n1\nI(p) = I\n\n(p1/m)m\n= mI\n\np1/m\nso\nI(p) = I\n\np1/m\nm\n\nand more generally,\n\nI\n\npn/m\nn\n=\nI(p). m\nThis is true for any fractio",
  "370": "er these lead to:\n\nI(p2) = 2I(p) or generally I(pn) = nI(p),\n\nthis means that\n\n1\nI(p) = I\n\n(p1/m)m\n= mI\n\np1/m\nso\nI(p) = I\n\np1/m\nm\n\nand more generally,\n\nI\n\npn/m\nn\n=\nI(p). m\nThis is true for any fraction n/m, which includes rationals, so just define it for\nall positive reals:\nI(pa) = aI(p).\n\nThe functions that do this are I(p) = −logb(p) for some b. Choose b = 2 for\n“bits.”\n\n4",
  "371": "Flipping a fair coin gives −log2(1/2) = 1 bit of information if it comes up either\nheads or tails.\n\nA biased coin landing on heads with p = .99 gives −log2(.99) = .0145 bits of\ninformation.\n\nA biased coin landing on heads with p = .01 gives −log2(.01) = 6.643 bits of\ninformation.\n\nNow, if we had lots of events, what’s the mean information of those events? Assume the events v1, ..., vJ occur with probabilities p1, ..., pJ, where [p1, ..., pJ] is a discrete probability distribution.\n\nX J\nEp [p1,...,pJ]I(p) =\npjI(p ∼\nj) =\n\nj=1\n− X\npj log2 pj =: H(p)\n\nj\n\nwhere p is the vector [p1, .",
  "518": " and new algorithms are constantly added to the list of packages you can download.\n\n3. R is a useful skill. Employers that value analytics recognize R as useful and important. If for no other reason, learning R is worthwhile to help boost your r´esum´e.\n\nN",
  "373": "Back to C4.5 (source material: Russell & Norvig, Mitchell, Quinlan) We consider a “test” split on attribute A at a branch.\n\nIn S we have #pos positives and #neg negatives. For each branch j, we have #posj positives and #negj negatives.\n\nThe training probabilities\n\nin branch j are:\n\n#posj #negj ,\n#posj + #negj #posj + #negj\n\n.\n\nThe Information Gain is calculated like this:\n\nGain(S, A) = expected reduction in entropy due to branching on attribute A\n= original\n\nentropy −entropy after branching #pos #neg\n= H\n,\n#pos + #neg #pos + #neg\n\nJ\n\n−\nX #posj + #negj\n#posj #negj H , .\n#pos + #neg\n\n#posj + #negj #posj + #negj\nj=1\n\nBack to the example with\n\nthe restauran\n\nts.\n\n1 1 2 4 6 2 4\nGain(S, Patrons) = H\n, −\nH([0, 1]) +\nH([1, 0]) +\nH , 2 2 12 12 12\n\n6 6\n\n≈0.541 bits.\n\n2 1 1 2 1 1\nGain(S, Type) = 1 −\nH ,\n+\nH , 12 2 2 12 2 2 4\n\n2 2 4 2 2 ,\n+\nH , 0 bits. 4 4\n\n≈\n\n+\nH 12\n\n12\n\n4 4\n\n6",
  "374": "Actually Patrons has the highest gain among the attributes, and is chosen to be the root of the tree. In general, we want to choose the feature A that maximizes Gain(S, A).\n\nOne problem with Gain is that it likes to partition too much, and favors numerous splits: e.g., if each branch contains 1 example:\n\nThen,\n\nH\n\n#posj #negj ,\n= 0 for all j,\n#posj + #negj #posj + #negj\nso all those negative terms would be zero and we’d\n\nchoose that attribute over all the others.\n\nAn alternative to Gain is the Gain Ratio. We want to have a large Gain, but also we want small partitions. We’ll choose our attribute according to:\n\nGain(S, A) ←want large SplitInfo(S, A) ←want small\n\nwhere SplitInfo(S, A) comes from the partition:\n\nJ Sj Sj\nSplitInfo(S, A) =\nX | | − log |S|\n\n| |\n\nj=1\n|S|\n\nwhere |Sj| is the number of examples in branch j. We want each term in the S sum to be large. That means we want | j| to be large, meaning that we want |S| lots of examples in each branch.\n\nKeep splitting until:\n\n7\n\n[IMAGE_O",
  "375": " of examples in branch j. We want each term in the S sum to be large. That means we want | j| to be large, meaning that we want |S| lots of examples in each branch.\n\nKeep splitting until:\n\n7\n\n[IMAGE_OCR 1] S$ taining data going to this branch\nA=[0,0,0,0,0\nA=[1,1,1,1,1]\nA=[00901]|\n#pos=1 I ] #pos=0\n#neg=0 A=[0,00,1,1] #neg=1\na #pos=1\nmer #neg=0",
  "376": "• no more examples left (no point trying to split)\n\n• all examples have the same class\n\n• no more attributes to split\n\nFor the restaurant example, we get this:\n\nA decisi\n\nee s plit by\n\n, Type,\n\non tr\n\nPa tro\n\nand Fr\n\ni/Sat.\n\nun gry\n\nns, H\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nA wrinkle: actually, it turns out that the class labels for the data were themselves generated from a tree. So to get the label for an example, they fed it into a tree, and got the label from the leaf. That tree is here:\n\n8",
  "377": "A decisi\n\non tree for t\n\nant exampl e, usin\n\nhe restaur\n\ng most of th e attrib\n\nutes.\n\nA Modern Approach, Prentice Hall, 2009.\n\nBut the one we found is simpler! Does that mean our algorithm isn’t doing a good job?\n\nThere are possibilities to replace H([p, 1 −p]), it is not the only thing we can use! One example is the Gini index 2p(1 −p) used by CART. Another example is the value 1 −max(p, 1 −p).1\n\n1If an event has prob p of success, this value is the proportion of time you guess incorrectly if you classify the\nevent to happen when p > 1/2 (and classify the event not to happen when p ≤1/2).\n\n9",
  "378": "Gin\n\nf p bet\n\nclassifica\n\ny,\n\nrop\n\ntion error for values o\n\nent\n\nof\n\nGra ph\n\ni i nde x, and mi s\n\nween 0 and 1\n\n.\n\nLearning, Springer, 2009.\n\nC4.5 uses information gain for splitting, and CART uses the Gini index. (CART only has binary splits.)\n\nPruning Let’s start with C4.5’s pruning. C4.5 recursively makes choices as to whether to prune on an attribute:\n\n• Option 1: leaving the tree as is\n\n• Option 2: replace that part of the tree with a leaf corresponding to the most frequent label in the data S going to that part of the tree.\n\n• Option 3: replace that part of the tree with one of its subtrees, corresponding to the most common branch in the split\n\nDemo To figure out which decision to make, C4.5 computes upper bounds on the probability of error for each option. I’ll show you how to do that shortly.\n\n• Prob of error for Option 1 ≤UpperBound1\n\n• Prob of error for Option 2 ≤UpperBound2\n\n10",
  "379": "• Prob of error for Option 3 ≤UpperBound3\n\nC4.5 chooses the option that has the lowest of these three upper bounds. This ensures that (w.h.p.) the error rate is fairly low.\n\nE.g., which has the smallest upper bound:\n\n• 1 incorrect out of 3\n\n• 5 incorrect out of 17, or\n\n• 9 incorrect out of 32?\n\nFor each option, we count the number correct and the number incorrect. We need upper confidence intervals on the proportion that are incorrect. To calculate the upper bounds, calculate confidence intervals on proportions.\n\nThe abstract problem is: say you flip a coin N times, with M heads. (Here N is the number of examples in the leaf, M is the number incorrectly classified.) What is an upper bound for the probability p of heads for the coin?\n\nThink visually about the binomial distribution, where we have N coin flips, and how it changes as p changes:\n\np=0.5\np=0.1\np=0.9\n\nWe want the upper bound to be as large as possible (largest possible p, it’s an upper bound), but still there needs to be a pro",
  "380": " have N coin flips, and how it changes as p changes:\n\np=0.5\np=0.1\np=0.9\n\nWe want the upper bound to be as large as possible (largest possible p, it’s an upper bound), but still there needs to be a probability α to get as few errors as we got. In other words, we want:\n\nPM ( ∼Bin(N,preasonable upper bound) M or fewer errors) ≥α\n\n11\n\n[IMAGE_OCR 1] ae O a4 aż m 0.03 0.05 004 002 % 5 10 15 20 25\n\n[IMAGE_OCR 2] 0.35 03 025 02 as a 05 % 5 10 15 20\n\n[IMAGE_OCR 3] 0.35 03 025 02 as a 05 % 5 10 15 20",
  "381": "which means we want to choose our upper bound, call it pα), so that it’s the largest possible value of preasonable upper bound that still satisfies that inequality.\n\nThat is,\n\nPM∼Bin(N,pα)(M or fewer errors) ≈α\n\nX M Bin(z, N, pα)\n\nz=0\n≈α\n\nX M N\npz (1 −p )N−z ≈α for M > 0\n(for M = 0 it’s (1 −p )N\nα z α α\nz=0\n≈α)\n\nWe can calculate this numerically without a problem. So now if you give me α\nM and N, I can give you pα. C4.5 uses α = .",
  "382": "\n≈α\n\nX M N\npz (1 −p )N−z ≈α for M > 0\n(for M = 0 it’s (1 −p )N\nα z α α\nz=0\n≈α)\n\nWe can calculate this numerically without a problem. So now if you give me α\nM and N, I can give you pα. C4.5 uses α = .25 by default. M for a given branch\nis how many misclassified examples are in the branch. N for a given branch is just the number of examples in the branch, |Sj|.\n\nSo we can calculate the upper bound on a branch, but it’s still not clear how to calculate the upper bound on a tree. Actually, we calculate an upper confidence bound on each branch on the tree and average it over the relative frequencies of landing in each branch of the tree. It’s best explained by example:\n\nLet’s consider a dataset of 16 examples describing toys (from the Kranf Site). We want to know if the toy is fun or not.\n\n12\n\n[IMAGE_OCR 1]\nl-a\na ‘Fash diiuton this way",
  "383": "Color Max number of players Fun? red 2 yes red 3 yes green 2 yes red 2 yes green 2 yes green 4 yes green 2 yes green 1 yes red 2 yes green 2 yes red 1 yes blue 2 no green 2 yes green 1 yes red 3 yes green 1 yes Think of a split on color.\n\nTo calculate the upper bound on the tree for Option 1, calculate p.25 for each branch, which are respectively .206, .143, and .75. Then the average is: 1 1\nAve of the upper bounds for tree =\n(6 .206 + 9\n16 ·\n· .143 + 1 · .75) = 3.273 × 16\nLet’s compare that to the error rate of Option 2, where we’d replace the tree\nwith a leaf with 6+9+1 = 16 examples in it, where 15 are positive, and 1 is\n1\nnegative. Calculate pα that solves α = P\nz=0 Bin(z, 16, pα), which is .157. The\naverage is: 1 1\nAve of the upper bounds for leaf =\n16 · .157 = 2.512\n16 × .",
  "384": "les in it, where 15 are positive, and 1 is\n1\nnegative. Calculate pα that solves α = P\nz=0 Bin(z, 16, pα), which is .157. The\naverage is: 1 1\nAve of the upper bounds for leaf =\n16 · .157 = 2.512\n16 × . 16\nSay we had to make the decision amongst only Options 1 and 2. Since 2.512 <\n3.273, the upper bound on the error for Option 2 is lower, so we’ll prune the tree\nto a leaf. Look at the data - does it make sense to do this?\n\n13",
  "385": "CART - Classification and Regression Trees (Breiman, Freedman, Olshen, Stone,\n1984)\n\nDoes only binary splits, not multiway splits (less interpretable, but simplifies splitting criteria).\n\nLet’s do classification first, and regression later. For splitting, CART uses the Gini index. The Gini index again is\n\nvariance of Bin(n, p)\np(1 −p) =\n= variance of Bernoulli(p).\nn For pruning, CART uses “minimal cost complexity.”\n\nEach subtree is assigned a cost. The first term in the cost is a misclassification error. The second term is a regularization term. If we choose C to be large, the tree that minimizes the cost will be sparser. If C is small, the tree that minimizes the cost will have better training accuracy.\n\nX\n1[yi=leaf’s class] + C [#leaves in subtree] .",
  "386": "se C to be large, the tree that minimizes the cost will be sparser. If C is small, the tree that minimizes the cost will have better training accuracy.\n\nX\n1[yi=leaf’s class] + C [#leaves in subtree] .\n\nX\n\ncost(subtree) =\n\n̸\n\nves j xi∈\n\nlea\n\nleaf j\n\nWe could create a sequence of nested subtrees by gradually increasing C.\n\nDraw a picture\n\nNow we need to choose C. Here’s one way to do this:\n\n• Step 1: For each C, hold out some data, split, then prune, producing a tree for each C.\n\n• Step 2: see which tree is the best on the holdout data, choose C.\n\n• Step 3: use all data, use chosen C, run split, then prune to produce the final tree.\n\nThere are other ways to do this! You’ll use cross-validation in the homework.\n\nReview C4.5 and CART\n\n14",
  "387": "CART’s Regression Trees\n\nFour-part figure in\n\ncl\n\nud ing partitions, a corresponding tree , and a perspective plot of the predicti on surface .\n\nLearning, Springer, 2009.\n\nCART decides which attributes to split and where to split them. In each leaf, we’re going to assign f(x) to be a constant.\n\nCan you guess what value to assign?\n\nConsider the empirical error, using the least squares loss:\n\nRtrain(f) =\nX (yi i −f(xi))2\n\n15",
  "388": "Break it up by leaves. Call the value of f(x) in leaf j by fj since it’s a constant.\n\nX (y f(x 2 i i))\n\nX\n\nRtrain(f) =\n\nX ves j i −\n\n∈\n\nlea\n\nleaf j\n\n=\n(y\n2 =:\nRtrain i fj) j (fj).\n\nX\n\nX\n\nleaf j −\n\nleaves j i∈\n\nlea\n\nves j\n\nTo choose the value of the fj’s so that they minimize Rtrain j , take the derivative, set it to 0. Let |Sj| be the number of examples in leaf j.\n\nd\n0 =\n˜ df i∈\n\nX ˜ (yi f)2\n˜f=fj\nleaf j −\n\n!\n\nX\n\nyi i −|Sj| ˜f\n\n!\n\n= −2\nX (yi i −˜f)\n\n= −2\n˜f=fj\n\n˜f=fj\n1\nfj =\nX\nyi = y¯\n\n|Sj| Sj,\n\ni∈leaf j\n\nwhere y¯Sj is the sample average of the labels for leaf j’s examples.\n\nSo now we know what value to assign for f in each leaf. How to split? Greedily want attribute A and split point s solving the following.\n\n\n\n\n\nmin (yi\nC1)2 + min\n(yi C2)2 . A, s C1 x\n\nX\n\nX\n\nC2 leaf| (A) − −\n\nmin\n\n\n\n( i leaf for each attribute A ∈{\n|x A)>s\ndo }\n\ni∈{ x ≤s} x\n\na\n\nlinesearch over s\n\nThe first term means that we’ll choose the optimal C1 = y¯{leaf|x(A)≤s .",
  "389": " . A, s C1 x\n\nX\n\nX\n\nC2 leaf| (A) − −\n\nmin\n\n\n\n( i leaf for each attribute A ∈{\n|x A)>s\ndo }\n\ni∈{ x ≤s} x\n\na\n\nlinesearch over s\n\nThe first term means that we’ll choose the optimal C1 = y¯{leaf|x(A)≤s . The second\n}\nterm means we’ll choose C2 = y¯{leaf x(A)>s .\n| }\n\nFor pruning, again CART does minimal cost complexity pruning:\n\ni∈S\n−y¯Sj)2 + C[# leaves in tree]\n\nX\n\nX yi\n\ncost =\n(\n\nves j x\n\nlea\n\nj\n\n16",
  "390": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "391": "Logistic Regression\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nThanks to Ashia Wilson Credit: J.S. Cramer’s “The Origin of Logistic Regression”\n\nOrigins: 19th Century.\n\n• Studying growth of populations and the course of chemical reactions using\n\nd W(t) = βW(t)\ndt ⇒\nW(t) = Aeβt\n\nwhich is a good model for unopposed growth, like the US population’s growth at the time.\n\n• Adolphe Quetelet (1796 - 1874), Belgian astronomer turned statistician,\nknew it produced impossible values and asked his pupil Pierre-Fran¸cois Verhulst (1804-1849) to help him work on a more realistic model. They chose\n\nd W(t) = βW(t) −Φ(W(t))\ndt\n\nto resist further growth, and with the choice of Φ to be a quadratic function, they got:\n\nd W(t) = βW(t)(Ω\ndt −W(t)),\n\nW(t)\nwhere Ωis the saturation limit of W. Writing P(t) =\nas the proportion of Ω saturation limit:\nd P(t) = βP(t)(1 −P(t))\ndt\ne(α+βt)\nP(t) = 1 + e(α+βt)\n\nwhere α is the constant from the integration. Verhulst’s “logistic” function has values between 0 and 1.\n\nDraw ",
  "392": "proportion of Ω saturation limit:\nd P(t) = βP(t)(1 −P(t))\ndt\ne(α+βt)\nP(t) = 1 + e(α+βt)\n\nwhere α is the constant from the integration. Verhulst’s “logistic” function has values between 0 and 1.\n\nDraw the logistic function\n\n1",
  "393": "He published in 3 papers between 1838 and 1847. The first paper demonstrated that the curve agrees very well with the actual course of the population in France, Belgium, Essex, and Russia for periods up to 1833. He did not say how he fitted\nthe curves. In the second paper he tried to fit the logistic to 3 points using 20-30\nyears of data (which in general is a not a great way to get a good model). His estimates of the limiting population Ωof 6.6 million for Belgium and 40 million\nfor France were a little off- these populations are now 11 million for Belgium\nand 65 million for France. In another paper, the estimate was corrected, and they estimated 9.5 million for Belgium (pretty close!).\n\nVerhulst died young in poor heath, and his work was forgotten about, but reinvented in 1920 by Raymond Pearl and Lowell Reed who were studying population growth of the US. They also tried to fit the logistic function to population growth, and estimated Ωfor the US to be 197 million (the current popula",
  "394": " Pearl and Lowell Reed who were studying population growth of the US. They also tried to fit the logistic function to population growth, and estimated Ωfor the US to be 197 million (the current population is 312 million). Actually, Pearl and collaborators spent 20 years applying the logistic growth curve to almost any living population (fruit flies, humans in North Africa, cantaloupes).\n\nVerhulst’s work was rediscovered just after Pearl and Reed’s first paper in 1920, but they didn’t acknowledge him in their second paper, and only in a footnote in a third paper (by Pearl) in 1922. They cited him in 1923, but didn’t use his terminology and called his papers “long since forgotten.” The name logistic was revived by Yule, in a presidential address to the Royal Statistical Society in 1925.\n\nThere was a lot of debate over whether the logistic function could replace the cdf of the normal distribution. The person who really showed this could happen was\nJoseph Berkson (1899-1982), who was the c",
  "395": "re was a lot of debate over whether the logistic function could replace the cdf of the normal distribution. The person who really showed this could happen was\nJoseph Berkson (1899-1982), who was the chief statistician at the Mayo Clinic,\nand who was a collaborator of Reed’s. But because of his personality and attacks on the method of maximum likelihood, controversy ensued... it took until around the 1960’s to resolve it!\n\nLet’s start with a probabilistic model. Assume x ∈Rn, and that x is deterministic (chosen, not random):\n\nY ∼Bernoulli(P(Y = 1|x)).\n\n2",
  "396": "Here Y takes either 0 or 1, but we need a ±1 for classification. I’ll write ˜0 for\nnow to stand for either 0 or −1. The model is:\n\nP(Y = 1\nln\n|x, λ)\n= λTx\nP(Y = ˜0|x, λ)\n\n|\n\n{z\n\n}\n\n“odds ratio” Why does this model make an\n\nWe want to make a linear combination of feature values, like in regular regression, which explains the right hand side. But that can take any real values. We need to turn it into a model for\nP(Y = 1|x, λ), which takes values only between 0 and 1. The odds ratio turns\nthose probabilities into positive real numbers, then the log turns it into any real numbers.\n\ny sense\n\nat all?\n\nP(Y = 1|x, λ) = eλT x\nP(Y = ˜0|x, λ)\n\nT\n(Y = 1|\nT P\nx, λ) = eλ xP(Y = ˜0|x, λ) = eλ x(1 −P(Y = 1|x, λ))\n\nP(Y = 1|\nT\nx, λ)\nh\n1 + eλ xi\n= eλT x\n\neλT x\nP(Y = 1|x, λ) = 1 + eλT\nlogistic x ← function\n\nWe’ll use maximum likelihood estimation. We choose parameters λ to maximize the likelihood of the data given the model.\n\nm iid\nL(λ) := P(Y1 = y1, . . . , Ym = ym|λ, x1, . . .",
  "397": "λT\nlogistic x ← function\n\nWe’ll use maximum likelihood estimation. We choose parameters λ to maximize the likelihood of the data given the model.\n\nm iid\nL(λ) := P(Y1 = y1, . . . , Ym = ym|λ, x1, . . . , xm) =\nY\nP(Yi = yi\ni=1\n|λ, xi).\n\nChoose\nλ∗∈argmax L(λ) = argmax log L(λ)\nλ\nλ\nlog-likelihood\nwhere we take the log for convenience since it doesn’t\n\n| {z\n\n}\n\neffect\n\nthe argmax.\n\nBut first, we’ll simplify. We need P(Y = yi|λ, xi) for both yi = 1 and yi = −1.\n \n\nT \n\nλ\nx\nyi = 1,\nneed P(Y = 1|λ, x\n1\ni) := pi =\ne i 1 T T T\n1+eλ\nx =\n=\ni\n1+e−λ\nxi\n1+e−y λ\nx i i T T If  \nyi = −\nλ\nx\nλ\nx 1 (i.e., ˜0),\nneed P(Y = ˜0|λ, xi) = 1 −p = 1+e\ni T T\nλ\nx − e i i\nλ\nx =\n1 T\n1+e\ni\n1+e\ni\n1+eλ\nxi\n=\n1\n1+e−\nT\ny λ\nx . i i \n\n3",
  "398": "So, we can just write\n\n1\nP(Y = yi|λ, xi) = 1 + e−y λT\n. i xi\n\nThen,\n\nλ∗∈argmax log L(λ)\nλ\nm\n= argmax\nX 1\nlog 1 + e−yiλT x\nλ\ni\ni=1\nm\n= argmin\nλ\n\nX\nlog(1 + e−yiλT xi).\n\ni=1\n\nThis agrees with the “frequentist” derivation we had before.\n\nThe loss is convex in λ so we can minimize by gradient descent.\n\n4",
  "399": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "519": "ng R is worthwhile to help boost your r´esum´e.\n\nNote that R is a programming language, and there is no intuitive graphical user interface with buttons you can click to run different methods. However, with some practice, this kind of environment makes it e",
  "520": "some practice, this kind of environment makes it easy to quickly code scripts and functions for various statistical purposes. To get the most out of this tutorial, follow the examples\nby typing them out in R on your own computer. A line that begins with > ",
  "495": "Rule Mining and the Apriori Algorithm\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nThe Apriori algorithm - often called the “first thing data miners try,” but somehow doesn’t appear in most data mining textbooks or courses!\n\nStart with market basket data:\n\nSome",
  "496": " or courses!\n\nStart with market basket data:\n\nSome important definitions:\n\n• Itemset: a subset of items, e.g., (bananas, cherries, elderberries), indexed by {2, 3, 5}.\n\n• Support of an itemset: number of transactions containing it,\n\nm\nSupp(bananas, cherrie",
  "500": "wnward closure:\n\nIf Supp(a ∪b) ≥θ then Supp(a) ≥θ and Supp(b) ≥θ.\n\nThat is, if a ∪b is a frequent item set, then so are a and b.\n\nSupp(a ∪b) = #times a and b are purchased\n≤#times a is purchased = Supp(a).\n\nApriori finds all frequent itemsets (a such that ",
  "501": "\nApriori finds all frequent itemsets (a such that Supp(a) ≥θ).\nWe can use Apriori’s result to get all strong rules a →b as follows:\n\n• For each frequent itemset ℓ:\n\n– Find all nonempty subsets of ℓ\n\n– For each subset a, output a →{ℓ\\ a} whenever\n\nSupp(ℓ) S",
  "502": "ach subset a, output a →{ℓ\\ a} whenever\n\nSupp(ℓ) Supp(a) ≥minconf.\n\nNow for Apriori. Use the downward closure property: generate all k-itemsets\n(itemsets of size k) from (k −1)-itemsets. It’s a breadth-first-search.\n\n2",
  "503": "Example:\nθ = 10\n\nerries\n\nbananas\n\ncherries\n\nes grap\n\napples\n\nelderb\n\n1-itemsets:\na b c d e\n/f\ng supp: 25 20 30 45 29 5 17\n\n2-itemsets:\n\n{ } {\n\na,b a,c} { } {a,e}\n\na,d ... { e,g} supp: 7 25 15 23 3\n\n3-itemsets: {a,c,d}\n{a,c,e} {b,d,g} ... supp: 15 22 15\n\n4-",
  "504": "ts: {a,c,d}\n{a,c,e} {b,d,g} ... supp: 15 22 15\n\n4-itemsets: {a,c,d,e}\nsupp: 12\n\nApriori Algorithm:\n\nInput: Matrix M\n\nL1={frequent 1-itemsets; i such that Supp(i) ≥θ}.\n\nFor k = 2, while Lk−1 = ∅(while there are large k −1-itemsets), k + +\n\n̸\n\n• Ck = apriori",
  "505": "are large k −1-itemsets), k + +\n\n̸\n\n• Ck = apriori gen(Lk\n1) generate candidate itemsets of size k −\n• Lk = {c : c ∈Ck, Supp(c) ≥θ} frequent itemsets of size k (loop over\ntransactions, scan the database)\n\nend\n\nS\n\nOutput: k Lk.\n\n3",
  "506": "The subroutine apriori gen joins Lk−1 to Lk−1.\n\napriori gen Subroutine:\n\nInput: Lk−1\n\nFind all pairs of itemsets in Lk−1 where the first k −2 items are identical.\n\ntoo big Union them (lexicographically) to get Ck ,\n\ne.g.,{a, b, c, d, e, f}, {a, b, c, d, e,",
  "507": "get Ck ,\n\ne.g.,{a, b, c, d, e, f}, {a, b, c, d, e, g} →{a, b, c, d, e, f, g}\n\nPrune: Ck = {\ntoo big c ∈Ck\n, all (k −1)-subsets cs of c obey cs ∈Lk−1}.\n\nOutput: Ck.\n\nExample of Prune step: consider {a, b, c, d, e, f, g} too big which is in Ck , and I want t",
  "508": " d, e, f, g} too big which is in Ck , and I want to know whether it’s in Ck. Look at {a, b, c, d, e, f, g}, {a, b, c, d, e, f, g},{a, b, c, d, e, f, g}, {a, b, c, d, e, f, g}, etc. If any are not in L6, then prune {a, b, c, d, e, f, g} from L7.\n\nExample of",
  "509": "n prune {a, b, c, d, e, f, g} from L7.\n\nExample of the aprior\n\ni a lgori\n\nthm.\n\n4",
  "510": "• Apriori scans the database at most how many times?\n\n• Huge number of candidate sets.\n\n/\n\n•\nwned huge number of apriori-like papers.\n\nSpa\n\nWhat do you do with the rules after they’re generated?\n\n• Information overload (give up)\n\n• Order rules by “interest",
  "511": "ion overload (give up)\n\n• Order rules by “interestingness”\n\n– Confidence Supp(a\nˆP(b|a) =\n∪b) Supp(a)\n\n– “Lift”/“Interest”\nˆP(b|a) Supp(b)\n=\nˆP(b) −Supp(a b 1 ∪) Supp(a) :\n\n– Hundreds!\n\nResearch questions:\n\n• mining more than just itemsets (e.g, sequences,",
  "512": "\n• mining more than just itemsets (e.g, sequences, trees, graphs)\n\n• incorporating taxonomy in items\n\n• boolean logic and “logical analysis of data”\n\n• Cynthia’s questions: Can we use rules within ML to get good predictive models?\n\n5",
  "513": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "514": "R for Machine Learning\n\nAllison Chang\n\n1 Introduction\n\nIt is common for today’s scientific and business industries to collect large amounts of data, and the ability to analyze the data and learn from it is critical to making informed decisions. Familiarity",
  "515": "critical to making informed decisions. Familiarity with software such as R allows users to visualize data, run statistical tests, and apply machine learning algorithms. Even if you already know other software, there are still good reasons to learn R:\n\n1. R",
  "516": "re, there are still good reasons to learn R:\n\n1. R is free. If your future employer does not already have R installed, you can always download it for free, unlike other proprietary software packages that require expensive licenses. No matter where you trav",
  "517": "quire expensive licenses. No matter where you travel, you can have access to R on your computer.\n\n2. R gives you access to cutting-edge technology. Top researchers develop statistical learning methods\nin R, and new algorithms are constantly added to the li",
  "643": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "525": "nd would be different on a Mac):\n\n> setwd(\"C:\\\\Datasets\")\n\n1.2 Installing and loading packages\n\nFunctions in R are grouped into packages, a number of which are automatically loaded when you start R. These include “base,” “utils,” “graphics,” and “stats.” M",
  "526": "nclude “base,” “utils,” “graphics,” and “stats.” Many of the most essential and frequently used functions come in these packages. However, you may need to download additional packages to obtain other useful functions. For example, an important classificati",
  "527": " functions. For example, an important classification method called Support Vector Machines is contained in a package called\n\n1",
  "528": "“e1071.” To install this package, click “Packages” in the top menu, then “Install package(s)...” When asked to select a CRAN mirror, choose a location close to you, such as “Canada (ON).” Finally select “e1071.” To load the package, type library(e1071) at ",
  "529": "071.” To load the package, type library(e1071) at the command prompt. Note that you need to install a package only once, but that if you want to use it, you need to load it each time you start R.\n\n1.3 Running code\n\nYou could use R by simply typing everythi",
  "530": "ng code\n\nYou could use R by simply typing everything at the command prompt, but this does not easily allow you to save, repeat, or share your code. Instead, go to “File” in the top menu and click on “New script.” This opens up a new window that you can sav",
  "531": "ript.” This opens up a new window that you can save as a .R file. To execute the code you type into this window, highlight the lines you\nwish to run, and press Ctrl-R on a PC or Command-Enter on a Mac. If you want to run an entire script, make\nsure the scr",
  "532": "ou want to run an entire script, make\nsure the script window is on top of all others, go to “Edit,” and click “Run all.” Any lines that are run appear in red at the command prompt.\n\n1.4 Help in R\n\nThe functions in R are generally well-documented. To find d",
  "533": "ions in R are generally well-documented. To find documentation for a particular function, type ?\nfollowed directly by the function name at the command prompt. For example, if you need help on the “sum” function, type ?sum. The help window that pops up typi",
  "534": "tion, type ?sum. The help window that pops up typically contains details on both the input and output for the function of interest. If you are getting errors or unexpected output, it is likely that your input is insufficient or invalid, so use the document",
  "535": "ut is insufficient or invalid, so use the documentation to figure out the proper way to call the function.\n\nIf you want to run a certain algorithm but do not know the name of the function in R, doing a Google search of R plus the algorithm name usually bri",
  "536": "le search of R plus the algorithm name usually brings up information on which function to use.\n\n2 Datasets\n\nWhen you test any machine learning algorithm, you should use a variety of datasets. R conveniently comes with its own datasets, and you can view a l",
  "537": " comes with its own datasets, and you can view a list of their names by typing data() at the command prompt. For instance, you may see a dataset called “cars.” Load the data by typing data(cars), and view the data by typing cars.\n\nAnother useful source of ",
  "538": "he data by typing cars.\n\nAnother useful source of available data is the UCI Machine Learning Repository, which contains a couple hundred datasets, mostly from a variety of real applications in science and business. The repository is located at\nhttp://archi",
  "539": "usiness. The repository is located at\nhttp://archive.ics.uci.edu/ml/datasets.html. These data are often used by machine learning researchers\nto develop and compare algorithms. We have downloaded a number of datasets for your use, and you can find the text ",
  "540": " datasets for your use, and you can find the text files\n\n. These\n\ninclude:\n\nin the Datasets section\n\nName Rows Cols Data Iris 150 4 Real Wine 178 13 Integer, Real Haberman’s Survival 306 3 Integer Housing 506 14 Categorical, Integer, Real Blood Transfusion",
  "541": "06 14 Categorical, Integer, Real Blood Transfusion Service Center 748 4 Integer Car Evaluation 1728 6 Categorical Mushroom 8124 119 Binary\nPen-based Recognition of Handwritten Digits\n10992 16 Integer\n\n2",
  "542": "You are encouraged to download your own datasets from the UCI site or other sources, and to use R to study the data. Note that for all except the Housing and Mushroom datasets, there is an additional class attribute column that is not included in the colum",
  "543": "attribute column that is not included in the column counts. Also note that if you download the Mushroom dataset from the UCI site, it has 22 categorical features; in our version, these have been transformed into 119 binary features.\n\n3 Basic Functions\n\nIn ",
  "721": "lusterin g.\n\nchical c\n\nlevels o f hierar\n\nSeveral\n\nStatistical Learning, Springer, 2009.\n\nApplication Slides\n\n8",
  "544": " into 119 binary features.\n\n3 Basic Functions\n\nIn this section, we cover how to create data tables, and analyze and plot data. We demonstrate by example how to use various functions. To see the value(s) of any variable, vector, or matrix at any time, simpl",
  "545": "any variable, vector, or matrix at any time, simply enter its name in the command line; you are encouraged to do this often until you feel comfortable with how each data structure\nis being stored. To see all the objects in your workspace, type ls(). Also n",
  "546": "l the objects in your workspace, type ls(). Also note that the arrow operator <- sets\nthe left-hand side equal to the right-hand side, and that a comment begins with #.\n\n3.1 Creating data\n\nTo create a variable x and set it equal to 1, type x <- 1. Now supp",
  "547": "ble x and set it equal to 1, type x <- 1. Now suppose we want to generate the vector [1, 2, 3, 4, 5],\nand call the vector v. There are a couple different ways to accomplish this:\n\n> v <- 1:5\n> v <- c(1,2,3,4,5)\n# c can be used to concatenate multiple vecto",
  "548": "4,5)\n# c can be used to concatenate multiple vectors\n> v <- seq(from=1,to=5,by=1)\n\nThese can be row vectors or column vectors. To generate a vector v0 of six zeros, use either of the following. Clearly the second choice is better if you are generating a lo",
  "549": "second choice is better if you are generating a long vector.\n\n> v0 <- c(0,0,0,0,0,0)\n> v0 <- seq(from=0,to=0,length.out=6)\n\nWe can combine vectors into matrices using cbind and rbind. For instance, if v1, v2, v3, and v4 are vectors of the same length, we c",
  "550": "2, v3, and v4 are vectors of the same length, we can combine them into matrices, using them either as columns or as rows:\n\n> v1 <- c(1,2,3,4,5)\n> v2 <- c(6,7,8,9,10)\n> v3 <- c(11,12,13,14,15)\n> v4 <- c(16,17,18,19,20)\n> cbind(v1,v2,v3,v4)\n> rbind(v1,v2,v3,",
  "551": "7,18,19,20)\n> cbind(v1,v2,v3,v4)\n> rbind(v1,v2,v3,v4)\n\nAnother way to create the second matrix is to use the matrix function to reshape a vector into a matrix of the right dimensions.\n\n> v <- seq(from=1,to=20,by=1)\n> matrix(v, nrow=4, ncol=5)\n\nNotice that ",
  "552": "20,by=1)\n> matrix(v, nrow=4, ncol=5)\n\nNotice that this is not exactly right—we need to specify that we want to fill in the matrix by row.\n\n> matrix(v, nrow=4, ncol=5, byrow=TRUE)\n\nIt is often helpful to name the columns and rows of a matrix using colnames ",
  "553": "e the columns and rows of a matrix using colnames and rownames. In the following, first we save the matrix as matrix20, and then we name the columns and rows.\n\n3",
  "554": "> matrix20 <- matrix(v, nrow=4, ncol=5, byrow=TRUE)\n> colnames(matrix20) <- c(\"Col1\",\"Col2\",\"Col3\",\"Col4\",\"Col5\")\n> rownames(matrix20) <- c(\"Row1\",\"Row2\",\"Row3\",\"Row4\")\n\nYou can type colnames(matrix20)/rownames(matrix20) at any point to see the column/row ",
  "555": "ames(matrix20) at any point to see the column/row names for matrix20.\nTo access a particular element in a vector or matrix, index it by number or by name with square braces:\n\n> v[3]\n# third element of v\n> matrix20[,\"Col2\"]\n# second column of matrix20\n> mat",
  "556": "atrix20[,\"Col2\"]\n# second column of matrix20\n> matrix20[\"Row4\",]\n# fourth row of matrix20\n> matrix20[\"Row3\",\"Col1\"]\n# element in third row and first column of matrix20\n> matrix20[3,1]\n# element in third row and first column of matrix20\n\nYou can find the le",
  "557": " and first column of matrix20\n\nYou can find the length of a vector or number of rows or columns in a matrix using length, nrow, and ncol.\n\n> length(v1)\n> nrow(matrix20)\n> ncol(matrix20)\n\nSince you will be working with external datasets, you will need funct",
  "558": "orking with external datasets, you will need functions to read in data tables from text files. For instance, suppose you wanted to read in the Haberman’s Survival dataset (from the UCI Repository). Use the read.table function:\n\ndataset <- read.table(\"C:\\\\D",
  "559": "read.table function:\n\ndataset <- read.table(\"C:\\\\Datasets\\\\haberman.csv\", header=FALSE, sep=\",\")\n\nThe first argument is the location (full path) of the file. If the first row of data contains column names, then the\nsecond argument should be header = TRUE, ",
  "560": "then the\nsecond argument should be header = TRUE, and otherwise it is header = FALSE. The third argument contains\nthe delimiter. If the data are separated by spaces or tabs, then the argument is sep = \" \" and sep = \"\\t\"\nrespectively. The default delimiter ",
  "561": "nd sep = \"\\t\"\nrespectively. The default delimiter (if you do not include this argument at all) is “white space” (one or more spaces, tabs, etc.). Alternatively, you can use setwd to change directory and use only the file name in the read.table function. If",
  "562": " only the file name in the read.table function. If the delimiter is a comma, you can also use read.csv and leave off the sep argument:\n\ndataset <- read.csv(\"C:\\\\Datasets\\\\haberman.csv\", header=FALSE)\n\nUse write.table to write a table to a file. Type ?write",
  "563": "rite.table to write a table to a file. Type ?write.table to see details about this function. If you need to write text to a file, use the cat function.\n\nA note about matrices versus data frames: A data frame is similar to a matrix, except it can also inclu",
  "564": "e is similar to a matrix, except it can also include\nnon-numeric attributes. For example, there may be a column of characters. Some functions require the data\npassed in to be in the form of a data frame, which would be stated in the documentation. You can ",
  "565": "ich would be stated in the documentation. You can always coerce a matrix into a data frame using as.data.frame. See Section 3.6 for an example.\n\nA note about factors: A factor is essentially a vector of categorical variables, encoded using integers. For in",
  "566": "egorical variables, encoded using integers. For instance, if each example in our dataset has a binary class attribute, say 0 or 1, then that attribute can be represented as a factor. Certain functions require one of their arguments to be a factor. Use as.f",
  "567": "re one of their arguments to be a factor. Use as.factor to encode a vector as a factor. See Sections 4.5 and 4.9 for examples.\n\n3.2 Sampling from probability distributions\n\nThere are a number of functions for sampling from probability distributions. For ex",
  "568": "or sampling from probability distributions. For example, the following commands\ngenerate random vectors of the user-specified length n from distributions (normal, exponential, poisson, uniform,\nbinomial) with user-specified parameters. There are other dist",
  "569": "th user-specified parameters. There are other distributions as well.\n\n4",
  "570": "> norm_vec <- rnorm(n=10, mean=5, sd=2)\n> exp_vec <- rexp(n=100, rate=3)\n> pois_vec <- rpois(n=50, lambda=6)\n> unif_vec <- runif(n=20, min=1, max=9)\n> bin_vec <- rbinom(n=20, size=1000, prob=0.7)\n\nSuppose you have a vector v of numbers. To randomly sample,",
  "571": "ou have a vector v of numbers. To randomly sample, say, 25 of the numbers, use the sample function:\n\n> sample(v, size=25, replace=FALSE)\n\nIf you want to sample with replacement, set the replace argument to TRUE. If you want to generate the same random vect",
  "572": "TRUE. If you want to generate the same random vector each time you call one of the random functions listed above, pick a “seed” for the random number generator using set.seed, for example set.seed(100).\n\n3.3 Analyzing data\n\nTo compute the mean, variance, s",
  "573": "3 Analyzing data\n\nTo compute the mean, variance, standard deviation, minimum, maximum, and sum of a set of numbers, use mean, var, sd, min, max, and sum. There are also rowSum and colSum to find the row and column sums for a matrix.\nTo find the component-w",
  "574": " column sums for a matrix.\nTo find the component-wise absolute value and square root of a set of numbers, use abs and sqrt. Correlation\nand covariance for two vectors are computed with cor and cov respectively.\n\nLike other programming languages, you can wr",
  "575": "ely.\n\nLike other programming languages, you can write if statements, and for and while loops. For instance, here is a simple loop that prints out even numbers between 1 and 10 (%% is the modulo operation):\n\n> for (i in 1:10){\n+ if (i %% 2 == 0){\n+\ncat(past",
  "576": "\n> for (i in 1:10){\n+ if (i %% 2 == 0){\n+\ncat(paste(i, \"is even.\\n\", sep=\" \"))\n# use paste to concatenate strings\n+\n}\n+ }\n\nThe 1:10 part of the for loop can be specified as a vector. For instance, if you wanted to loop over indices 1, 2, 3, 5, 6, and 7, yo",
  "577": "nted to loop over indices 1, 2, 3, 5, 6, and 7, you could type for (i in c(1:3,5:7)). To pick out the indices of elements in a vector that satisfy a certain property, use which, for example:\n\n> which(v >= 0)\n# indices of nonnegative elements of v\n> v[which",
  "578": ")\n# indices of nonnegative elements of v\n> v[which(v >= 0)]\n# nonnegative elements of v\n\n3.4 Plotting data\n\nWe use the Haberman’s Survival data (read into data frame dataset) to demonstrate plotting functions. Each row of data represents a patient who had ",
  "579": "ns. Each row of data represents a patient who had surgery for breast cancer. The three features are: the age of the patient at the time of surgery, the year of the surgery, and the number of positive axillary nodes detected. Here we plot:\n\n1. Scatterplot o",
  "580": "ry nodes detected. Here we plot:\n\n1. Scatterplot of the first and third features,\n\n2. Histogram of the second feature,\n\n3. Boxplot of the first feature.\n\nTo put all three plots in a 1 × 3 matrix, use par(mfrow=c(1,3)). To put each plot in its own window, u",
  "581": "row=c(1,3)). To put each plot in its own window, use\nwin.graph() to create new windows.\n\n> plot(dataset[,1], dataset[,3], main=\"Scatterplot\", xlab=\"Age\", ylab=\"Number of Nodes\", pch=20)\n> hist(dataset[,2], main=\"Histogram\", xlab=\"Year\", ylab=\"Count\")\n> box",
  "582": "main=\"Histogram\", xlab=\"Year\", ylab=\"Count\")\n> boxplot(dataset[,1], main=\"Boxplot\", xlab=\"Age\")\n\n5",
  "583": "Scatterplot Histogram Boxplot\n\n0 10 20 30 40 50\n\n0 10 20 30 40 50 60\n\n30 40 50 60 70 80\n\nNumber of Nodes\n\nCount\n\n30 40 50 60 70 80 58 60 62 64 66 68\n\nAge Year Age\n\nFigure 1: Plotting examples.\n\nThe pch argument in the plot function can be varied to change ",
  "584": "ment in the plot function can be varied to change the marker. Use points and lines to add extra points and lines to an existing plot. You can save the plots in a number of different formats; make sure the plot window is on top, and go to “File” then “Save ",
  "585": "lot window is on top, and go to “File” then “Save as.”\n\n3.5 Formulas\n\nCertain functions have a “formula” as one of their arguments. Usually this is a way to express the form of a model. Here is a simple example. Suppose you have a response variable y and i",
  "586": "mple. Suppose you have a response variable y and independent variables x1, x2, and\nx3. To express that y depends linearly on x1, x2, and x3, you would use the formula y ∼ x1 + x2 + x3, where\ny, x1, x2, and x3 are also column names in your data matrix. See ",
  "587": "x3 are also column names in your data matrix. See Section 3.6 for an example. Type ?formula for details on how to capture nonlinear models.\n\n3.6 Linear regression\n\nOne of the most common modeling approaches in statistical learning is linear regression. In ",
  "588": " in statistical learning is linear regression. In R, use the lm function to generate these models. The general form of a linear regression model is\n\nY = β0 + β1X1 + β2X2 + · · · + βkXk + ε,\n\nwhere ε is normally distributed with mean 0 and some variance σ2 ",
  "589": "ally distributed with mean 0 and some variance σ2 .\n\nLet y be a vector of dependent variables, and x1 and x2 be vectors of independent variables. We want to find\nthe coefficients of the linear regression model Y = β0 + β1X1 + β2X2 + ε. The following comman",
  "590": "del Y = β0 + β1X1 + β2X2 + ε. The following commands generate the\nlinear regression model and give a summary of it.\n\n> lm_model <- lm(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)))\n> summary(lm_model)\n\nThe vector of coefficients for the model is containe",
  "591": "e vector of coefficients for the model is contained in lm model$coefficients.\n\n4 Machine Learning Algorithms\n\nWe give the functions corresponding to the algorithms covered in class. Look over the documentation for each function on your own as only the most",
  "592": "ion for each function on your own as only the most basic details are given in this tutorial.\n\n6",
  "593": "4.1 Prediction\n\nFor most of the following algorithms (as well as linear regression), we would in practice first generate the model using training data, and then predict values for test data. To make predictions, we use the predict function. To see document",
  "594": "ions, we use the predict function. To see documentation, go to the help page for the algorithm, and then scroll through the Contents menu on the left side to find the corresponding predict function. Or simply type ?predict.name, where name is the function ",
  "595": "ly type ?predict.name, where name is the function corresponding to the algorithm. Typically, the first argument is the variable in which you saved the model, and the second argument is a matrix or data frame of test data. Note that when you call the functi",
  "596": "e of test data. Note that when you call the function, you can just type predict instead of predict.name. For instance, if we were to predict for the linear regression model above, and x1 test and x2 test are vectors containing test data, we can use the com",
  "597": "e vectors containing test data, we can use the command\n\n> predicted_values <- predict(lm_model, newdata=as.data.frame(cbind(x1_test, x2_test)))\n\n4.2 Apriori\n\nTo run the Apriori algorithm, first install the arules package and load it. See Section 1.2 for in",
  "598": "arules package and load it. See Section 1.2 for installing and loading new packages. Here is an example of how to run the Apriori algorithm using the Mushroom dataset. Note that the dataset must be a binary incidence matrix; the column names should corresp",
  "599": " incidence matrix; the column names should correspond to the “items” that make up the “transactions.” The following commands print out a summary of the results and a list of the generated rules.\n\n> dataset <- read.csv(\"C:\\\\Datasets\\\\mushroom.csv\", header =",
  "600": "<- read.csv(\"C:\\\\Datasets\\\\mushroom.csv\", header = TRUE)\n> mushroom_rules <- apriori(as.matrix(dataset), parameter = list(supp = 0.8, conf = 0.9))\n> summary(mushroom_rules)\n> inspect(mushroom_rules)\n\nYou can modify the parameter settings depending on your ",
  "601": "n modify the parameter settings depending on your desired support and confidence thresholds.\n\n4.3 Logistic Regression\n\nYou do not need to install an extra package for logistic regression. Using the same notation as in Section 3.6, the command is:\n\n> glm_mo",
  "602": "ation as in Section 3.6, the command is:\n\n> glm_mod <-glm(y ∼ x1+x2, family=binomial(link=\"logit\"), data=as.data.frame(cbind(y,x1,x2)))\n\n4.4\nK-Means Clustering\n\nYou do not need an extra package. If X is the data matrix and m is the number of clusters, then",
  "603": " data matrix and m is the number of clusters, then the command is:\n\n> kmeans_model <- kmeans(x=X, centers=m)\n\n4.5\nk-Nearest Neighbor Classification\n\nInstall and load the class package. Let X train and X test be matrices of the training and test data respec",
  "604": "t be matrices of the training and test data respectively, and labels be a binary vector of class attributes for the training examples. For k equal to K, the command is:\n\n> knn_model <- knn(train=X_train, test=X_test, cl=as.factor(labels), k=K)\n\nThen knn mo",
  "605": "st=X_test, cl=as.factor(labels), k=K)\n\nThen knn model is a factor vector of class attributes for the test set.\n\n7",
  "606": "4.6 Na¨ıve Bayes\n\nInstall and load the e1071 package. Using the same notation as in Section 3.6, the command is:\n\n> nB_model <- naiveBayes(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)))\n\n4.7 Decision Trees (CART)\n\nCART is implemented in the rpart package",
  "607": "s (CART)\n\nCART is implemented in the rpart package. Again using the formula, the command is\n\n> cart_model <- rpart(y ∼ x1 + x2, data=as.data.frame(cbind(y,x1,x2)), method=\"class\")\n\nYou can use plot.rpart and text.rpart to plot the decision tree.\n\n4.8 AdaBo",
  "608": "d text.rpart to plot the decision tree.\n\n4.8 AdaBoost\n\nThere are a number of different boosting functions in R. We show here one implementation that uses decision trees as base classifiers. Thus the rpart package should be loaded. Also, the boosting functi",
  "609": "ackage should be loaded. Also, the boosting function ada is in the ada\npackage. Let X be the matrix of features, and labels be a vector of 0-1 class labels. The command is\n\n> boost_model <- ada(x=X, y=labels)\n\n4.9 Support Vector Machines (SVM)\n\nThe SVM alg",
  "610": "s)\n\n4.9 Support Vector Machines (SVM)\n\nThe SVM algorithm is in the e1071 package. Let X be the matrix of features, and labels be a vector of 0-1 class\nlabels. Let the regularization parameter be C. Use the following commands to generate the SVM model and v",
  "611": "following commands to generate the SVM model and view details:\n\n> svm_model <- svm(x=X, y=as.factor(labels), kernel =\"radial\", cost=C)\n> summary(svm_model)\n\nThere are a variety of parameters such as the kernel type and value of the C parameter, so check th",
  "612": "nel type and value of the C parameter, so check the documentation on how to alter them.\n\n8",
  "613": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "614": "Fundamentals of Learning\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nImportant Problems in Data Mining\n\n1. Finding patterns (correlations) in large datasets\n-e.g. (Diapers → Beer). Use Apriori!\n\n2. Clustering - grouping data into clusters that “belong” togethe",
  "615": " grouping data into clusters that “belong” together - objects\nwithin a cluster are more similar to each other than to those in other clusters.\n\n• Kmeans, Kmedians\n\ni=1, xi ∈X ⊂ Rn\n\n• Input: {xi}m\n\n• Output: f : X →{1, . . . , K} (K clusters)\n\n• clustering ",
  "616": " f : X →{1, . . . , K} (K clusters)\n\n• clustering consumers for market research, clustering genes into families, image segmentation (medical imaging)\n\n3. Classification\n\n• Input: {(xi, yi)}m “examples,” “instances with labels,” “observations”\ni=1\n• xi ∈X ,",
  "617": "stances with labels,” “observations”\ni=1\n• xi ∈X , yi ∈ {−1, 1} “binary”\n\n• Output: f : X → R and use sign(f) to classify.\n\n• automatic handwriting recognition, speech recognition, biometrics, doc­ ument classification\n\n• “LeNet”\n\n4. Regression\n\n1\n\n[IMAGE_",
  "618": "assification\n\n• “LeNet”\n\n4. Regression\n\n1\n\n[IMAGE_OCR 1]\n3 +\n\n+ + o\n\nre -\n. SE",
  "619": "• Input: {(xi, yi)}m\n\ni=1, xi ∈X , yi ∈ R\n\n• Output: f : X → R\n\n• predicting an individual’s income, predict house prices, predict stock prices, predict test scores\n\n5. Ranking (later) - in between classification and regression. Search engines\nuse ranking ",
  "620": "cation and regression. Search engines\nuse ranking methods\n\n6. Density Estimation - predict conditional probabilities\n\ni=1, xi ∈X , yi ∈ {−1, 1}\n\n• {(xi, yi)}m\n\n• Output: f : X → [0, 1] as “close” to P (y = 1|x) as possible.\n\n• estimate probability of failu",
  "621": "1|x) as possible.\n\n• estimate probability of failure, probability to default on loan\n\nRule mining and clustering are unsupervised methods (no ground truth), and classification, ranking, and density estimation are supervised methods (there is ground truth).",
  "622": "on are supervised methods (there is ground truth). In all of these problems, we do not assume we know the distribution that the data are drawn from!\n\nTraining and Testing (in-sample and out-of-sample) for supervised learning\n\nTraining: training data are in",
  "623": "upervised learning\n\nTraining: training data are input, and model f is the output.\n\ni=1 =⇒ Algorithm =⇒ f.\n\n{(xi, yi)}m\n\nTesting: You want to predict y for a new x, where (x, y) comes from the same distribution as {(xi, yi)}m\n\ni=1.\n\nThat is, (x, y) ∼ D(X , ",
  "624": "ion as {(xi, yi)}m\n\ni=1.\n\nThat is, (x, y) ∼ D(X , Y) and each (xi, yi) ∼ D(X , Y).\n\nCompute f(x) and compare it to y. How well does f(x) match y? Measure goodness of f using a loss function R : Y × Y → R:\n\nRtest(f) = E(x,y)∼DR(f(x), y)\n\n=\nR(f(x), y)dD(x, y",
  "625": "Rtest(f) = E(x,y)∼DR(f(x), y)\n\n=\nR(f(x), y)dD(x, y). (x,y)∼D\n\n2",
  "626": "Rtest is also called the true risk or the test error.\n\nCan we calculate Rtest?\n\nWe want Rtest to be small, to indicate that f(x) would be a good predictor (“es­ timator”) of y.\n\nFor instance\n\nR(f(x), y) = (f(x) − y)2\nleast squares loss, or\nR(f(x), y) = 1[s",
  "627": "f(x) − y)2\nleast squares loss, or\nR(f(x), y) = 1[sign(f(x))\n(mis)classification error\n=y]\n\nWhich problems might these loss functions correspond to?\n\nHow can we ensure Rtest(f) is small?\n\nLook at how well f performs (on average) on {(xi, yi)}i.\n\nm m 1\nRtrai",
  "628": "performs (on average) on {(xi, yi)}i.\n\nm m 1\nRtrain(f) =\nR(f(xi), yi).\nm i=1\nRtrain is also called the empirical risk or training error. For example,\n\nm m 1\nRtrain(f) =\n=yi].\n1[sign(f(xi))\nm i=1\n\n(How many handwritten digits did f classify incorrectly?)\n\nS",
  "629": "handwritten digits did f classify incorrectly?)\n\nSay our algorithm constructs f so that Rtrain(f) is small. If Rtrain(f) is small, hopefully Rtest(f) is too.\n\nWe would like a guarantee on how close Rtrain is to Rtest . When would it be close to Rtest?\n\n• I",
  "630": "s to Rtest . When would it be close to Rtest?\n\n• If m is large.\n\n• If f is “simple.”\n\n3",
  "631": "Illustration\n\nIn one of the figures in the illustration, f:\n\n• was overfitted to the data\n\n• modeled the noise\n\n• “memorized” the examples, but didn’t give us much other useful information\n\n• doesn’t “generalize,” i.e., predict. We didn’t “learn” anything!",
  "632": "alize,” i.e., predict. We didn’t “learn” anything!\n\nComputational Learning Theory, a.k.a. Statistical Learning Theory, a.k.a., learning theory, and in particular, Vapnik’s Structural Risk Minimization (SRM) addresses generalization. Here’s SRM’s classic pi",
  "633": " addresses generalization. Here’s SRM’s classic picture:\n\nWhich is harder to check for, overfitting or underfitting?\n\n4",
  "634": "Computational learning theory addresses how to construct probabilistic guaran­ tees on the true risk. In order to do this, it quantifies the class of “simple models.”\n\nBias/Variance Tradeoff is related to learning theory (actually, bias is related to\nlearn",
  "635": "earning theory (actually, bias is related to\nlearning theory).\n\nInference Notes - Bias/Variance Tradeoff\n\n5",
  "636": "Regularized Learning Expression\n\nStructural Risk Minimization says that we need some bias in order to learn/generalize\n(avoid overfitting). Bias can take many forms:\n\n• “simple” models f(x) =\nλjx(j) where IλI2 =\nλ2 < C\nj 2 j j\n\n• “prior” in Bayesian statis",
  "637": "λI2 =\nλ2 < C\nj 2 j j\n\n• “prior” in Bayesian statistics\n\n• connectivity of neurons in the brain\n\nRegularized Learning Expression:\n\nm\n\nR(f(xi), yi) + CRreg(f)\n\ni This expression is kind of omnipresent. This form captures many algorithms: SVM, boosting, ridge",
  "638": "orm captures many algorithms: SVM, boosting, ridge regression, LASSO, and logistic regression.\n\nIn the regularized learning expression, the loss R(f(xi), yi) could be:\n\n• “least squares loss” (f(xi) − yi)2\n\n• “misclassification error” 1[yi\nf(xi))] = 1[yif(",
  "639": "\n• “misclassification error” 1[yi\nf(xi))] = 1[yif(xi)≤0]\n\n– Note that minimizing 1[yif(xi)≤0] is computationally hard. i\n\n̸=sign(\n\nCommons license. For more information, see http://ocw.mit.edu/fairuse.\n\n−yif(xi)\n• “logistic loss” log2 1 + e\n⇐= logistic reg",
  "640": "f(xi)\n• “logistic loss” log2 1 + e\n⇐= logistic regression\n\n6\n\n[IMAGE_OCR 1]\n35 — tj <0)\nexp)\n3 04,1 +exp yo)\n—— max(0,1-¥\"09)\n25 2 15 1\nu = SS\n==)\no\n-2 AS A -0.5 o 0.5 1 15 2\ny* F090.",
  "641": "• “hinge loss” max(0, 1 − yif(xi)) ⇐= SVM\n\n−yif(xi) ⇐ • “exponential loss” e\n= AdaBoost\n\nIn the regularized learning expression, we define a couple of options for Rreg(f).\nUsually f is linear, f(x) =\nj λjx(j). We choose Rreg(f) to be either:\n\nP\n\n2 =\nj λj\n\n",
  "644": "Bias/Variance Tradeoff\n\nA parameter is some quantity about a distribution that we would like to know.\nWe’ll estimate the parameter θ using an estimator θˆ. The bias of estimator θˆ for\nparameter θ is defined as:\n\n• Bias(θˆ,θ) := E(θˆ) − θ, where the expect",
  "645": "d as:\n\n• Bias(θˆ,θ) := E(θˆ) − θ, where the expectation is with respect to the distribution\nthat θˆ is constructed from.\n\nAn estimator whose bias is 0 is called unbiased. Contrast bias with:\n\n• Var(θˆ) = E(θˆ− E(θˆ))2 .\n\nFigure s howing the f our comb inat",
  "646": "θˆ− E(θˆ))2 .\n\nFigure s howing the f our comb inations of l ow and hi gh bias, and low and high variance.\n\nOf course, we’d like an estimator with low bias and low variance.\n\nA little bit of decision theory\n\n(The following is based on notes of David McAlles",
  "647": "\n(The following is based on notes of David McAllester.)\n\nLet’s say our data come from some distribution D on X × Y, where Y ⊂ R. Usually we don’t know D (we instead only have data) but for the moment, let’s say we know it. We want to learn a function f : X",
  "648": " say we know it. We want to learn a function f : X →Y.\n\nThen if we could choose any f we want, what would we choose? Maybe we’d choose f to minimize the least squares error:\n\nEx,y∼D[(y − f(x))2].\n\n1",
  "649": "It turns out that the f ∗ that minimizes the above error is the conditional expec­ tation!\n\nDraw a picture\n\nProposition.\n\nf ∗ (x) = Ey[y|x].\n\nProof. Consider each x separately. For each x there’s a marginal distribution on y. In other words, look at Ey[(y ",
  "650": " distribution on y. In other words, look at Ey[(y − f(x))2|x] for each x. So, pick an x. For this x, define ¯y to be Ey[y|x]. Now,\n\nEy[(y − f(x))2|x]\n= Ey[(y − y¯ + ¯y − f(x))2|x]\n= Ey[(y − y¯)2|x] + Ey[(¯y − f(x))2|x] + 2Ey[(y − y¯)(¯y − f(x))|x]\n= Ey[(y ",
  "651": " − f(x))2|x] + 2Ey[(y − y¯)(¯y − f(x))|x]\n= Ey[(y − y¯)2|x] + (¯y − f(x))2 + 2(¯y − f(x))Ey[(y − y¯)|x]\n= Ey[(y − y¯)2|x] + (¯y − f(x))2\n\nwhere the last step follows from the definition of ¯y.\n\nSo how do we pick f(x)? Well, we can’t do anything about the f",
  "652": " pick f(x)? Well, we can’t do anything about the first term, it doesn’t depend on f(x). The best choice of f(x) minimizes the second term,\nwhich happens at f(x) = y¯, where remember ¯y = Ey[y|x].\n\nSo we know for each x what to choose in order to minimize E",
  "653": "w for each x what to choose in order to minimize Ey[(y − f(x))2|x]. To complete the argument, note that:\n\nEx,y[(y − f(x))2] = Ex[Ey[(y − f(x))2|x]]\n\nand we have found the minima of the inside term for each x. •\n\nNote that if we’re interested instead in the",
  "654": "x. •\n\nNote that if we’re interested instead in the absolute loss Ex,y[|y − f(x)|], it is possible to show that the best predictor is the conditional median, that is,\nf(x) = median[y|x].\n\n2",
  "655": "Back to Bias/Variance Decomposition\n\nLet’s think about a situation where we created our function f using data S. Why would we do that of course?\n\nWe have training set S = (x1, y1), . . . , (xm, ym), where each example is drawn iid\nfrom D. We want to use S ",
  "656": "ach example is drawn iid\nfrom D. We want to use S to learn a function fS : X →Y.\n\nWe want to know what the error of fS is on average. In other words, we want to know what Ex,y,S[(y−fS(x))2] is. That will help us figure out how to minimize it.\nThis is going",
  "657": "lp us figure out how to minimize it.\nThis is going to be a neat result - it’s going to decompose into bias and variance\nterms!\n\nFirst, let’s consider some learning algorithm (which produced fS) and its ex­ pected prediction error:\n\nEx,y,S[(y − fS(x))2].\n\nR",
  "658": "pected prediction error:\n\nEx,y,S[(y − fS(x))2].\n\nRemember that the estimator fS is random, since it depends on the randomly drawn training data. Here, the expectation is taken with respect to a new ran­ domly drawn point x, y ∼ D and training data S ∼ Dm .",
  "659": "ly drawn point x, y ∼ D and training data S ∼ Dm .\n\nLet us define the mean prediction of the algorithm at point x to be:\n\n¯f(x) = ES[fS(x)].\n\nIn other words, to get this value, we’d get infinitely many training sets, run the learning algorithm on all of th",
  "660": "ning sets, run the learning algorithm on all of them to get infinite predictions fS(x) for each x.\n\n¯ Then for each x we’d average the predictions to get f(x).\n\nWe can now decompose the error, at a fixed x, as follows:\n\nEy,S[(y − fS(x))2]\n\n= Ey,S[(y − y¯ +",
  "661": " as follows:\n\nEy,S[(y − fS(x))2]\n\n= Ey,S[(y − y¯ + ¯y − fS(x))2]\n= Ey(y − y¯)2 + ES(¯y − fS(x))2 + 2Ey,S[(y − y¯)(¯y − fS(x))].\n\nThe third term here is zero, since Ey,S[(y−y¯)(¯y−fS(x))] = Ey(y−y¯)ES(¯y−fS(x)),\nand the first part of that is Ey(y − y¯) = 0.",
  "662": "x)),\nand the first part of that is Ey(y − y¯) = 0.\n\n3",
  "663": "The first term is the variance of y around its mean. We don’t have control over that when we choose fS. This term is zero if y is deterministically related to x.\n\nLet’s look at the second term:\n\nES(¯y − fS(x))2\n\n¯ ¯\n= ES(¯y − f(x) + f(x) − fS(x))2\n\n¯ ¯\n= E",
  "664": "x))2\n\n¯ ¯\n= ES(¯y − f(x) + f(x) − fS(x))2\n\n¯ ¯\n= ES(¯y − f(x))2 + ES(f¯(x) − fS(x))2 + 2ES[(¯y − f(x))(f¯(x) − fS(x))]\n\n¯ ¯ The last term is zero, since (¯y − f(x)) is a constant, and f(x) is the mean of ¯ fS(x) with respect to S. Also the first term isn’t",
  "665": "fS(x) with respect to S. Also the first term isn’t random. It’s (¯y − f(x))2 .\n\nPutting things together, what we have is this (reversing some terms):\n\nEy,S[(y − fS(x))2] = Ey(y − y¯)2 + ES(f¯(x) − fS(x))2 + (¯y − f¯(x))2 .\n\nIn this expression, the second t",
  "666": " (¯y − f¯(x))2 .\n\nIn this expression, the second term is the variance of our estimator around its mean. It controls how our predictions vary around its average prediction. The third term is the bias squared, where the bias is the difference between the av­",
  "667": ", where the bias is the difference between the av­ erage prediction and the true conditional mean.\n\nWe’ve just proved the following:\n\nTheorem.\n\nFor each fixed x, Ey,S[(y − fS(x))2] = vary|x(y) + varS(fS(x)) + bias(fS(x))2 .\n\nSo\nEx,y,S[(y − fS(x))2] = Ex[va",
  "668": " + bias(fS(x))2 .\n\nSo\nEx,y,S[(y − fS(x))2] = Ex[vary|x(y) + varS(fS(x)) + bias(fS(x))2].\n\nThat is the bias-variance decomposition.\n\nThe “Bias-Variance” tradeoff: want to choose fS to balance between reducing the\nsecond and third terms in order to make the ",
  "669": "g the\nsecond and third terms in order to make the lowest MSE. We can’t just minimize one or the other, it needs to be a balance. Sometimes, if you are willing to inject some bias, this can allow you to substantially reduce the variance. E.g., modeling with",
  "670": "tantially reduce the variance. E.g., modeling with lower degree polynomials, rather than higher degree polynomials.\n\n4",
  "671": "Question: Intuitively, what happens to the second term if fS fits the data per­ fectly every time (overfitting)?\n\nQuestion: Intuitively, what happens to the last two terms if fS is a flat line every time?\n\nThe bottom line: In order to predict well, you nee",
  "672": "The bottom line: In order to predict well, you need to strike a balance between bias and variance.\n\n• The variance term controls wiggliness, so you’ll want to choose simple func­ tions that can’t yield predictions that are too varied.\n\n• The bias term cont",
  "673": "ictions that are too varied.\n\n• The bias term controls how close the average model prediction is close to the truth, ¯y. You’ll need to pay attention to the data in order to reduce the bias term.\n\n• Since you can’t calculate either the bias or the variance",
  "674": "ou can’t calculate either the bias or the variance term, what we usually do is just impose some “structure” into the functions we’re fitting with, so the class of functions we are working with is small (e.g., low degree polynomials). We then try to fit the",
  "675": "., low degree polynomials). We then try to fit the data well using those functions. Hope­ fully this strikes the right balance of wiggliness (variance) and capturing the mean of the data (bias).\n\n• One thing we like to do is make assumptions on the distrib",
  "676": "g we like to do is make assumptions on the distribution D, or at least on the class of functions that might be able to fit well. Those assumptions each lead to a different algorithm (i.e. model). How well the algorithm works or not depends on how true the ",
  "677": "he algorithm works or not depends on how true the assumption is.\n\n• Even when we’re not working with least squares error, we hope a similar idea holds (and will work on proving that later in the course). We’ll use the same type of idea, where we impose som",
  "678": "’ll use the same type of idea, where we impose some structure, and hope it reduces wiggliness and will still give accurate predictions.\n\nGo back to the other notes!\n\n5",
  "679": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "680": "Clustering\n\nMIT 15.097 Course Notes Cynthia Rudin and S¸eyda Ertekin\n\nCredit: Dasgupta, Hastie, Tibshirani, Friedman\n\nClustering (a.k.a. data segmentation) Let’s segment a collection of examples into “clusters” so that objects within a cluster are more clo",
  "681": "ers” so that objects within a cluster are more closely related to one another than objects assigned to different clusters. We want to assign each ex­ ample xi to a cluster k ∈{1, ...., K}.\n\nThe K-Means algorithm is a very popular way to do this. It assumes",
  "722": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "682": "rithm is a very popular way to do this. It assumes points lie\nin Euclidean space.\n\nn Input: Finite set {x }m\ni 1=1, xi ∈ R\nOutput: z1, ..., zK cluster centers\n\nGoal: Minimize\n\ncost(z1, ..., zK ) :=\nmin Ixi − zkI2\n\n2. k i The choice of the squared norm is f",
  "683": "− zkI2\n\n2. k i The choice of the squared norm is fortuitous, it really helps simplify the math!\n\nn If we’re given points {zk}k, they can induce a Voronoi p artition of R : they break the space into cells where each cell corresponds to one of the zk’s. That",
  "684": "ere each cell corresponds to one of the zk’s. That is, each cell contains the region of space whose nearest representative is zk.\n\nDraw a picture\n\nWe can look at the examples in each of these regions of space, which are the clusters. Specifically,\n\nCk := {",
  "685": "ce, which are the clusters. Specifically,\n\nCk := {xi : the closest representative to xi is zk}.\n\nLet’s compute the cost another way. Before, we summed over examples, and then picked the right representative zk for each example. This time, we’ll sum over cl",
  "686": " zk for each example. This time, we’ll sum over clusters, and look at all the examples in that cluster:\n\ncost(z1, ..., zK ) =\nIxi − zkI2\n\n2.\n\nk {i:xi∈Ck}\n\n1",
  "687": "While we’re analyzing, we’ll need to consider suboptimal partitions of the data, where an example might not be assigned to the nearest representative. So we redefine the cost:\n\nX X\n\nk ∥xi −zk∥2 2. (1)\n\ncost(C1, ..., CK; z1, ..., zK) =\n\n{i:xi∈Ck}\n\nLet’s say",
  "688": "(C1, ..., CK; z1, ..., zK) =\n\n{i:xi∈Ck}\n\nLet’s say we only have one cluster to deal with. Call it C. The representative is z. The cost is then:\ncost(C; z) =\n∥xi −z∥2 2.\n\nX\n\n{ ∈} Where should we place z?\n\ni:xi C\n\nAs you probably guessed, we would put it at ",
  "689": "xi C\n\nAs you probably guessed, we would put it at the mean of the examples in C. But\nalso, the additional cost incurred by picking z = mean(C) can be characterized\nvery simply:\n\n̸\n\nLemma 1. For any set C ⊂Rn and any z ∈Rn,\n\ncost(C; z) = cost(C, mean(C)) + ",
  "690": "n and any z ∈Rn,\n\ncost(C; z) = cost(C, mean(C)) + C\nz mean(C) 2 2. | | · ∥− ∥\n\nLet’s go ahead and prove it. In order to do that, we need to do another biasvariance decomposition (this one’s pretty much identical to one of the ones we did before).\n\nLemma 2.",
  "691": "tical to one of the ones we did before).\n\nLemma 2. Let X Rn be any random variable. For any z Rn, we have: ∈ ∈\n\nEX∥X −z∥2\n2 = EX∥X −EXX∥2\n2 + ∥z −EXX∥2\n2.\n\nProof. Let x¯ := EXX.\n\nj −z(j))2 X\n\nEX∥X −z∥2\n2 = EX\n(X(j)\n\nj\n−x¯(j) + x¯(j) −zj)2\nX\n\n= EX\n(X(j)\n\nj\n",
  "692": "EX\n(X(j)\n\nj\n−x¯(j) + x¯(j) −zj)2\nX\n\n= EX\n(X(j)\n\nj\n−x¯(j))2 + EX\n(x¯(j)\n\nj −zj)2 X X\n\n= EX\n(X(j)\n\nj −x¯(j))(x¯(j) −z(j)) X\n\n+2EX\n(X(j)\n\n= EX∥X −x¯∥2\n2 + EX∥x¯ −z∥2\n2 + 0. ■\n\n2",
  "693": "To prove Lemma 1, pick a specific choice for X, namely X is a uniform random draw from the points xi in set C. So X has a discrete distribution. What will happen with this choice of X is that the expectation will reduce to the cost we already defined above",
  "694": "n will reduce to the cost we already defined above.\n\nX\n\nEX∥X −z∥2\n2 =\n(prob. that point i is chosen)∥xi −z∥2 2 i:xi C\n\n{ ∈} X 1\n=\n\n{i:xi∈C} |C ∥x | i −z∥2 1\n2 =\ncost(C, z) |C| (2)\n\nand if we use Lemma 2 substituting z to be x¯ (a.k.a., EXX, or mean(C)) and",
  "695": "stituting z to be x¯ (a.k.a., EXX, or mean(C)) and simplify as in (2):\n\n1 EX∥X −x¯∥2\n2 =\ncost(C, mean(C)). C | | (3)\n\nWe had already defined cost earlier, and the choice of X was nice because its expectation is just the cost. Let’s recopy Lemma 2’s stateme",
  "696": "n is just the cost. Let’s recopy Lemma 2’s statement here, using the x¯ notation:\n\nEX∥X −z∥2\n2 = EX∥X −x¯∥2\n2 + ∥z −x¯∥2\n2.\n\nPlugging in (2) and (3),\n\n1 1\ncost(C, z) =\ncost(C, mean(C)) +\n|C| |C ∥z −x¯∥2 | 2. (4)\n\nMultiplying through,\n\ncost(C; z) = cost(C, ",
  "697": ". (4)\n\nMultiplying through,\n\ncost(C; z) = cost(C, mean(C)) + |C| · ∥z −mean(C)∥2\n2.\n\nAnd that’s the statement of Lemma 1. ■\n\nTo really minimize the cost (1), you’d need to try all possible assignments of the m data points to K clusters. Uck! The number of ",
  "698": "e m data points to K clusters. Uck! The number of distinct assignments is (Jain and Dubes 1988):\n\nK 1\nS(m, K) =\n(\nK! k=1\n−1)K−k K km k\n\nX\n\nS(10, 4) = 34K, S(19, 4)\n1010, ... so not doable. ≈\n\nLet’s try some heuristic gradient-descent-ish method instead.\n\n3",
  "699": "The K-Means Algorithm\n\nChoose the value of K before you start.\n\nn Initialize centers z1, ..., zK ∈ R and clusters C1, ..., CK in any way. Repeat until there is no further change in cost: for each k: Ck ←{xi : the closest representative is zk}\nfor each k: z",
  "700": " : the closest representative is zk}\nfor each k: zk = mean(Ck)\n\nThis is simple enough, and takes O(Km) time per iteration.\n\nPPT demo\n\nOf course, it doesn’t always converge to the optimal solution.\n\nBut does the cost converge?\n\nLemma 3. During the course of",
  "701": " the cost converge?\n\nLemma 3. During the course of the K-Means algorithm, the cost monotonically\ndecreases.\n\n(t) (t) (t) (t) Proof. Let z1 , ..., zK , C1 , ..., CK denote the centers and clusters at the start of\nthe tth iterate of K-Means. The first step o",
  "702": "rt of\nthe tth iterate of K-Means. The first step of the iteration assigns each data point\nto its closest center, therefore, the cluster assignment is better:\n\n(t+1)\n(t+1)\n(t) (t) (t) (t) (t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 ,",
  "703": ".., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\nOn the second step, each cluster is re-centered at its mean, so the representatives\nare better. By Lemma 1,\n\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t+1)\n(t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cos",
  "704": ")\n(t) (t) cost(C1 , ..., CK , z1 , ..., zK ) ≤ cost(C1 , ..., CK , z1 , ..., zK ).\n\n•\n\nSo does the cost converge?\n\n4",
  "705": "Example of how K-Means could converge to the wrong thing\n\nHow might you make K-Means more likely to converge to the optimal?\n\nHow might you choose K? (Why can’t you measure test error?)\n\nOther ways to evaluate clusters (“cluster validation”)\n\nThere are loa",
  "706": "ate clusters (“cluster validation”)\n\nThere are loads of cluster validity measures, alternatives to the cost. Draw a picture\n\n• Davies-Baldwin Index - looks at average intracluster distance (within-cluster\ndistance) to the centroid (want it to be small), an",
  "707": "istance) to the centroid (want it to be small), and intercluster distances between centroids (want it to be large).\n\n• Dunn Index - looks pairwise at minimal intercluster distance (want it to be\nlarge) and maximal intracluster distance (want it to be small",
  "708": "maximal intracluster distance (want it to be small).\n\nExample: Microarray data. Have 6830 genes (rows) and 64 patients (columns). The color of each box is a measurement of the expression level of a gene. The expression level of a gene is basically how much",
  "709": "e expression level of a gene is basically how much of its special protein it is pro­ ducing. The physical chip itself doesn’t actually measure protein levels, but a proxy for them (which is RNA, which sticks to the DNA on the chip). If the color is green, ",
  "710": "s to the DNA on the chip). If the color is green, it means low expression levels, if the color is red, it means higher expression levels. Each patient is represented by a vector, which is the expression level of their genes. It’s a column vector with value",
  "711": "el of their genes. It’s a column vector with values given in color:\n\n5",
  "712": "Commons license. For more information, see http://ocw.mit.edu/fairuse.\n\nEach patient (column) has some type of cancer. Want to cluster patients to see whether patients with the same types of cancers cluster together. So each cluster center is an “average” ",
  "713": " together. So each cluster center is an “average” patient expression level vector for some type of cancer. It’s also a column vector\n\nGraph of sum of squa\n\nst\n\nu\n\ne r s .\n\nres vs. number of cl\n\nHm, there’s no kink in this figure. Compare K = 3 solution wit",
  "714": "no kink in this figure. Compare K = 3 solution with “true” clusters:\n\nT a b l e\n\nc o m p a r\n\ni n g\n\nK\n=\n\nue clust ers.\n\n3\n\ns o l u\n\nt i o n\n\nw\n\ni t h\n\nt r\n\nSpringer, 2009.\n\n6\n\n[IMAGE_OCR 1] oes ge\n= mal = af aa 2\n\naa ee Mery cian A EJ NEA A “ip yall bala\n",
  "715": "af aa 2\n\naa ee Mery cian A EJ NEA A “ip yall bala\n- ga el \"Ei a\" =\nELE Pizda z. pk el U mu ZAJ li ges hae HOF\n= z mimo cm s s '\nmyk rt) a tz Se c AE FEG 12 ie id» pet",
  "716": "It’s pretty good at keeping the same cancers in the same cluster. The two breast cancers in the 2nd cluster were actually melanomas that metastasized.\n\nGenerally we cluster genes, not patients. Would really like to get something like this in practice:\n\nCou",
  "717": " like to get something like this in practice:\n\nCourtesy of the Rockefeller University Press. Used with permission.\n\nFigure 7 from Rumfelt, Lynn, et al. \"Lineage Specification and Plasticity in CD19- Early B\ncell Precursors.\" Journal of Experimental Medicin",
  "718": "\ncell Precursors.\" Journal of Experimental Medicine 203 (2006): 675-87.\n\nwhere each row is a gene, and the columns are different immune cell types.\n\nA major issue with K-means: as K changes, cluster membership can change\narbitrarily. A solution is Hierarch",
  "719": "hip can change\narbitrarily. A solution is Hierarchical Clustering.\n\n• clusters at the next level of the hierarchy are created by merging clusters at the next lowest level.\n\n– lowest level: each cluster has 1 example\n\n– highest level: there’s only 1 cluster",
  "720": "1 example\n\n– highest level: there’s only 1 cluster, containing all of the data.\n\n7\n\n[IMAGE_OCR 1] MLP CLP Fr.A Fr. B&C A uma B pw EBFI c D ot",
  "723": "K-NN\n\n15.097 MIT, Spring 2012, Cynthia Rudin Credit: Seyda Ertekin\n\nK-Nearest Neighbors\n\n• Amongst the simplest of all machine learning algorithms. No eXplicit training or model. • Can be used both for classifcaton and regression.\n• Use XIs K-Nearest Neigh",
  "724": "sifcaton and regression.\n• Use XIs K-Nearest Neighbors to vote on what\nXIs label should be.\n\n1",
  "725": "K-Nearest Neighbors\n\n• Classify using the majority vote of the k closest training points\n\nX X X\n\n(a) 1-nearest neighbor\n(b) 2-nearest neighbor\n(c) 3-nearest neighbor\n\nK-Nearest Neighbors\n\n• K-NN algorithm does not explicitly compute decision\nboundaries. Th",
  "726": "oes not explicitly compute decision\nboundaries. The boundaries between distinct classes form a subset of the Voronoi diagram of the training data.\n\nEach line segment is equidistant to neighboring points.\n\n2",
  "727": "K-Nearest Neighbors\n\n• For regression: the value for the test eXample becomes the (weighted) average of the values of the K neighbors.\n\nMaking K-NN More Powerful\n\n• A good value for K can be determined by considering a range of K values.\n\n– K too small: we",
  "728": "onsidering a range of K values.\n\n– K too small: we’ll model the noise\n\n– K too large: neighbors include too many points from other classes\n\n• There are problems when there is a spread of distances among the K­\nNN. Use a distance-based voting scheme, where ",
  "729": " K­\nNN. Use a distance-based voting scheme, where closer neighbors have\nmore influence.\n\n• The distance measure has to be meaningful – attributes should be scaled\n\n– Eg. Income varies 10,000-1,000,000 while height varies 1.5-1.8 meters\n\n3",
  "730": "Pros/Cons to K-NN\n\nPros:\n\n• Simple and powerful. No need for tuning complex parameters to build a model.\n\n• No training involved (“lazy”). New training examples can be added easily.\n\nPros/Cons to K-NN\n\nCons:\n\n• Expensive and slow: O(md), m= # examples, d= ",
  "731": ":\n\n• Expensive and slow: O(md), m= # examples, d= # dimensions\n\n– To determine the nearest neighbor of a new point x, must compute the distance to all m training examples. Runtime performance is slow, but can be improved.\n\n• Pre-sort training examples into",
  "732": "an be improved.\n\n• Pre-sort training examples into fast data structures\n\n• Compute only an approximate distance\n\n• Remove redundant data (condensing)\n\n4",
  "733": "K-NN Applicatons\n\n• Handwriten character classifcaton using nearest neighbor in large databases. Smith, S.J et. al.; IEEE PAMI, 2004. Classify handwriten characters into numbers.\n\n• Fast content-based image retrieval based on equal-average K-nearest-neighb",
  "734": " retrieval based on equal-average K-nearest-neighbor\nsearch schemes z. Lu, H. Burkhardt, S. Boehmer; LNCS, 2006. CBIR (Content based image retrieval), return the closest neighbors as the relevant items to a query.\n\n• Use of K-Nearest Neighbor classifer for",
  "735": " query.\n\n• Use of K-Nearest Neighbor classifer for intrusion detecton\nYihua Liao, V.Rao Vemuri; Computers and Security Journal, 2002 Classify program behavior as normal or intrusive.\n\n• Fault Detecton Using the k-Nearest Neighbor Rule for Semiconductor Man",
  "736": " the k-Nearest Neighbor Rule for Semiconductor Manufacturing\nProcesses He, Q.P., Jin Wang; IEEE Transactons in Semiconductor Manufacturing, 2007 Early fault detecton in industrial systems.\n\n5",
  "737": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "738": "Na¨ıve Bayes\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nThanks to S¸eyda Ertekin Credit: Ng, Mitchell\n\nThe Na¨ıve Bayes algorithm comes from a generative model. There is an important distinction between generative and discriminative models. In all cases, we\nw",
  "739": "tive and discriminative models. In all cases, we\nwant to predict the label y, given x, that is, we want P(Y = y|X = x). Throughout the paper, we’ll remember that the probability distribution for measure P is\nover an unknown distribution over X × Y.\n\nNa¨ıve",
  "740": "s\nover an unknown distribution over X × Y.\n\nNa¨ıve Bayes Generative Model\nEstimate P(X = x|Y = y) and P(Y = y)\nand use Bayes rule to get P(Y = y|X = x)\nDiscriminative Model\nDirectly estimate P(Y = y|X = x)\n\nMost of the top 10 classification algorithms are ",
  "741": "\nMost of the top 10 classification algorithms are discriminative (K-NN, CART,\nC4.5, SVM, AdaBoost).\n\nFor Na¨ıve Bayes, we make an assumption that if we know the class label y, then we know the mechanism (the random process) of how x is generated.\n\nNa¨ıve B",
  "742": "e random process) of how x is generated.\n\nNa¨ıve Bayes is great for very high dimensional problems because it makes a very strong assumption. Very high dimensional problems suffer from the curse of dimensionality – it’s difficult to understand what’s going",
  "743": "nality – it’s difficult to understand what’s going on in a high dimensional space without tons of data.\n\nExample: Constructing a spam filter. Each example is an email, each dimension “j” of vector x represents the presence of a word.\n\n1",
  "744": " a\n\n1 0\n\naardvark\n\n\n\n0 .. x . 1 ..\n\n aardwolf  .  .. . 0\n\n=\n\n\n\n buy  ... zyxt\n\nThis x represents an email containing the words “a” and “buy”, but not “aardvark” or “zyxt”. The size of the vocabulary could be ∼50,000 words, so we are i",
  "745": "the vocabulary could be ∼50,000 words, so we are in a 50,000 dimensional space.\n\nNa¨ıve Bayes makes the assumption that the x(j)’s are conditionally independent\ngiven y. Say y = 1 means spam email, word 2,087 is “buy”, and word 39,831 is\n“price.” Na¨ıve Ba",
  "746": "87 is “buy”, and word 39,831 is\n“price.” Na¨ıve Bayes assumes that if y = 1 (it’s spam), then knowing x(2,087) = 1\n(email contains “buy”) won’t effect your belief about x(39,381) (email contains “price”).\n\nNote: This does not mean x(2,087) and x(39,831) ar",
  "747": "Note: This does not mean x(2,087) and x(39,831) are independent, that is,\n\nP(X(2,087) = x(2,087)) = P(X(2,087) = x(2,087)|X(39,831) = x(39,831)).\n\nIt only means they are conditionally independent given y. Using the definition of conditional probability rec",
  "748": "sing the definition of conditional probability recursively,\n\nP(X(1) = x(1), . . . , X(50,000) = x(50,000)|Y = y) =\nP(X(1) = x(1)|Y = y)P(X(2) = x(2)|Y = y, X(1) = x(1))\nP(X(3) = x(3)|Y = y, X(1) = x(1), X(2) = x(2))\n. . . P(X(50,000) = x(50,000)|Y = y, X(1",
  "749": ") = x(2))\n. . . P(X(50,000) = x(50,000)|Y = y, X(1) = x(1), . . . , X(49,999) = x(49,999)).\n\nThe independence assumption gives:\n\nP(X(1) = x(1), . . . , X(n) = x(n)|Y = y)\n= P(X(1) = x(1)|Y = y)P(X(2) = x(2)|Y = y) . . . P(X(n) = x(n)|Y = y)\nn\n=\nY\nP(X(j) = ",
  "750": "|Y = y) . . . P(X(n) = x(n)|Y = y)\nn\n=\nY\nP(X(j) = x(j)\n\nj=1\n|Y = y).\n(1)\n\n2",
  "751": "Bayes rule says\n\n(1) (1)\nP(Y = y)P(X(1) = x(1), ..., X(n) = x(n) Y = y)\nP(Y = y|X\n= x\n, ..., X(n) = x(n)) =\n|\nP(X(1) = x(1), ..., X(n) = x(n))\n\nso plugging in (1), we have\n\nn\nP(Y = y)\nP(X(j) = x(j)\nj=1\nY = y)\nP(Y = y|X(1) = x(1), ..., X(n) = x(n)) =\n\nQ |\n\n",
  "752": "y)\nP(Y = y|X(1) = x(1), ..., X(n) = x(n)) =\n\nQ |\n\nP(X(1) = x(1), ..., X(n) = x(n))\n\nFor a new test instance, called xtest, we want to choose the most probable value of y, that is\n\n(1) (n)\nP(Y = y˜) Q\n(1) j P(X\n= xtest, ..., X(n) = x\nyNB ∈ test arg max\n|Y =",
  "753": "P(X\n= xtest, ..., X(n) = x\nyNB ∈ test arg max\n|Y = y˜)\n\n(1) (n) y˜\nP(X(1) = x\n, ..., X(n) test\n= xtest)\nn\n= arg max P(Y = y˜)\nY\nP(X(j) = x(j)|Y = y˜).\ny˜\nj=1\n\n(j)\nSo now, we just need P(Y = y˜) for each possible y˜, and P(X(j) = xtest|Y = y˜)\nfor each j an",
  "754": "sible y˜, and P(X(j) = xtest|Y = y˜)\nfor each j and y˜. Of course we can’t compute those. Let’s use the empirical probability estimates:\n\n1 ˆ\n[y =y˜]\nP(Y = y˜) =\nP i i\n= fraction of data where the label is y˜\nm 1 i [ (j) ˆ\n\nP\n\n(j) P\nxi =x\n,y =y˜\n(j) (j) ] ",
  "755": "y˜\nm 1 i [ (j) ˆ\n\nP\n\n(j) P\nxi =x\n,y =y˜\n(j) (j) ] P(X\n= x\n|Y = y˜) =\ntest i\n= Conf(Y = y˜ →X(j)\n(j) test\n= xtest).\n1 i\n[yi=y˜]\n\nThat’s the simplest version of Na¨ıve Bayes:\n\nn ˆ ˆ (j) y\n∈arg max P(Y = y˜)\nY P(X(j) NB\n= xtest\ny˜\nj=1\n|Y = y˜).\n\nThere could p",
  "756": " P(X(j) NB\n= xtest\ny˜\nj=1\n|Y = y˜).\n\nThere could potentially be a problem that most of the conditional probabilities are 0 because the dimensionality of the data is very high compared to the amount ˆ (j)\nof data. This causes a problem because if even one P",
  "757": " data. This causes a problem because if even one P(X(j) = xtest|Y = y˜) is zero\nthen the whole right side is zero. In other words, if no training examples from class “spam” have the word “tomato,” we’d never classify a test example containing the word “tom",
  "758": "r classify a test example containing the word “tomato” as spam!\n\n3",
  "759": "To avoid this, we (sort of) set the probabilities to a small positive value when there are no data. In particular, we use a “Bayesian shrinkage estimate” of P(X(j) (j)\n= xtest|Y = y˜) where we add some hallucinated examples. There are K\nhallucinated exampl",
  "760": "ucinated examples. There are K\nhallucinated examples spread evenly over the possible values of X(j). K is the\nnumber of distinct values of X(j). The probabilities are pulled toward 1/K. So,\nnow we replace:\n\n1 i (j) (j)\n+ 1\n=\n) [ ˆ x x\n,y =y˜\nP(X(j) (j ]\n= ",
  "761": "\n1 i (j) (j)\n+ 1\n=\n) [ ˆ x x\n,y =y˜\nP(X(j) (j ]\n= xtest|Y = y˜) =\n\nP\n\nP i test i 1 i\n[yi=y˜] + K\n\nˆP(Y = y˜) =\nP 1 i\n[yi=y˜] + 1\nm + K\nˆ\nThis is called Laplace smoothing. The smoothing for P(Y = y˜) is probably unnecessary and has little to no effect.\n\nNa¨",
  "762": "ably unnecessary and has little to no effect.\n\nNa¨ıve Bayes is not necessarily the best algorithm, but is a good first thing to try, and performs surprisingly well given its simplicity!\n\nThere are extensions to continuous data and other variations too.\n\nPP",
  "763": "s to continuous data and other variations too.\n\nPPT Slides\n\n4",
  "764": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "765": "Decision Trees\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nCredit: Russell & Norvig, Mitchell, Kohavi & Quinlan, Carter, Vanden Berghen\n\nWhy trees?\n\n• interpretable/intuitive, popular in medical applications because they mimic\nthe way a doctor thinks\n\n• model ",
  "766": "cause they mimic\nthe way a doctor thinks\n\n• model discrete outcomes nicely\n\n• can be very powerful, can be as complex as you need them\n\n• C4.5 and CART - from “top 10” - decision trees are very popular\n\nSome real examples (from Russell & Norvig, Mitchell)\n",
  "767": "e real examples (from Russell & Norvig, Mitchell)\n\n• BP’s GasOIL system for separating gas and oil on offshore platforms - decision trees replaced a hand-designed rules system with 2500 rules. C4.5-based\nsystem outperformed human experts and saved BP milli",
  "768": "stem outperformed human experts and saved BP millions. (1986)\n\n• learning to fly a Cessna on a flight simulator by watching human experts fly the simulator (1992)\n\n• can also learn to play tennis, analyze C-section risk, etc.\n\nHow to build a decision tree:",
  "769": "-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “splitting” attributes one by one. To determine which attribute to split, look at “node impurity.”\n\n• Assign leaf nodes the majority vote in the leaf.\n\n• When",
  "770": " leaf nodes the majority vote in the leaf.\n\n• When we get to the bottom, prune the tree to prevent overfitting\n\nWhy is this a good way to build a tree?\n\n1",
  "771": "I have to warn you that C4.5 and CART are not elegant by any means that I can define elegant. But the resulting trees can be very elegant. Plus there are 2 of the top 10 algorithms in data mining that are decision tree algorithms! So it’s worth it for us t",
  "772": "ecision tree algorithms! So it’s worth it for us to know what’s under the hood... even though, well... let’s just say it ain’t pretty.\n\nExample: Will the customer wait for a table? (from Russell & Norvig)\n\nHere are the attributes:\n\nList of ten attributes t",
  "773": "Here are the attributes:\n\nList of ten attributes that contribute to whether the customer will wait for a table.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are the examples:\n\nTab le of twe lv e ex am ples , t he att ribu\n\ntes fo r ea ch , an d pred i ct",
  "774": "s , t he att ribu\n\ntes fo r ea ch , an d pred i ctio n of\n\nwhe the r th e cu st om er w i ll wai t.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are two options for the first feature to split at the top of the tree. Which one should we choose? Which one ",
  "775": "f the tree. Which one should we choose? Which one gives me the most information?\n\n2",
  "776": "Two examples of de cision trees. Opti on\n\nfir\n\non B is\n\nA i s sp lit\n\nsplit first by Ty pe.\n\nwh ile\n\nO pti\n\nst by\n\nP atr\n\non s,\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWhat we need is a formula to compute “information.” Before we",
  "777": "d is a formula to compute “information.” Before we do that, here’s another example. Let’s say we pick one of them (Patrons). Maybe then we’ll pick Hungry next, because it has a lot of “information”:\n\nA decision tree sp lit first by Patron s,\n\ngry\n\nthe n by",
  "778": "sion tree sp lit first by Patron s,\n\ngry\n\nthe n by Hun\n\n.\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWe’ll build up to the derivation of C4.5. Origins: Hunt 1962, ID3 of Quinlan 1979 (600 lines of Pascal), C4 (Quinlan 1987). C4.5 is",
  "779": " (600 lines of Pascal), C4 (Quinlan 1987). C4.5 is 9000 lines of C (Quinlan 1993). We start with some basic information theory.\n\n3",
  "780": "Information Theory (from slides of Tom Carter, June 2011)\n\n“Information” from observing the occurrence of an event\n:= #bits needed to encode the probability of the event p = −log2 p.\n\nE.g., a coin flip from a fair coin contains 1 bit of information. If the",
  "781": " a fair coin contains 1 bit of information. If the event has probability 1, we get no information from the occurrence of the event.\n\nWhere did this definition of information come from? Turns out it’s pretty cool. We want to define I so that it obeys all th",
  "782": " cool. We want to define I so that it obeys all these things:\n\n• I(p) ≥0, I(1) = 0; the information of any event is non-negative, no information from events with prob 1\n\n• I(p1 · p2) = I(p1) + I(p2); the information from two independent events\nshould be th",
  "783": "formation from two independent events\nshould be the sum of their informations\n\n• I(p) is continuous, slight changes in probability correspond to slight changes in information\n\nTogether these lead to:\n\nI(p2) = 2I(p) or generally I(pn) = nI(p),\n\nthis means t",
  "784": " = 2I(p) or generally I(pn) = nI(p),\n\nthis means that\n\n1\nI(p) = I\n\n(p1/m)m\n= mI\n\np1/m\nso\nI(p) = I\n\np1/m\nm\n\nand more generally,\n\nI\n\npn/m\nn\n=\nI(p). m\nThis is true for any fraction n/m, which includes rationals, so just define it for\nall positive reals:\nI(pa)",
  "785": "s, so just define it for\nall positive reals:\nI(pa) = aI(p).\n\nThe functions that do this are I(p) = −logb(p) for some b. Choose b = 2 for\n“bits.”\n\n4",
  "786": "Flipping a fair coin gives −log2(1/2) = 1 bit of information if it comes up either\nheads or tails.\n\nA biased coin landing on heads with p = .99 gives −log2(.99) = .0145 bits of\ninformation.\n\nA biased coin landing on heads with p = .01 gives −log2(.01) = 6.",
  "787": "anding on heads with p = .01 gives −log2(.01) = 6.643 bits of\ninformation.\n\nNow, if we had lots of events, what’s the mean information of those events? Assume the events v1, ..., vJ occur with probabilities p1, ..., pJ, where [p1, ..., pJ] is a discrete pr",
  "788": " p1, ..., pJ, where [p1, ..., pJ] is a discrete probability distribution.\n\nX J\nEp [p1,...,pJ]I(p) =\npjI(p ∼\nj) =\n\nj=1\n− X\npj log2 pj =: H(p)\n\nj\n\nwhere p is the vector [p1, ..., pJ]. H(p) is called the entropy of discrete distribution p.\n\nSo if there are on",
  "789": "py of discrete distribution p.\n\nSo if there are only 2 events (binary), with probabilities p = [p, 1 −p],\n\nH(p) = −p log2(p) −(1 −p) log2(1 −p).\n\nIf the probabilities were [1/2, 1/2],\n\n1 1\nH(p) = −2\nlog2\n= 1\n(Yes, we knew that.) 2 2\n\nOr if the probabilitie",
  "790": "1\n(Yes, we knew that.) 2 2\n\nOr if the probabilities were [0.99, 0.01],\n\nH(p) = 0.08 bits.\n\nAs one of the probabilities in the vector p goes to 1, H(p) →0, which is what we want.\n\nBack to C4.5, which uses Information Gain as the splitting criteria.\n\n5",
  "791": "Back to C4.5 (source material: Russell & Norvig, Mitchell, Quinlan) We consider a “test” split on attribute A at a branch.\n\nIn S we have #pos positives and #neg negatives. For each branch j, we have #posj positives and #negj negatives.\n\nThe training probab",
  "792": "ositives and #negj negatives.\n\nThe training probabilities\n\nin branch j are:\n\n#posj #negj ,\n#posj + #negj #posj + #negj\n\n.\n\nThe Information Gain is calculated like this:\n\nGain(S, A) = expected reduction in entropy due to branching on attribute A\n= original\n",
  "793": "ntropy due to branching on attribute A\n= original\n\nentropy −entropy after branching #pos #neg\n= H\n,\n#pos + #neg #pos + #neg\n\nJ\n\n−\nX #posj + #negj\n#posj #negj H , .\n#pos + #neg\n\n#posj + #negj #posj + #negj\nj=1\n\nBack to the example with\n\nthe restauran\n\nts.\n\n",
  "794": "=1\n\nBack to the example with\n\nthe restauran\n\nts.\n\n1 1 2 4 6 2 4\nGain(S, Patrons) = H\n, −\nH([0, 1]) +\nH([1, 0]) +\nH , 2 2 12 12 12\n\n6 6\n\n≈0.541 bits.\n\n2 1 1 2 1 1\nGain(S, Type) = 1 −\nH ,\n+\nH , 12 2 2 12 2 2 4\n\n2 2 4 2 2 ,\n+\nH , 0 bits. 4 4\n\n≈\n\n+\nH 12\n\n12\n\n4",
  "795": "4\n\n2 2 4 2 2 ,\n+\nH , 0 bits. 4 4\n\n≈\n\n+\nH 12\n\n12\n\n4 4\n\n6",
  "796": "Actually Patrons has the highest gain among the attributes, and is chosen to be the root of the tree. In general, we want to choose the feature A that maximizes Gain(S, A).\n\nOne problem with Gain is that it likes to partition too much, and favors numerous ",
  "797": " likes to partition too much, and favors numerous splits: e.g., if each branch contains 1 example:\n\nThen,\n\nH\n\n#posj #negj ,\n= 0 for all j,\n#posj + #negj #posj + #negj\nso all those negative terms would be zero and we’d\n\nchoose that attribute over all the ot",
  "798": "ro and we’d\n\nchoose that attribute over all the others.\n\nAn alternative to Gain is the Gain Ratio. We want to have a large Gain, but also we want small partitions. We’ll choose our attribute according to:\n\nGain(S, A) ←want large SplitInfo(S, A) ←want small",
  "799": "Gain(S, A) ←want large SplitInfo(S, A) ←want small\n\nwhere SplitInfo(S, A) comes from the partition:\n\nJ Sj Sj\nSplitInfo(S, A) =\nX | | − log |S|\n\n| |\n\nj=1\n|S|\n\nwhere |Sj| is the number of examples in branch j. We want each term in the S sum to be large. That",
  "800": ". We want each term in the S sum to be large. That means we want | j| to be large, meaning that we want |S| lots of examples in each branch.\n\nKeep splitting until:\n\n7\n\n[IMAGE_OCR 1] S$ taining data going to this branch\nA=[0,0,0,0,0\nA=[1,1,1,1,1]\nA=[00901]|",
  "801": " this branch\nA=[0,0,0,0,0\nA=[1,1,1,1,1]\nA=[00901]|\n#pos=1 I ] #pos=0\n#neg=0 A=[0,00,1,1] #neg=1\na #pos=1\nmer #neg=0",
  "802": "• no more examples left (no point trying to split)\n\n• all examples have the same class\n\n• no more attributes to split\n\nFor the restaurant example, we get this:\n\nA decisi\n\nee s plit by\n\n, Type,\n\non tr\n\nPa tro\n\nand Fr\n\ni/Sat.\n\nun gry\n\nns, H\n\nArtificial Intel",
  "803": "o\n\nand Fr\n\ni/Sat.\n\nun gry\n\nns, H\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nA wrinkle: actually, it turns out that the class labels for the data were themselves generated from a tree. So to get the label for an example, they fed it ",
  "804": ". So to get the label for an example, they fed it into a tree, and got the label from the leaf. That tree is here:\n\n8",
  "805": "A decisi\n\non tree for t\n\nant exampl e, usin\n\nhe restaur\n\ng most of th e attrib\n\nutes.\n\nA Modern Approach, Prentice Hall, 2009.\n\nBut the one we found is simpler! Does that mean our algorithm isn’t doing a good job?\n\nThere are possibilities to replace H([p, ",
  "806": "od job?\n\nThere are possibilities to replace H([p, 1 −p]), it is not the only thing we can use! One example is the Gini index 2p(1 −p) used by CART. Another example is the value 1 −max(p, 1 −p).1\n\n1If an event has prob p of success, this value is the propor",
  "807": "nt has prob p of success, this value is the proportion of time you guess incorrectly if you classify the\nevent to happen when p > 1/2 (and classify the event not to happen when p ≤1/2).\n\n9",
  "808": "Gin\n\nf p bet\n\nclassifica\n\ny,\n\nrop\n\ntion error for values o\n\nent\n\nof\n\nGra ph\n\ni i nde x, and mi s\n\nween 0 and 1\n\n.\n\nLearning, Springer, 2009.\n\nC4.5 uses information gain for splitting, and CART uses the Gini index. (CART only has binary splits.)\n\nPruning Le",
  "809": " index. (CART only has binary splits.)\n\nPruning Let’s start with C4.5’s pruning. C4.5 recursively makes choices as to whether to prune on an attribute:\n\n• Option 1: leaving the tree as is\n\n• Option 2: replace that part of the tree with a leaf corresponding",
  "810": "ce that part of the tree with a leaf corresponding to the most frequent label in the data S going to that part of the tree.\n\n• Option 3: replace that part of the tree with one of its subtrees, corresponding to the most common branch in the split\n\nDemo To f",
  "811": " to the most common branch in the split\n\nDemo To figure out which decision to make, C4.5 computes upper bounds on the probability of error for each option. I’ll show you how to do that shortly.\n\n• Prob of error for Option 1 ≤UpperBound1\n\n• Prob of error fo",
  "812": "rror for Option 1 ≤UpperBound1\n\n• Prob of error for Option 2 ≤UpperBound2\n\n10",
  "813": "• Prob of error for Option 3 ≤UpperBound3\n\nC4.5 chooses the option that has the lowest of these three upper bounds. This ensures that (w.h.p.) the error rate is fairly low.\n\nE.g., which has the smallest upper bound:\n\n• 1 incorrect out of 3\n\n• 5 incorrect o",
  "814": "er bound:\n\n• 1 incorrect out of 3\n\n• 5 incorrect out of 17, or\n\n• 9 incorrect out of 32?\n\nFor each option, we count the number correct and the number incorrect. We need upper confidence intervals on the proportion that are incorrect. To calculate the upper",
  "815": "portion that are incorrect. To calculate the upper bounds, calculate confidence intervals on proportions.\n\nThe abstract problem is: say you flip a coin N times, with M heads. (Here N is the number of examples in the leaf, M is the number incorrectly classi",
  "816": "es in the leaf, M is the number incorrectly classified.) What is an upper bound for the probability p of heads for the coin?\n\nThink visually about the binomial distribution, where we have N coin flips, and how it changes as p changes:\n\np=0.5\np=0.1\np=0.9\n\nW",
  "817": "how it changes as p changes:\n\np=0.5\np=0.1\np=0.9\n\nWe want the upper bound to be as large as possible (largest possible p, it’s an upper bound), but still there needs to be a probability α to get as few errors as we got. In other words, we want:\n\nPM ( ∼Bin(N",
  "818": "s as we got. In other words, we want:\n\nPM ( ∼Bin(N,preasonable upper bound) M or fewer errors) ≥α\n\n11\n\n[IMAGE_OCR 1] ae O a4 aż m 0.03 0.05 004 002 % 5 10 15 20 25\n\n[IMAGE_OCR 2] 0.35 03 025 02 as a 05 % 5 10 15 20\n\n[IMAGE_OCR 3] 0.35 03 025 02 as a 05 % 5",
  "819": "10 15 20\n\n[IMAGE_OCR 3] 0.35 03 025 02 as a 05 % 5 10 15 20",
  "820": "which means we want to choose our upper bound, call it pα), so that it’s the largest possible value of preasonable upper bound that still satisfies that inequality.\n\nThat is,\n\nPM∼Bin(N,pα)(M or fewer errors) ≈α\n\nX M Bin(z, N, pα)\n\nz=0\n≈α\n\nX M N\npz (1 −p )N",
  "821": ") ≈α\n\nX M Bin(z, N, pα)\n\nz=0\n≈α\n\nX M N\npz (1 −p )N−z ≈α for M > 0\n(for M = 0 it’s (1 −p )N\nα z α α\nz=0\n≈α)\n\nWe can calculate this numerically without a problem. So now if you give me α\nM and N, I can give you pα. C4.5 uses α = .25 by default. M for a given",
  "822": "ou pα. C4.5 uses α = .25 by default. M for a given branch\nis how many misclassified examples are in the branch. N for a given branch is just the number of examples in the branch, |Sj|.\n\nSo we can calculate the upper bound on a branch, but it’s still not cl",
  "823": "the upper bound on a branch, but it’s still not clear how to calculate the upper bound on a tree. Actually, we calculate an upper confidence bound on each branch on the tree and average it over the relative frequencies of landing in each branch of the tree",
  "824": " frequencies of landing in each branch of the tree. It’s best explained by example:\n\nLet’s consider a dataset of 16 examples describing toys (from the Kranf Site). We want to know if the toy is fun or not.\n\n12\n\n[IMAGE_OCR 1]\nl-a\na ‘Fash diiuton this way",
  "825": "Color Max number of players Fun? red 2 yes red 3 yes green 2 yes red 2 yes green 2 yes green 4 yes green 2 yes green 1 yes red 2 yes green 2 yes red 1 yes blue 2 no green 2 yes green 1 yes red 3 yes green 1 yes Think of a split on color.\n\nTo calculate the ",
  "826": " yes Think of a split on color.\n\nTo calculate the upper bound on the tree for Option 1, calculate p.25 for each branch, which are respectively .206, .143, and .75. Then the average is: 1 1\nAve of the upper bounds for tree =\n(6 .206 + 9\n16 ·\n· .143 + 1 · .7",
  "827": "bounds for tree =\n(6 .206 + 9\n16 ·\n· .143 + 1 · .75) = 3.273 × 16\nLet’s compare that to the error rate of Option 2, where we’d replace the tree\nwith a leaf with 6+9+1 = 16 examples in it, where 15 are positive, and 1 is\n1\nnegative. Calculate pα that solves",
  "828": "ive, and 1 is\n1\nnegative. Calculate pα that solves α = P\nz=0 Bin(z, 16, pα), which is .157. The\naverage is: 1 1\nAve of the upper bounds for leaf =\n16 · .157 = 2.512\n16 × . 16\nSay we had to make the decision amongst only Options 1 and 2. Since 2.512 <\n3.273",
  "829": " amongst only Options 1 and 2. Since 2.512 <\n3.273, the upper bound on the error for Option 2 is lower, so we’ll prune the tree\nto a leaf. Look at the data - does it make sense to do this?\n\n13",
  "830": "CART - Classification and Regression Trees (Breiman, Freedman, Olshen, Stone,\n1984)\n\nDoes only binary splits, not multiway splits (less interpretable, but simplifies splitting criteria).\n\nLet’s do classification first, and regression later. For splitting, ",
  "831": "ation first, and regression later. For splitting, CART uses the Gini index. The Gini index again is\n\nvariance of Bin(n, p)\np(1 −p) =\n= variance of Bernoulli(p).\nn For pruning, CART uses “minimal cost complexity.”\n\nEach subtree is assigned a cost. The first",
  "832": "xity.”\n\nEach subtree is assigned a cost. The first term in the cost is a misclassification error. The second term is a regularization term. If we choose C to be large, the tree that minimizes the cost will be sparser. If C is small, the tree that minimizes",
  "833": "be sparser. If C is small, the tree that minimizes the cost will have better training accuracy.\n\nX\n1[yi=leaf’s class] + C [#leaves in subtree] .\n\nX\n\ncost(subtree) =\n\n̸\n\nves j xi∈\n\nlea\n\nleaf j\n\nWe could create a sequence of nested subtrees by gradually incr",
  "834": "te a sequence of nested subtrees by gradually increasing C.\n\nDraw a picture\n\nNow we need to choose C. Here’s one way to do this:\n\n• Step 1: For each C, hold out some data, split, then prune, producing a tree for each C.\n\n• Step 2: see which tree is the bes",
  "835": "e for each C.\n\n• Step 2: see which tree is the best on the holdout data, choose C.\n\n• Step 3: use all data, use chosen C, run split, then prune to produce the final tree.\n\nThere are other ways to do this! You’ll use cross-validation in the homework.\n\nRevie",
  "836": "ou’ll use cross-validation in the homework.\n\nReview C4.5 and CART\n\n14",
  "837": "CART’s Regression Trees\n\nFour-part figure in\n\ncl\n\nud ing partitions, a corresponding tree , and a perspective plot of the predicti on surface .\n\nLearning, Springer, 2009.\n\nCART decides which attributes to split and where to split them. In each leaf, we’re ",
  "838": "plit and where to split them. In each leaf, we’re going to assign f(x) to be a constant.\n\nCan you guess what value to assign?\n\nConsider the empirical error, using the least squares loss:\n\nRtrain(f) =\nX (yi i −f(xi))2\n\n15",
  "839": "Break it up by leaves. Call the value of f(x) in leaf j by fj since it’s a constant.\n\nX (y f(x 2 i i))\n\nX\n\nRtrain(f) =\n\nX ves j i −\n\n∈\n\nlea\n\nleaf j\n\n=\n(y\n2 =:\nRtrain i fj) j (fj).\n\nX\n\nX\n\nleaf j −\n\nleaves j i∈\n\nlea\n\nves j\n\nTo choose the value of the fj’s so",
  "840": "i∈\n\nlea\n\nves j\n\nTo choose the value of the fj’s so that they minimize Rtrain j , take the derivative, set it to 0. Let |Sj| be the number of examples in leaf j.\n\nd\n0 =\n˜ df i∈\n\nX ˜ (yi f)2\n˜f=fj\nleaf j −\n\n!\n\nX\n\nyi i −|Sj| ˜f\n\n!\n\n= −2\nX (yi i −˜f)\n\n= −2\n˜f=",
  "841": "\n\nX\n\nyi i −|Sj| ˜f\n\n!\n\n= −2\nX (yi i −˜f)\n\n= −2\n˜f=fj\n\n˜f=fj\n1\nfj =\nX\nyi = y¯\n\n|Sj| Sj,\n\ni∈leaf j\n\nwhere y¯Sj is the sample average of the labels for leaf j’s examples.\n\nSo now we know what value to assign for f in each leaf. How to split? Greedily want att",
  "842": "or f in each leaf. How to split? Greedily want attribute A and split point s solving the following.\n\n\n\n\n\nmin (yi\nC1)2 + min\n(yi C2)2 . A, s C1 x\n\nX\n\nX\n\nC2 leaf| (A) − −\n\nmin\n\n\n\n( i leaf for each attribute A ∈{\n|x A)>s\ndo }\n\ni∈{ x ≤s} x\n\na\n\nlinesearch o",
  "843": "te A ∈{\n|x A)>s\ndo }\n\ni∈{ x ≤s} x\n\na\n\nlinesearch over s\n\nThe first term means that we’ll choose the optimal C1 = y¯{leaf|x(A)≤s . The second\n}\nterm means we’ll choose C2 = y¯{leaf x(A)>s .\n| }\n\nFor pruning, again CART does minimal cost complexity pruning:\n",
  "844": " again CART does minimal cost complexity pruning:\n\ni∈S\n−y¯Sj)2 + C[# leaves in tree]\n\nX\n\nX yi\n\ncost =\n(\n\nves j x\n\nlea\n\nj\n\n16",
  "845": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "846": "Logistic Regression\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nThanks to Ashia Wilson Credit: J.S. Cramer’s “The Origin of Logistic Regression”\n\nOrigins: 19th Century.\n\n• Studying growth of populations and the course of chemical reactions using\n\nd W(t) = βW(t",
  "847": " course of chemical reactions using\n\nd W(t) = βW(t)\ndt ⇒\nW(t) = Aeβt\n\nwhich is a good model for unopposed growth, like the US population’s growth at the time.\n\n• Adolphe Quetelet (1796 - 1874), Belgian astronomer turned statistician,\nknew it produced impos",
  "848": "onomer turned statistician,\nknew it produced impossible values and asked his pupil Pierre-Fran¸cois Verhulst (1804-1849) to help him work on a more realistic model. They chose\n\nd W(t) = βW(t) −Φ(W(t))\ndt\n\nto resist further growth, and with the choice of Φ ",
  "849": "o resist further growth, and with the choice of Φ to be a quadratic function, they got:\n\nd W(t) = βW(t)(Ω\ndt −W(t)),\n\nW(t)\nwhere Ωis the saturation limit of W. Writing P(t) =\nas the proportion of Ω saturation limit:\nd P(t) = βP(t)(1 −P(t))\ndt\ne(α+βt)\nP(t) ",
  "850": "on limit:\nd P(t) = βP(t)(1 −P(t))\ndt\ne(α+βt)\nP(t) = 1 + e(α+βt)\n\nwhere α is the constant from the integration. Verhulst’s “logistic” function has values between 0 and 1.\n\nDraw the logistic function\n\n1",
  "851": "He published in 3 papers between 1838 and 1847. The first paper demonstrated that the curve agrees very well with the actual course of the population in France, Belgium, Essex, and Russia for periods up to 1833. He did not say how he fitted\nthe curves. In ",
  "852": "1833. He did not say how he fitted\nthe curves. In the second paper he tried to fit the logistic to 3 points using 20-30\nyears of data (which in general is a not a great way to get a good model). His estimates of the limiting population Ωof 6.6 million for ",
  "853": "es of the limiting population Ωof 6.6 million for Belgium and 40 million\nfor France were a little off- these populations are now 11 million for Belgium\nand 65 million for France. In another paper, the estimate was corrected, and they estimated 9.5 million ",
  "854": "ate was corrected, and they estimated 9.5 million for Belgium (pretty close!).\n\nVerhulst died young in poor heath, and his work was forgotten about, but reinvented in 1920 by Raymond Pearl and Lowell Reed who were studying population growth of the US. They",
  "855": "ho were studying population growth of the US. They also tried to fit the logistic function to population growth, and estimated Ωfor the US to be 197 million (the current population is 312 million). Actually, Pearl and collaborators spent 20 years applying ",
  "856": ", Pearl and collaborators spent 20 years applying the logistic growth curve to almost any living population (fruit flies, humans in North Africa, cantaloupes).\n\nVerhulst’s work was rediscovered just after Pearl and Reed’s first paper in 1920, but they didn",
  "857": "earl and Reed’s first paper in 1920, but they didn’t acknowledge him in their second paper, and only in a footnote in a third paper (by Pearl) in 1922. They cited him in 1923, but didn’t use his terminology and called his papers “long since forgotten.” The",
  "858": " and called his papers “long since forgotten.” The name logistic was revived by Yule, in a presidential address to the Royal Statistical Society in 1925.\n\nThere was a lot of debate over whether the logistic function could replace the cdf of the normal dist",
  "859": " function could replace the cdf of the normal distribution. The person who really showed this could happen was\nJoseph Berkson (1899-1982), who was the chief statistician at the Mayo Clinic,\nand who was a collaborator of Reed’s. But because of his personali",
  "860": "llaborator of Reed’s. But because of his personality and attacks on the method of maximum likelihood, controversy ensued... it took until around the 1960’s to resolve it!\n\nLet’s start with a probabilistic model. Assume x ∈Rn, and that x is deterministic (c",
  "861": "odel. Assume x ∈Rn, and that x is deterministic (chosen, not random):\n\nY ∼Bernoulli(P(Y = 1|x)).\n\n2",
  "862": "Here Y takes either 0 or 1, but we need a ±1 for classification. I’ll write ˜0 for\nnow to stand for either 0 or −1. The model is:\n\nP(Y = 1\nln\n|x, λ)\n= λTx\nP(Y = ˜0|x, λ)\n\n|\n\n{z\n\n}\n\n“odds ratio” Why does this model make an\n\nWe want to make a linear combinat",
  "863": "s model make an\n\nWe want to make a linear combination of feature values, like in regular regression, which explains the right hand side. But that can take any real values. We need to turn it into a model for\nP(Y = 1|x, λ), which takes values only between 0",
  "864": "r\nP(Y = 1|x, λ), which takes values only between 0 and 1. The odds ratio turns\nthose probabilities into positive real numbers, then the log turns it into any real numbers.\n\ny sense\n\nat all?\n\nP(Y = 1|x, λ) = eλT x\nP(Y = ˜0|x, λ)\n\nT\n(Y = 1|\nT P\nx, λ) = eλ xP",
  "865": " eλT x\nP(Y = ˜0|x, λ)\n\nT\n(Y = 1|\nT P\nx, λ) = eλ xP(Y = ˜0|x, λ) = eλ x(1 −P(Y = 1|x, λ))\n\nP(Y = 1|\nT\nx, λ)\nh\n1 + eλ xi\n= eλT x\n\neλT x\nP(Y = 1|x, λ) = 1 + eλT\nlogistic x ← function\n\nWe’ll use maximum likelihood estimation. We choose parameters λ to maximize",
  "866": "ood estimation. We choose parameters λ to maximize the likelihood of the data given the model.\n\nm iid\nL(λ) := P(Y1 = y1, . . . , Ym = ym|λ, x1, . . . , xm) =\nY\nP(Yi = yi\ni=1\n|λ, xi).\n\nChoose\nλ∗∈argmax L(λ) = argmax log L(λ)\nλ\nλ\nlog-likelihood\nwhere we take",
  "867": "= argmax log L(λ)\nλ\nλ\nlog-likelihood\nwhere we take the log for convenience since it doesn’t\n\n| {z\n\n}\n\neffect\n\nthe argmax.\n\nBut first, we’ll simplify. We need P(Y = yi|λ, xi) for both yi = 1 and yi = −1.\n \n\nT \n\nλ\nx\nyi = 1,\nneed P(Y = 1|λ, x\n1\ni) := pi =",
  "868": "\n\nT \n\nλ\nx\nyi = 1,\nneed P(Y = 1|λ, x\n1\ni) := pi =\ne i 1 T T T\n1+eλ\nx =\n=\ni\n1+e−λ\nxi\n1+e−y λ\nx i i T T If  \nyi = −\nλ\nx\nλ\nx 1 (i.e., ˜0),\nneed P(Y = ˜0|λ, xi) = 1 −p = 1+e\ni T T\nλ\nx − e i i\nλ\nx =\n1 T\n1+e\ni\n1+e\ni\n1+eλ\nxi\n=\n1\n1+e−\nT\ny λ\nx . i i \n\n3",
  "869": "So, we can just write\n\n1\nP(Y = yi|λ, xi) = 1 + e−y λT\n. i xi\n\nThen,\n\nλ∗∈argmax log L(λ)\nλ\nm\n= argmax\nX 1\nlog 1 + e−yiλT x\nλ\ni\ni=1\nm\n= argmin\nλ\n\nX\nlog(1 + e−yiλT xi).\n\ni=1\n\nThis agrees with the “frequentist” derivation we had before.\n\nThe loss is convex in ",
  "870": " derivation we had before.\n\nThe loss is convex in λ so we can minimize by gradient descent.\n\n4",
  "871": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "872": "This document is created only for automated end-to-end testing of a Moodle-based\nconversational learning system.\n\nThe official project codename is ALFA-OMEGA-123.\n\nThe recommended ingestion batch size for document processing is 17.\n\nThe official semester milestone date is 2031-11-19.\n\nThe system defines three distinct user roles:\n\nAdmin – responsible for system configuration and maintenance.\n\nTeacher – responsible for uploading course materials and managing learning content.\n\nStudent – responsible for inter",
  "873": "sible for uploading course materials and managing learning content.\n\nStudent – responsible for interacting with the chatbot and completing quizzes.\n\nOnly Teachers are allowed to upload course materials.\n\nStudents are allowed to ask questions using the chatbot and participate in quizzes.\n\nThe chatbot is designed to answer questions only based on the materials provided within the course.\n\nIf a question cannot be answered using the available course materials, the chatbot should clearly state that the informati",
  "874": "e answered using the available course materials, the chatbot should clearly state that the information is not available.\n\nThe chatbot must not invent or hallucinate answers that are not grounded in the course documents.\n\nThe following unique marker appears only in this document and nowhere else:\n\nZEBRA-42\n\nThis marker is used exclusively for automated testing and validation purposes.\n\nThe conversational system uses a Retrieval-Augmented Generation (RAG) approach.\n\nRelevant document fragments are retrieved f",
  "875": "m uses a Retrieval-Augmented Generation (RAG) approach.\n\nRelevant document fragments are retrieved first, and then used as context for generating answers.\n\nEach generated answer should reference at least one source document fragment.",
  "1143": "This document is created only for automated end-to-end testing of a Moodle-based\nconversational learning system.\n\nThe official project codename is ALFA-OMEGA-123.\n\nThe recommended ingestion batch size for document processing is 17.\n\nThe official semester milestone date is 2031-11-19.\n\nThe system defines three distinct user roles:\n\nAdmin – responsible for system configuration and maintenance.\n\nTeacher – responsible for uploading course materials and managing learning content.\n\nStudent – responsible for interacting with the chatbot and completing quizzes.\n\nOnly Teachers are allowed to upload course materials.\n\nStudents are allowed to ask questions using the chatbot and participate in quizzes.\n\nThe chatbot is designed to answer questions only based on the materials provided within the course.\n\nIf a question cannot be answered using the available course materials, the chatbot should clearly state that the information is not available.\n\nThe chatbot must not invent or hallucinate answers tha",
  "1144": "\n\nIf a question cannot be answered using the available course materials, the chatbot should clearly state that the information is not available.\n\nThe chatbot must not invent or hallucinate answers that are not grounded in the course documents.\n\nThe following unique marker appears only in this document and nowhere else:\n\nZEBRA-42\n\nThis marker is used exclusively for automated testing and validation purposes.\n\nThe conversational system uses a Retrieval-Augmented Generation (RAG) approach.\n\nRelevant document fragments are retrieved first, and then used as context for generating answers.\n\nEach generated answer should reference at least one source document fragment.",
  "1145": "This document is created only for automated end-to-end testing of a Moodle-based\nconversational learning system.\n\nThe official project codename is ALFA-OMEGA-123.\n\nThe recommended ingestion batch size for document processing is 17.\n\nThe official semester milestone date is 2031-11-19.\n\nThe system defines three distinct user roles:\n\nAdmin – responsible for system configuration and maintenance.\n\nTeacher – responsible for uploading course materials and managing learning content.\n\nStudent – responsible for interacting with the chatbot and completing quizzes.\n\nOnly Teachers are allowed to upload course materials.\n\nStudents are allowed to ask questions using the chatbot and participate in quizzes.\n\nThe chatbot is designed to answer questions only based on the materials provided within the course.\n\nIf a question cannot be answered using the available course materials, the chatbot should clearly state that the information is not available.\n\nThe chatbot must not invent or hallucinate answers that are not grounded in the course documents.\n\nThe following unique marker appears only in this document and nowhere else:\n\nZEBRA-42\n\nThis marker is used exclusively for automated testing and validation purposes.\n\nThe conversational system uses a Retrieval-Augmented Generation (RAG) approach.\n\nRelevant document fragments are retrieved first, and then used as context for generating answers.\n\nEach generated answer should reference at least one source document fragment."
}