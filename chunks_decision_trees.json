{
  "58": "Decision Trees\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nCredit: Russell & Norvig, Mitchell, Kohavi & Quinlan, Carter, Vanden Berghen\n\nWhy trees?\n\n• interpretable/intuitive, popular in medical applications because they mimic\nthe way a doctor thinks\n\n• model discrete outcomes nicely\n\n• can be very powerful, can be as complex as you need them\n\n• C4.5 and CART - from “top 10” - decision trees are very popular\n\nSome real examples (from Russell & Norvig, Mitchell)\n\n• BP’s GasOIL system for separating gas and oil on offshore platforms - decision trees replaced a hand-designed rules system with 2500 rules. C4.5-based\nsystem outperformed human experts and saved BP millions. (1986)\n\n• learning to fly a Cessna on a flight simulator by watching human experts fly the simulator (1992)\n\n• can also learn to play tennis, analyze C-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “splitting” attributes one by one. To determine which attribute to split, look at “node impurity.”\n\n• Assign leaf nodes the majority vote in the leaf.\n\n• When we get to the bottom, prune the tree to prevent overfitting\n\nWhy is this a good way to build a tree?\n\n1",
  "59": "I have to warn you that C4.5 and CART are not elegant by any means that I can define elegant. But the resulting trees can be very elegant. Plus there are 2 of the top 10 algorithms in data mining that are decision tree algorithms! So it’s worth it for us to know what’s under the hood... even though, well... let’s just say it ain’t pretty.\n\nExample: Will the customer wait for a table? (from Russell & Norvig)\n\nHere are the attributes:\n\nList of ten attributes that contribute to whether the customer will wait for a table.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are the examples:\n\nTab le of twe lv e ex am ples , t he att ribu\n\ntes fo r ea ch , an d pred i ctio n of\n\nwhe the r th e cu st om er w i ll wai t.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are two options for the first feature to split at the top of the tree. Which one should we choose? Which one gives me the most information?\n\n2",
  "60": "Two examples of de cision trees. Opti on\n\nfir\n\non B is\n\nA i s sp lit\n\nsplit first by Ty pe.\n\nwh ile\n\nO pti\n\nst by\n\nP atr\n\non s,\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWhat we need is a formula to compute “information.” Before we do that, here’s another example. Let’s say we pick one of them (Patrons). Maybe then we’ll pick Hungry next, because it has a lot of “information”:\n\nA decision tree sp lit first by Patron s,\n\ngry\n\nthe n by Hun\n\n.\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWe’ll build up to the derivation of C4.5. Origins: Hunt 1962, ID3 of Quinlan 1979 (600 lines of Pascal), C4 (Quinlan 1987). C4.5 is 9000 lines of C (Quinlan 1993). We start with some basic information theory.\n\n3",
  "61": "Information Theory (from slides of Tom Carter, June 2011)\n\n“Information” from observing the occurrence of an event\n:= #bits needed to encode the probability of the event p = −log2 p.\n\nE.g., a coin flip from a fair coin contains 1 bit of information. If the event has probability 1, we get no information from the occurrence of the event.\n\nWhere did this definition of information come from? Turns out it’s pretty cool. We want to define I so that it obeys all these things:\n\n• I(p) ≥0, I(1) = 0; the information of any event is non-negative, no information from events with prob 1\n\n• I(p1 · p2) = I(p1) + I(p2); the information from two independent events\nshould be the sum of their informations\n\n• I(p) is continuous, slight changes in probability correspond to slight changes in information\n\nTogether these lead to:\n\nI(p2) = 2I(p) or generally I(pn) = nI(p),\n\nthis means that\n\n1\nI(p) = I\n\n(p1/m)m\n= mI\n\np1/m\nso\nI(p) = I\n\np1/m\nm\n\nand more generally,\n\nI\n\npn/m\nn\n=\nI(p). m\nThis is true for any fraction n/m, which includes rationals, so just define it for\nall positive reals:\nI(pa) = aI(p).\n\nThe functions that do this are I(p) = −logb(p) for some b. Choose b = 2 for\n“bits.”\n\n4",
  "62": "Flipping a fair coin gives −log2(1/2) = 1 bit of information if it comes up either\nheads or tails.\n\nA biased coin landing on heads with p = .99 gives −log2(.99) = .0145 bits of\ninformation.\n\nA biased coin landing on heads with p = .01 gives −log2(.01) = 6.643 bits of\ninformation.\n\nNow, if we had lots of events, what’s the mean information of those events? Assume the events v1, ..., vJ occur with probabilities p1, ..., pJ, where [p1, ..., pJ] is a discrete probability distribution.\n\nX J\nEp [p1,...,pJ]I(p) =\npjI(p ∼\nj) =\n\nj=1\n− X\npj log2 pj =: H(p)\n\nj\n\nwhere p is the vector [p1, ..., pJ]. H(p) is called the entropy of discrete distribution p.\n\nSo if there are only 2 events (binary), with probabilities p = [p, 1 −p],\n\nH(p) = −p log2(p) −(1 −p) log2(1 −p).\n\nIf the probabilities were [1/2, 1/2],\n\n1 1\nH(p) = −2\nlog2\n= 1\n(Yes, we knew that.) 2 2\n\nOr if the probabilities were [0.99, 0.01],\n\nH(p) = 0.08 bits.\n\nAs one of the probabilities in the vector p goes to 1, H(p) →0, which is what we want.\n\nBack to C4.5, which uses Information Gain as the splitting criteria.\n\n5",
  "63": "Back to C4.5 (source material: Russell & Norvig, Mitchell, Quinlan) We consider a “test” split on attribute A at a branch.\n\nIn S we have #pos positives and #neg negatives. For each branch j, we have #posj positives and #negj negatives.\n\nThe training probabilities\n\nin branch j are:\n\n#posj #negj ,\n#posj + #negj #posj + #negj\n\n.\n\nThe Information Gain is calculated like this:\n\nGain(S, A) = expected reduction in entropy due to branching on attribute A\n= original\n\nentropy −entropy after branching #pos #neg\n= H\n,\n#pos + #neg #pos + #neg\n\nJ\n\n−\nX #posj + #negj\n#posj #negj H , .\n#pos + #neg\n\n#posj + #negj #posj + #negj\nj=1\n\nBack to the example with\n\nthe restauran\n\nts.\n\n1 1 2 4 6 2 4\nGain(S, Patrons) = H\n, −\nH([0, 1]) +\nH([1, 0]) +\nH , 2 2 12 12 12\n\n6 6\n\n≈0.541 bits.\n\n2 1 1 2 1 1\nGain(S, Type) = 1 −\nH ,\n+\nH , 12 2 2 12 2 2 4\n\n2 2 4 2 2 ,\n+\nH , 0 bits. 4 4\n\n≈\n\n+\nH 12\n\n12\n\n4 4\n\n6",
  "64": "Actually Patrons has the highest gain among the attributes, and is chosen to be the root of the tree. In general, we want to choose the feature A that maximizes Gain(S, A).\n\nOne problem with Gain is that it likes to partition too much, and favors numerous splits: e.g., if each branch contains 1 example:\n\nThen,\n\nH\n\n#posj #negj ,\n= 0 for all j,\n#posj + #negj #posj + #negj\nso all those negative terms would be zero and we’d\n\nchoose that attribute over all the others.\n\nAn alternative to Gain is the Gain Ratio. We want to have a large Gain, but also we want small partitions. We’ll choose our attribute according to:\n\nGain(S, A) ←want large SplitInfo(S, A) ←want small\n\nwhere SplitInfo(S, A) comes from the partition:\n\nJ Sj Sj\nSplitInfo(S, A) =\nX | | − log |S|\n\n| |\n\nj=1\n|S|\n\nwhere |Sj| is the number of examples in branch j. We want each term in the S sum to be large. That means we want | j| to be large, meaning that we want |S| lots of examples in each branch.\n\nKeep splitting until:\n\n7\n\n[IMAGE_OCR 1] S$ taining data going to this branch\nA=[0,0,0,0,0\nA=[1,1,1,1,1]\nA=[00901]|\n#pos=1 I ] #pos=0\n#neg=0 A=[0,00,1,1] #neg=1\na #pos=1\nmer #neg=0",
  "65": "• no more examples left (no point trying to split)\n\n• all examples have the same class\n\n• no more attributes to split\n\nFor the restaurant example, we get this:\n\nA decisi\n\nee s plit by\n\n, Type,\n\non tr\n\nPa tro\n\nand Fr\n\ni/Sat.\n\nun gry\n\nns, H\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nA wrinkle: actually, it turns out that the class labels for the data were themselves generated from a tree. So to get the label for an example, they fed it into a tree, and got the label from the leaf. That tree is here:\n\n8",
  "66": "A decisi\n\non tree for t\n\nant exampl e, usin\n\nhe restaur\n\ng most of th e attrib\n\nutes.\n\nA Modern Approach, Prentice Hall, 2009.\n\nBut the one we found is simpler! Does that mean our algorithm isn’t doing a good job?\n\nThere are possibilities to replace H([p, 1 −p]), it is not the only thing we can use! One example is the Gini index 2p(1 −p) used by CART. Another example is the value 1 −max(p, 1 −p).1\n\n1If an event has prob p of success, this value is the proportion of time you guess incorrectly if you classify the\nevent to happen when p > 1/2 (and classify the event not to happen when p ≤1/2).\n\n9",
  "67": "Gin\n\nf p bet\n\nclassifica\n\ny,\n\nrop\n\ntion error for values o\n\nent\n\nof\n\nGra ph\n\ni i nde x, and mi s\n\nween 0 and 1\n\n.\n\nLearning, Springer, 2009.\n\nC4.5 uses information gain for splitting, and CART uses the Gini index. (CART only has binary splits.)\n\nPruning Let’s start with C4.5’s pruning. C4.5 recursively makes choices as to whether to prune on an attribute:\n\n• Option 1: leaving the tree as is\n\n• Option 2: replace that part of the tree with a leaf corresponding to the most frequent label in the data S going to that part of the tree.\n\n• Option 3: replace that part of the tree with one of its subtrees, corresponding to the most common branch in the split\n\nDemo To figure out which decision to make, C4.5 computes upper bounds on the probability of error for each option. I’ll show you how to do that shortly.\n\n• Prob of error for Option 1 ≤UpperBound1\n\n• Prob of error for Option 2 ≤UpperBound2\n\n10",
  "68": "• Prob of error for Option 3 ≤UpperBound3\n\nC4.5 chooses the option that has the lowest of these three upper bounds. This ensures that (w.h.p.) the error rate is fairly low.\n\nE.g., which has the smallest upper bound:\n\n• 1 incorrect out of 3\n\n• 5 incorrect out of 17, or\n\n• 9 incorrect out of 32?\n\nFor each option, we count the number correct and the number incorrect. We need upper confidence intervals on the proportion that are incorrect. To calculate the upper bounds, calculate confidence intervals on proportions.\n\nThe abstract problem is: say you flip a coin N times, with M heads. (Here N is the number of examples in the leaf, M is the number incorrectly classified.) What is an upper bound for the probability p of heads for the coin?\n\nThink visually about the binomial distribution, where we have N coin flips, and how it changes as p changes:\n\np=0.5\np=0.1\np=0.9\n\nWe want the upper bound to be as large as possible (largest possible p, it’s an upper bound), but still there needs to be a probability α to get as few errors as we got. In other words, we want:\n\nPM ( ∼Bin(N,preasonable upper bound) M or fewer errors) ≥α\n\n11\n\n[IMAGE_OCR 1] ae O a4 aż m 0.03 0.05 004 002 % 5 10 15 20 25\n\n[IMAGE_OCR 2] 0.35 03 025 02 as a 05 % 5 10 15 20\n\n[IMAGE_OCR 3] 0.35 03 025 02 as a 05 % 5 10 15 20",
  "69": "which means we want to choose our upper bound, call it pα), so that it’s the largest possible value of preasonable upper bound that still satisfies that inequality.\n\nThat is,\n\nPM∼Bin(N,pα)(M or fewer errors) ≈α\n\nX M Bin(z, N, pα)\n\nz=0\n≈α\n\nX M N\npz (1 −p )N−z ≈α for M > 0\n(for M = 0 it’s (1 −p )N\nα z α α\nz=0\n≈α)\n\nWe can calculate this numerically without a problem. So now if you give me α\nM and N, I can give you pα. C4.5 uses α = .25 by default. M for a given branch\nis how many misclassified examples are in the branch. N for a given branch is just the number of examples in the branch, |Sj|.\n\nSo we can calculate the upper bound on a branch, but it’s still not clear how to calculate the upper bound on a tree. Actually, we calculate an upper confidence bound on each branch on the tree and average it over the relative frequencies of landing in each branch of the tree. It’s best explained by example:\n\nLet’s consider a dataset of 16 examples describing toys (from the Kranf Site). We want to know if the toy is fun or not.\n\n12\n\n[IMAGE_OCR 1]\nl-a\na ‘Fash diiuton this way",
  "70": "Color Max number of players Fun? red 2 yes red 3 yes green 2 yes red 2 yes green 2 yes green 4 yes green 2 yes green 1 yes red 2 yes green 2 yes red 1 yes blue 2 no green 2 yes green 1 yes red 3 yes green 1 yes Think of a split on color.\n\nTo calculate the upper bound on the tree for Option 1, calculate p.25 for each branch, which are respectively .206, .143, and .75. Then the average is: 1 1\nAve of the upper bounds for tree =\n(6 .206 + 9\n16 ·\n· .143 + 1 · .75) = 3.273 × 16\nLet’s compare that to the error rate of Option 2, where we’d replace the tree\nwith a leaf with 6+9+1 = 16 examples in it, where 15 are positive, and 1 is\n1\nnegative. Calculate pα that solves α = P\nz=0 Bin(z, 16, pα), which is .157. The\naverage is: 1 1\nAve of the upper bounds for leaf =\n16 · .157 = 2.512\n16 × . 16\nSay we had to make the decision amongst only Options 1 and 2. Since 2.512 <\n3.273, the upper bound on the error for Option 2 is lower, so we’ll prune the tree\nto a leaf. Look at the data - does it make sense to do this?\n\n13",
  "71": "CART - Classification and Regression Trees (Breiman, Freedman, Olshen, Stone,\n1984)\n\nDoes only binary splits, not multiway splits (less interpretable, but simplifies splitting criteria).\n\nLet’s do classification first, and regression later. For splitting, CART uses the Gini index. The Gini index again is\n\nvariance of Bin(n, p)\np(1 −p) =\n= variance of Bernoulli(p).\nn For pruning, CART uses “minimal cost complexity.”\n\nEach subtree is assigned a cost. The first term in the cost is a misclassification error. The second term is a regularization term. If we choose C to be large, the tree that minimizes the cost will be sparser. If C is small, the tree that minimizes the cost will have better training accuracy.\n\nX\n1[yi=leaf’s class] + C [#leaves in subtree] .\n\nX\n\ncost(subtree) =\n\n̸\n\nves j xi∈\n\nlea\n\nleaf j\n\nWe could create a sequence of nested subtrees by gradually increasing C.\n\nDraw a picture\n\nNow we need to choose C. Here’s one way to do this:\n\n• Step 1: For each C, hold out some data, split, then prune, producing a tree for each C.\n\n• Step 2: see which tree is the best on the holdout data, choose C.\n\n• Step 3: use all data, use chosen C, run split, then prune to produce the final tree.\n\nThere are other ways to do this! You’ll use cross-validation in the homework.\n\nReview C4.5 and CART\n\n14",
  "72": "CART’s Regression Trees\n\nFour-part figure in\n\ncl\n\nud ing partitions, a corresponding tree , and a perspective plot of the predicti on surface .\n\nLearning, Springer, 2009.\n\nCART decides which attributes to split and where to split them. In each leaf, we’re going to assign f(x) to be a constant.\n\nCan you guess what value to assign?\n\nConsider the empirical error, using the least squares loss:\n\nRtrain(f) =\nX (yi i −f(xi))2\n\n15",
  "73": "Break it up by leaves. Call the value of f(x) in leaf j by fj since it’s a constant.\n\nX (y f(x 2 i i))\n\nX\n\nRtrain(f) =\n\nX ves j i −\n\n∈\n\nlea\n\nleaf j\n\n=\n(y\n2 =:\nRtrain i fj) j (fj).\n\nX\n\nX\n\nleaf j −\n\nleaves j i∈\n\nlea\n\nves j\n\nTo choose the value of the fj’s so that they minimize Rtrain j , take the derivative, set it to 0. Let |Sj| be the number of examples in leaf j.\n\nd\n0 =\n˜ df i∈\n\nX ˜ (yi f)2\n˜f=fj\nleaf j −\n\n!\n\nX\n\nyi i −|Sj| ˜f\n\n!\n\n= −2\nX (yi i −˜f)\n\n= −2\n˜f=fj\n\n˜f=fj\n1\nfj =\nX\nyi = y¯\n\n|Sj| Sj,\n\ni∈leaf j\n\nwhere y¯Sj is the sample average of the labels for leaf j’s examples.\n\nSo now we know what value to assign for f in each leaf. How to split? Greedily want attribute A and split point s solving the following.\n\n\n\n\n\nmin (yi\nC1)2 + min\n(yi C2)2 . A, s C1 x\n\nX\n\nX\n\nC2 leaf| (A) − −\n\nmin\n\n\n\n( i leaf for each attribute A ∈{\n|x A)>s\ndo }\n\ni∈{ x ≤s} x\n\na\n\nlinesearch over s\n\nThe first term means that we’ll choose the optimal C1 = y¯{leaf|x(A)≤s . The second\n}\nterm means we’ll choose C2 = y¯{leaf x(A)>s .\n| }\n\nFor pruning, again CART does minimal cost complexity pruning:\n\ni∈S\n−y¯Sj)2 + C[# leaves in tree]\n\nX\n\nX yi\n\ncost =\n(\n\nves j x\n\nlea\n\nj\n\n16",
  "74": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "222": "Decision Trees\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nCredit: Russell & Norvig, Mitchell, Kohavi & Quinlan, Carter, Vanden Berghen\n\nWhy trees?\n\n• interpretable/intuitive, popular in medical applications because they mimic\nthe way a doctor thinks\n\n• model discrete outcomes nicely\n\n• can be very powerful, can be as complex as you need them\n\n• C4.5 and CART - from “top 10” - decision trees are very popular\n\nSome real examples (from Russell & Norvig, Mitchell)\n\n• BP’s GasOIL system for separating gas and oil o",
  "223": "e real examples (from Russell & Norvig, Mitchell)\n\n• BP’s GasOIL system for separating gas and oil on offshore platforms - decision trees replaced a hand-designed rules system with 2500 rules. C4.5-based\nsystem outperformed human experts and saved BP millions. (1986)\n\n• learning to fly a Cessna on a flight simulator by watching human experts fly the simulator (1992)\n\n• can also learn to play tennis, analyze C-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “s",
  "224": "-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “splitting” attributes one by one. To determine which attribute to split, look at “node impurity.”\n\n• Assign leaf nodes the majority vote in the leaf.\n\n• When we get to the bottom, prune the tree to prevent overfitting\n\nWhy is this a good way to build a tree?\n\n1",
  "225": "I have to warn you that C4.5 and CART are not elegant by any means that I can define elegant. But the resulting trees can be very elegant. Plus there are 2 of the top 10 algorithms in data mining that are decision tree algorithms! So it’s worth it for us to know what’s under the hood... even though, well... let’s just say it ain’t pretty.\n\nExample: Will the customer wait for a table? (from Russell & Norvig)\n\nHere are the attributes:\n\nList of ten attributes that contribute to whether the customer will wait f",
  "226": "Here are the attributes:\n\nList of ten attributes that contribute to whether the customer will wait for a table.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are the examples:\n\nTab le of twe lv e ex am ples , t he att ribu\n\ntes fo r ea ch , an d pred i ctio n of\n\nwhe the r th e cu st om er w i ll wai t.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are two options for the first feature to split at the top of the tree. Which one should we choose? Which one gives me the most information?\n\n2",
  "227": "Two examples of de cision trees. Opti on\n\nfir\n\non B is\n\nA i s sp lit\n\nsplit first by Ty pe.\n\nwh ile\n\nO pti\n\nst by\n\nP atr\n\non s,\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWhat we need is a formula to compute “information.” Before we do that, here’s another example. Let’s say we pick one of them (Patrons). Maybe then we’ll pick Hungry next, because it has a lot of “information”:\n\nA decision tree sp lit first by Patron s,\n\ngry\n\nthe n by Hun\n\n.",
  "228": "ause it has a lot of “information”:\n\nA decision tree sp lit first by Patron s,\n\ngry\n\nthe n by Hun\n\n.\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWe’ll build up to the derivation of C4.5. Origins: Hunt 1962, ID3 of Quinlan 1979 (600 lines of Pascal), C4 (Quinlan 1987). C4.5 is 9000 lines of C (Quinlan 1993). We start with some basic information theory.\n\n3",
  "229": "Information Theory (from slides of Tom Carter, June 2011)\n\n“Information” from observing the occurrence of an event\n:= #bits needed to encode the probability of the event p = −log2 p.\n\nE.g., a coin flip from a fair coin contains 1 bit of information. If the event has probability 1, we get no information from the occurrence of the event.\n\nWhere did this definition of information come from? Turns out it’s pretty cool. We want to define I so that it obeys all these things:\n\n• I(p) ≥0, I(1) = 0; the information ",
  "230": " cool. We want to define I so that it obeys all these things:\n\n• I(p) ≥0, I(1) = 0; the information of any event is non-negative, no information from events with prob 1\n\n• I(p1 · p2) = I(p1) + I(p2); the information from two independent events\nshould be the sum of their informations\n\n• I(p) is continuous, slight changes in probability correspond to slight changes in information\n\nTogether these lead to:\n\nI(p2) = 2I(p) or generally I(pn) = nI(p),\n\nthis means that\n\n1\nI(p) = I\n\n(p1/m)m\n= mI\n\np1/m\nso\nI(p) = I\n\np",
  "231": " = 2I(p) or generally I(pn) = nI(p),\n\nthis means that\n\n1\nI(p) = I\n\n(p1/m)m\n= mI\n\np1/m\nso\nI(p) = I\n\np1/m\nm\n\nand more generally,\n\nI\n\npn/m\nn\n=\nI(p). m\nThis is true for any fraction n/m, which includes rationals, so just define it for\nall positive reals:\nI(pa) = aI(p).\n\nThe functions that do this are I(p) = −logb(p) for some b. Choose b = 2 for\n“bits.”\n\n4",
  "232": "Flipping a fair coin gives −log2(1/2) = 1 bit of information if it comes up either\nheads or tails.\n\nA biased coin landing on heads with p = .99 gives −log2(.99) = .0145 bits of\ninformation.\n\nA biased coin landing on heads with p = .01 gives −log2(.01) = 6.643 bits of\ninformation.\n\nNow, if we had lots of events, what’s the mean information of those events? Assume the events v1, ..., vJ occur with probabilities p1, ..., pJ, where [p1, .",
  "233": "on of those events? Assume the events v1, ..., vJ occur with probabilities p1, ..., pJ, where [p1, ..., pJ] is a discrete probability distribution.\n\nX J\nEp [p1,...,pJ]I(p) =\npjI(p ∼\nj) =\n\nj=1\n− X\npj log2 pj =: H(p)\n\nj\n\nwhere p is the vector [p1, ..., pJ]. H(p) is called the entropy of discrete distribution p.\n\nSo if there are only 2 events (binary), with probabilities p = [p, 1 −p],\n\nH(p) = −p log2(p) −(1 −p) log2(1 −p).\n\nIf the probabilities were [1/2, 1/2],\n\n1 1\nH(p) = −2\nlog2\n= 1\n(Yes, we knew that.) 2 2",
  "234": " log2(1 −p).\n\nIf the probabilities were [1/2, 1/2],\n\n1 1\nH(p) = −2\nlog2\n= 1\n(Yes, we knew that.) 2 2\n\nOr if the probabilities were [0.99, 0.01],\n\nH(p) = 0.08 bits.\n\nAs one of the probabilities in the vector p goes to 1, H(p) →0, which is what we want.\n\nBack to C4.5, which uses Information Gain as the splitting criteria.\n\n5",
  "235": "Back to C4.5 (source material: Russell & Norvig, Mitchell, Quinlan) We consider a “test” split on attribute A at a branch.\n\nIn S we have #pos positives and #neg negatives. For each branch j, we have #posj positives and #negj negatives.\n\nThe training probabilities\n\nin branch j are:\n\n#posj #negj ,\n#posj + #negj #posj + #negj\n\n.",
  "236": "gatives.\n\nThe training probabilities\n\nin branch j are:\n\n#posj #negj ,\n#posj + #negj #posj + #negj\n\n.\n\nThe Information Gain is calculated like this:\n\nGain(S, A) = expected reduction in entropy due to branching on attribute A\n= original\n\nentropy −entropy after branching #pos #neg\n= H\n,\n#pos + #neg #pos + #neg\n\nJ\n\n−\nX #posj + #negj\n#posj #negj H , .",
  "237": "ropy after branching #pos #neg\n= H\n,\n#pos + #neg #pos + #neg\n\nJ\n\n−\nX #posj + #negj\n#posj #negj H , .\n#pos + #neg\n\n#posj + #negj #posj + #negj\nj=1\n\nBack to the example with\n\nthe restauran\n\nts.\n\n1 1 2 4 6 2 4\nGain(S, Patrons) = H\n, −\nH([0, 1]) +\nH([1, 0]) +\nH , 2 2 12 12 12\n\n6 6\n\n≈0.541 bits.\n\n2 1 1 2 1 1\nGain(S, Type) = 1 −\nH ,\n+\nH , 12 2 2 12 2 2 4\n\n2 2 4 2 2 ,\n+\nH , 0 bits. 4 4\n\n≈\n\n+\nH 12\n\n12\n\n4 4\n\n6",
  "238": "Actually Patrons has the highest gain among the attributes, and is chosen to be the root of the tree. In general, we want to choose the feature A that maximizes Gain(S, A).\n\nOne problem with Gain is that it likes to partition too much, and favors numerous splits: e.g., if each branch contains 1 example:\n\nThen,\n\nH\n\n#posj #negj ,\n= 0 for all j,\n#posj + #negj #posj + #negj\nso all those negative terms would be zero and we’d\n\nchoose that attribute over all the others.\n\nAn alternative to Gain is the Gain Ratio. W",
  "239": "ro and we’d\n\nchoose that attribute over all the others.\n\nAn alternative to Gain is the Gain Ratio. We want to have a large Gain, but also we want small partitions. We’ll choose our attribute according to:\n\nGain(S, A) ←want large SplitInfo(S, A) ←want small\n\nwhere SplitInfo(S, A) comes from the partition:\n\nJ Sj Sj\nSplitInfo(S, A) =\nX | | − log |S|\n\n| |\n\nj=1\n|S|\n\nwhere |Sj| is the number of examples in branch j. We want each term in the S sum to be large. That means we want | j| to be large, meaning that we w",
  "240": ". We want each term in the S sum to be large. That means we want | j| to be large, meaning that we want |S| lots of examples in each branch.\n\nKeep splitting until:\n\n7\n\n[IMAGE_OCR 1] S$ taining data going to this branch\nA=[0,0,0,0,0\nA=[1,1,1,1,1]\nA=[00901]|\n#pos=1 I ] #pos=0\n#neg=0 A=[0,00,1,1] #neg=1\na #pos=1\nmer #neg=0",
  "241": "• no more examples left (no point trying to split)\n\n• all examples have the same class\n\n• no more attributes to split\n\nFor the restaurant example, we get this:\n\nA decisi\n\nee s plit by\n\n, Type,\n\non tr\n\nPa tro\n\nand Fr\n\ni/Sat.\n\nun gry\n\nns, H\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nA wrinkle: actually, it turns out that the class labels for the data were themselves generated from a tree. So to get the label for an example, they fed it into a tree, and got the label from the leaf. That",
  "242": ". So to get the label for an example, they fed it into a tree, and got the label from the leaf. That tree is here:\n\n8",
  "243": "A decisi\n\non tree for t\n\nant exampl e, usin\n\nhe restaur\n\ng most of th e attrib\n\nutes.\n\nA Modern Approach, Prentice Hall, 2009.\n\nBut the one we found is simpler! Does that mean our algorithm isn’t doing a good job?\n\nThere are possibilities to replace H([p, 1 −p]), it is not the only thing we can use! One example is the Gini index 2p(1 −p) used by CART. Another example is the value 1 −max(p, 1 −p).1\n\n1If an event has prob p of success, this value is the proportion of time you guess incorrectly if you classify",
  "244": "nt has prob p of success, this value is the proportion of time you guess incorrectly if you classify the\nevent to happen when p > 1/2 (and classify the event not to happen when p ≤1/2).\n\n9",
  "245": "Gin\n\nf p bet\n\nclassifica\n\ny,\n\nrop\n\ntion error for values o\n\nent\n\nof\n\nGra ph\n\ni i nde x, and mi s\n\nween 0 and 1\n\n.\n\nLearning, Springer, 2009.\n\nC4.5 uses information gain for splitting, and CART uses the Gini index. (CART only has binary splits.)\n\nPruning Let’s start with C4.5’s pruning. C4.5 recursively makes choices as to whether to prune on an attribute:\n\n• Option 1: leaving the tree as is\n\n• Option 2: replace that part of the tree with a leaf corresponding to the most frequent label in the data S going to",
  "246": "ce that part of the tree with a leaf corresponding to the most frequent label in the data S going to that part of the tree.\n\n• Option 3: replace that part of the tree with one of its subtrees, corresponding to the most common branch in the split\n\nDemo To figure out which decision to make, C4.5 computes upper bounds on the probability of error for each option. I’ll show you how to do that shortly.\n\n• Prob of error for Option 1 ≤UpperBound1\n\n• Prob of error for Option 2 ≤UpperBound2\n\n10",
  "247": "• Prob of error for Option 3 ≤UpperBound3\n\nC4.5 chooses the option that has the lowest of these three upper bounds. This ensures that (w.h.p.) the error rate is fairly low.\n\nE.g., which has the smallest upper bound:\n\n• 1 incorrect out of 3\n\n• 5 incorrect out of 17, or\n\n• 9 incorrect out of 32?\n\nFor each option, we count the number correct and the number incorrect. We need upper confidence intervals on the proportion that are incorrect. To calculate the upper bounds, calculate confidence intervals on proport",
  "248": "portion that are incorrect. To calculate the upper bounds, calculate confidence intervals on proportions.\n\nThe abstract problem is: say you flip a coin N times, with M heads. (Here N is the number of examples in the leaf, M is the number incorrectly classified.) What is an upper bound for the probability p of heads for the coin?\n\nThink visually about the binomial distribution, where we have N coin flips, and how it changes as p changes:\n\np=0.5\np=0.1\np=0.9\n\nWe want the upper bound to be as large as possible ",
  "249": "how it changes as p changes:\n\np=0.5\np=0.1\np=0.9\n\nWe want the upper bound to be as large as possible (largest possible p, it’s an upper bound), but still there needs to be a probability α to get as few errors as we got. In other words, we want:\n\nPM ( ∼Bin(N,preasonable upper bound) M or fewer errors) ≥α\n\n11\n\n[IMAGE_OCR 1] ae O a4 aż m 0.03 0.05 004 002 % 5 10 15 20 25\n\n[IMAGE_OCR 2] 0.35 03 025 02 as a 05 % 5 10 15 20\n\n[IMAGE_OCR 3] 0.35 03 025 02 as a 05 % 5 10 15 20",
  "250": "which means we want to choose our upper bound, call it pα), so that it’s the largest possible value of preasonable upper bound that still satisfies that inequality.\n\nThat is,\n\nPM∼Bin(N,pα)(M or fewer errors) ≈α\n\nX M Bin(z, N, pα)\n\nz=0\n≈α\n\nX M N\npz (1 −p )N−z ≈α for M > 0\n(for M = 0 it’s (1 −p )N\nα z α α\nz=0\n≈α)\n\nWe can calculate this numerically without a problem. So now if you give me α\nM and N, I can give you pα. C4.5 uses α = .",
  "251": "s numerically without a problem. So now if you give me α\nM and N, I can give you pα. C4.5 uses α = .25 by default. M for a given branch\nis how many misclassified examples are in the branch. N for a given branch is just the number of examples in the branch, |Sj|.\n\nSo we can calculate the upper bound on a branch, but it’s still not clear how to calculate the upper bound on a tree. Actually, we calculate an upper confidence bound on each branch on the tree and average it over the relative frequencies of landin",
  "252": "r confidence bound on each branch on the tree and average it over the relative frequencies of landing in each branch of the tree. It’s best explained by example:\n\nLet’s consider a dataset of 16 examples describing toys (from the Kranf Site). We want to know if the toy is fun or not.\n\n12\n\n[IMAGE_OCR 1]\nl-a\na ‘Fash diiuton this way",
  "253": "Color Max number of players Fun? red 2 yes red 3 yes green 2 yes red 2 yes green 2 yes green 4 yes green 2 yes green 1 yes red 2 yes green 2 yes red 1 yes blue 2 no green 2 yes green 1 yes red 3 yes green 1 yes Think of a split on color.\n\nTo calculate the upper bound on the tree for Option 1, calculate p.25 for each branch, which are respectively .206, .143, and .75. Then the average is: 1 1\nAve of the upper bounds for tree =\n(6 .206 + 9\n16 ·\n· .143 + 1 · .",
  "254": "and .75. Then the average is: 1 1\nAve of the upper bounds for tree =\n(6 .206 + 9\n16 ·\n· .143 + 1 · .75) = 3.273 × 16\nLet’s compare that to the error rate of Option 2, where we’d replace the tree\nwith a leaf with 6+9+1 = 16 examples in it, where 15 are positive, and 1 is\n1\nnegative. Calculate pα that solves α = P\nz=0 Bin(z, 16, pα), which is .157. The\naverage is: 1 1\nAve of the upper bounds for leaf =\n16 · .157 = 2.512\n16 × .",
  "255": " pα), which is .157. The\naverage is: 1 1\nAve of the upper bounds for leaf =\n16 · .157 = 2.512\n16 × . 16\nSay we had to make the decision amongst only Options 1 and 2. Since 2.512 <\n3.273, the upper bound on the error for Option 2 is lower, so we’ll prune the tree\nto a leaf. Look at the data - does it make sense to do this?\n\n13",
  "256": "CART - Classification and Regression Trees (Breiman, Freedman, Olshen, Stone,\n1984)\n\nDoes only binary splits, not multiway splits (less interpretable, but simplifies splitting criteria).\n\nLet’s do classification first, and regression later. For splitting, CART uses the Gini index. The Gini index again is\n\nvariance of Bin(n, p)\np(1 −p) =\n= variance of Bernoulli(p).\nn For pruning, CART uses “minimal cost complexity.”\n\nEach subtree is assigned a cost. The first term in the cost is a misclassification error. Th",
  "257": "xity.”\n\nEach subtree is assigned a cost. The first term in the cost is a misclassification error. The second term is a regularization term. If we choose C to be large, the tree that minimizes the cost will be sparser. If C is small, the tree that minimizes the cost will have better training accuracy.\n\nX\n1[yi=leaf’s class] + C [#leaves in subtree] .",
  "258": "imizes the cost will have better training accuracy.\n\nX\n1[yi=leaf’s class] + C [#leaves in subtree] .\n\nX\n\ncost(subtree) =\n\n̸\n\nves j xi∈\n\nlea\n\nleaf j\n\nWe could create a sequence of nested subtrees by gradually increasing C.\n\nDraw a picture\n\nNow we need to choose C. Here’s one way to do this:\n\n• Step 1: For each C, hold out some data, split, then prune, producing a tree for each C.\n\n• Step 2: see which tree is the best on the holdout data, choose C.\n\n• Step 3: use all data, use chosen C, run split, then prune ",
  "259": "he best on the holdout data, choose C.\n\n• Step 3: use all data, use chosen C, run split, then prune to produce the final tree.\n\nThere are other ways to do this! You’ll use cross-validation in the homework.\n\nReview C4.5 and CART\n\n14",
  "260": "CART’s Regression Trees\n\nFour-part figure in\n\ncl\n\nud ing partitions, a corresponding tree , and a perspective plot of the predicti on surface .\n\nLearning, Springer, 2009.\n\nCART decides which attributes to split and where to split them. In each leaf, we’re going to assign f(x) to be a constant.\n\nCan you guess what value to assign?\n\nConsider the empirical error, using the least squares loss:\n\nRtrain(f) =\nX (yi i −f(xi))2\n\n15",
  "261": "Break it up by leaves. Call the value of f(x) in leaf j by fj since it’s a constant.\n\nX (y f(x 2 i i))\n\nX\n\nRtrain(f) =\n\nX ves j i −\n\n∈\n\nlea\n\nleaf j\n\n=\n(y\n2 =:\nRtrain i fj) j (fj).\n\nX\n\nX\n\nleaf j −\n\nleaves j i∈\n\nlea\n\nves j\n\nTo choose the value of the fj’s so that they minimize Rtrain j , take the derivative, set it to 0. Let |Sj| be the number of examples in leaf j.\n\nd\n0 =\n˜ df i∈\n\nX ˜ (yi f)2\n˜f=fj\nleaf j −\n\n!\n\nX\n\nyi i −|Sj| ˜f\n\n!",
  "262": "the number of examples in leaf j.\n\nd\n0 =\n˜ df i∈\n\nX ˜ (yi f)2\n˜f=fj\nleaf j −\n\n!\n\nX\n\nyi i −|Sj| ˜f\n\n!\n\n= −2\nX (yi i −˜f)\n\n= −2\n˜f=fj\n\n˜f=fj\n1\nfj =\nX\nyi = y¯\n\n|Sj| Sj,\n\ni∈leaf j\n\nwhere y¯Sj is the sample average of the labels for leaf j’s examples.\n\nSo now we know what value to assign for f in each leaf. How to split? Greedily want attribute A and split point s solving the following.\n\n\n\n\n\nmin (yi\nC1)2 + min\n(yi C2)2 .",
  "263": "dily want attribute A and split point s solving the following.\n\n\n\n\n\nmin (yi\nC1)2 + min\n(yi C2)2 . A, s C1 x\n\nX\n\nX\n\nC2 leaf| (A) − −\n\nmin\n\n\n\n( i leaf for each attribute A ∈{\n|x A)>s\ndo }\n\ni∈{ x ≤s} x\n\na\n\nlinesearch over s\n\nThe first term means that we’ll choose the optimal C1 = y¯{leaf|x(A)≤s . The second\n}\nterm means we’ll choose C2 = y¯{leaf x(A)>s .\n| }\n\nFor pruning, again CART does minimal cost complexity pruning:\n\ni∈S\n−y¯Sj)2 + C[# leaves in tree]\n\nX\n\nX yi\n\ncost =\n(\n\nves j x\n\nlea\n\nj\n\n16",
  "264": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "372": "vJ occur with probabilities p1, ..., pJ, where [p1, ..., pJ] is a discrete probability distribution.\n\nX J\nEp [p1,...,pJ]I(p) =\npjI(p ∼\nj) =\n\nj=1\n− X\npj log2 pj =: H(p)\n\nj\n\nwhere p is the vector [p1, ..., pJ]. H(p) is called the entropy of discrete distribution p.\n\nSo if there are only 2 events (binary), with probabilities p = [p, 1 −p],\n\nH(p) = −p log2(p) −(1 −p) log2(1 −p).\n\nIf the probabilities were [1/2, 1/2],\n\n1 1\nH(p) = −2\nlog2\n= 1\n(Yes, we knew that.) 2 2\n\nOr if the probabilities were [0.99, 0.01],\n\nH(p) = 0.08 bits.\n\nAs one of the probabilities in the vector p goes to 1, H(p) →0, which is what we want.\n\nBack to C4.5, which uses Information Gain as the splitting criteria.\n\n5",
  "365": "Decision Trees\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nCredit: Russell & Norvig, Mitchell, Kohavi & Quinlan, Carter, Vanden Berghen\n\nWhy trees?\n\n• interpretable/intuitive, popular in medical applications because they mimic\nthe way a doctor thinks\n\n• model discrete outcomes nicely\n\n• can be very powerful, can be as complex as you need them\n\n• C4.5 and CART - from “top 10” - decision trees are very popular\n\nSome real examples (from Russell & Norvig, Mitchell)\n\n• BP’s GasOIL system for separating gas and oil on offshore platforms - decision trees replaced a hand-designed rules system with 2500 rules. C4.5-based\nsystem outperformed human experts and saved BP millions. (1986)\n\n• learning to fly a Cessna on a flight simulator by watching human experts fly the simulator (1992)\n\n• can also learn to play tennis, analyze C-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “splitting” attributes one by one. To determine which attribute to split, look",
  "366": "o play tennis, analyze C-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “splitting” attributes one by one. To determine which attribute to split, look at “node impurity.”\n\n• Assign leaf nodes the majority vote in the leaf.\n\n• When we get to the bottom, prune the tree to prevent overfitting\n\nWhy is this a good way to build a tree?\n\n1",
  "367": "I have to warn you that C4.5 and CART are not elegant by any means that I can define elegant. But the resulting trees can be very elegant. Plus there are 2 of the top 10 algorithms in data mining that are decision tree algorithms! So it’s worth it for us to know what’s under the hood... even though, well... let’s just say it ain’t pretty.\n\nExample: Will the customer wait for a table? (from Russell & Norvig)\n\nHere are the attributes:\n\nList of ten attributes that contribute to whether the customer will wait for a table.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are the examples:\n\nTab le of twe lv e ex am ples , t he att ribu\n\ntes fo r ea ch , an d pred i ctio n of\n\nwhe the r th e cu st om er w i ll wai t.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are two options for the first feature to split at the top of the tree. Which one should we choose? Which one gives me the most information?\n\n2",
  "368": "Two examples of de cision trees. Opti on\n\nfir\n\non B is\n\nA i s sp lit\n\nsplit first by Ty pe.\n\nwh ile\n\nO pti\n\nst by\n\nP atr\n\non s,\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWhat we need is a formula to compute “information.” Before we do that, here’s another example. Let’s say we pick one of them (Patrons). Maybe then we’ll pick Hungry next, because it has a lot of “information”:\n\nA decision tree sp lit first by Patron s,\n\ngry\n\nthe n by Hun\n\n.\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWe’ll build up to the derivation of C4.5. Origins: Hunt 1962, ID3 of Quinlan 1979 (600 lines of Pascal), C4 (Quinlan 1987). C4.5 is 9000 lines of C (Quinlan 1993). We start with some basic information theory.\n\n3",
  "369": "Information Theory (from slides of Tom Carter, June 2011)\n\n“Information” from observing the occurrence of an event\n:= #bits needed to encode the probability of the event p = −log2 p.\n\nE.g., a coin flip from a fair coin contains 1 bit of information. If the event has probability 1, we get no information from the occurrence of the event.\n\nWhere did this definition of information come from? Turns out it’s pretty cool. We want to define I so that it obeys all these things:\n\n• I(p) ≥0, I(1) = 0; the information of any event is non-negative, no information from events with prob 1\n\n• I(p1 · p2) = I(p1) + I(p2); the information from two independent events\nshould be the sum of their informations\n\n• I(p) is continuous, slight changes in probability correspond to slight changes in information\n\nTogether these lead to:\n\nI(p2) = 2I(p) or generally I(pn) = nI(p),\n\nthis means that\n\n1\nI(p) = I\n\n(p1/m)m\n= mI\n\np1/m\nso\nI(p) = I\n\np1/m\nm\n\nand more generally,\n\nI\n\npn/m\nn\n=\nI(p). m\nThis is true for any fractio",
  "370": "er these lead to:\n\nI(p2) = 2I(p) or generally I(pn) = nI(p),\n\nthis means that\n\n1\nI(p) = I\n\n(p1/m)m\n= mI\n\np1/m\nso\nI(p) = I\n\np1/m\nm\n\nand more generally,\n\nI\n\npn/m\nn\n=\nI(p). m\nThis is true for any fraction n/m, which includes rationals, so just define it for\nall positive reals:\nI(pa) = aI(p).\n\nThe functions that do this are I(p) = −logb(p) for some b. Choose b = 2 for\n“bits.”\n\n4",
  "371": "Flipping a fair coin gives −log2(1/2) = 1 bit of information if it comes up either\nheads or tails.\n\nA biased coin landing on heads with p = .99 gives −log2(.99) = .0145 bits of\ninformation.\n\nA biased coin landing on heads with p = .01 gives −log2(.01) = 6.643 bits of\ninformation.\n\nNow, if we had lots of events, what’s the mean information of those events? Assume the events v1, ..., vJ occur with probabilities p1, ..., pJ, where [p1, ..., pJ] is a discrete probability distribution.\n\nX J\nEp [p1,...,pJ]I(p) =\npjI(p ∼\nj) =\n\nj=1\n− X\npj log2 pj =: H(p)\n\nj\n\nwhere p is the vector [p1, .",
  "373": "Back to C4.5 (source material: Russell & Norvig, Mitchell, Quinlan) We consider a “test” split on attribute A at a branch.\n\nIn S we have #pos positives and #neg negatives. For each branch j, we have #posj positives and #negj negatives.\n\nThe training probabilities\n\nin branch j are:\n\n#posj #negj ,\n#posj + #negj #posj + #negj\n\n.\n\nThe Information Gain is calculated like this:\n\nGain(S, A) = expected reduction in entropy due to branching on attribute A\n= original\n\nentropy −entropy after branching #pos #neg\n= H\n,\n#pos + #neg #pos + #neg\n\nJ\n\n−\nX #posj + #negj\n#posj #negj H , .\n#pos + #neg\n\n#posj + #negj #posj + #negj\nj=1\n\nBack to the example with\n\nthe restauran\n\nts.\n\n1 1 2 4 6 2 4\nGain(S, Patrons) = H\n, −\nH([0, 1]) +\nH([1, 0]) +\nH , 2 2 12 12 12\n\n6 6\n\n≈0.541 bits.\n\n2 1 1 2 1 1\nGain(S, Type) = 1 −\nH ,\n+\nH , 12 2 2 12 2 2 4\n\n2 2 4 2 2 ,\n+\nH , 0 bits. 4 4\n\n≈\n\n+\nH 12\n\n12\n\n4 4\n\n6",
  "374": "Actually Patrons has the highest gain among the attributes, and is chosen to be the root of the tree. In general, we want to choose the feature A that maximizes Gain(S, A).\n\nOne problem with Gain is that it likes to partition too much, and favors numerous splits: e.g., if each branch contains 1 example:\n\nThen,\n\nH\n\n#posj #negj ,\n= 0 for all j,\n#posj + #negj #posj + #negj\nso all those negative terms would be zero and we’d\n\nchoose that attribute over all the others.\n\nAn alternative to Gain is the Gain Ratio. We want to have a large Gain, but also we want small partitions. We’ll choose our attribute according to:\n\nGain(S, A) ←want large SplitInfo(S, A) ←want small\n\nwhere SplitInfo(S, A) comes from the partition:\n\nJ Sj Sj\nSplitInfo(S, A) =\nX | | − log |S|\n\n| |\n\nj=1\n|S|\n\nwhere |Sj| is the number of examples in branch j. We want each term in the S sum to be large. That means we want | j| to be large, meaning that we want |S| lots of examples in each branch.\n\nKeep splitting until:\n\n7\n\n[IMAGE_O",
  "375": " of examples in branch j. We want each term in the S sum to be large. That means we want | j| to be large, meaning that we want |S| lots of examples in each branch.\n\nKeep splitting until:\n\n7\n\n[IMAGE_OCR 1] S$ taining data going to this branch\nA=[0,0,0,0,0\nA=[1,1,1,1,1]\nA=[00901]|\n#pos=1 I ] #pos=0\n#neg=0 A=[0,00,1,1] #neg=1\na #pos=1\nmer #neg=0",
  "376": "• no more examples left (no point trying to split)\n\n• all examples have the same class\n\n• no more attributes to split\n\nFor the restaurant example, we get this:\n\nA decisi\n\nee s plit by\n\n, Type,\n\non tr\n\nPa tro\n\nand Fr\n\ni/Sat.\n\nun gry\n\nns, H\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nA wrinkle: actually, it turns out that the class labels for the data were themselves generated from a tree. So to get the label for an example, they fed it into a tree, and got the label from the leaf. That tree is here:\n\n8",
  "377": "A decisi\n\non tree for t\n\nant exampl e, usin\n\nhe restaur\n\ng most of th e attrib\n\nutes.\n\nA Modern Approach, Prentice Hall, 2009.\n\nBut the one we found is simpler! Does that mean our algorithm isn’t doing a good job?\n\nThere are possibilities to replace H([p, 1 −p]), it is not the only thing we can use! One example is the Gini index 2p(1 −p) used by CART. Another example is the value 1 −max(p, 1 −p).1\n\n1If an event has prob p of success, this value is the proportion of time you guess incorrectly if you classify the\nevent to happen when p > 1/2 (and classify the event not to happen when p ≤1/2).\n\n9",
  "378": "Gin\n\nf p bet\n\nclassifica\n\ny,\n\nrop\n\ntion error for values o\n\nent\n\nof\n\nGra ph\n\ni i nde x, and mi s\n\nween 0 and 1\n\n.\n\nLearning, Springer, 2009.\n\nC4.5 uses information gain for splitting, and CART uses the Gini index. (CART only has binary splits.)\n\nPruning Let’s start with C4.5’s pruning. C4.5 recursively makes choices as to whether to prune on an attribute:\n\n• Option 1: leaving the tree as is\n\n• Option 2: replace that part of the tree with a leaf corresponding to the most frequent label in the data S going to that part of the tree.\n\n• Option 3: replace that part of the tree with one of its subtrees, corresponding to the most common branch in the split\n\nDemo To figure out which decision to make, C4.5 computes upper bounds on the probability of error for each option. I’ll show you how to do that shortly.\n\n• Prob of error for Option 1 ≤UpperBound1\n\n• Prob of error for Option 2 ≤UpperBound2\n\n10",
  "379": "• Prob of error for Option 3 ≤UpperBound3\n\nC4.5 chooses the option that has the lowest of these three upper bounds. This ensures that (w.h.p.) the error rate is fairly low.\n\nE.g., which has the smallest upper bound:\n\n• 1 incorrect out of 3\n\n• 5 incorrect out of 17, or\n\n• 9 incorrect out of 32?\n\nFor each option, we count the number correct and the number incorrect. We need upper confidence intervals on the proportion that are incorrect. To calculate the upper bounds, calculate confidence intervals on proportions.\n\nThe abstract problem is: say you flip a coin N times, with M heads. (Here N is the number of examples in the leaf, M is the number incorrectly classified.) What is an upper bound for the probability p of heads for the coin?\n\nThink visually about the binomial distribution, where we have N coin flips, and how it changes as p changes:\n\np=0.5\np=0.1\np=0.9\n\nWe want the upper bound to be as large as possible (largest possible p, it’s an upper bound), but still there needs to be a pro",
  "380": " have N coin flips, and how it changes as p changes:\n\np=0.5\np=0.1\np=0.9\n\nWe want the upper bound to be as large as possible (largest possible p, it’s an upper bound), but still there needs to be a probability α to get as few errors as we got. In other words, we want:\n\nPM ( ∼Bin(N,preasonable upper bound) M or fewer errors) ≥α\n\n11\n\n[IMAGE_OCR 1] ae O a4 aż m 0.03 0.05 004 002 % 5 10 15 20 25\n\n[IMAGE_OCR 2] 0.35 03 025 02 as a 05 % 5 10 15 20\n\n[IMAGE_OCR 3] 0.35 03 025 02 as a 05 % 5 10 15 20",
  "381": "which means we want to choose our upper bound, call it pα), so that it’s the largest possible value of preasonable upper bound that still satisfies that inequality.\n\nThat is,\n\nPM∼Bin(N,pα)(M or fewer errors) ≈α\n\nX M Bin(z, N, pα)\n\nz=0\n≈α\n\nX M N\npz (1 −p )N−z ≈α for M > 0\n(for M = 0 it’s (1 −p )N\nα z α α\nz=0\n≈α)\n\nWe can calculate this numerically without a problem. So now if you give me α\nM and N, I can give you pα. C4.5 uses α = .",
  "382": "\n≈α\n\nX M N\npz (1 −p )N−z ≈α for M > 0\n(for M = 0 it’s (1 −p )N\nα z α α\nz=0\n≈α)\n\nWe can calculate this numerically without a problem. So now if you give me α\nM and N, I can give you pα. C4.5 uses α = .25 by default. M for a given branch\nis how many misclassified examples are in the branch. N for a given branch is just the number of examples in the branch, |Sj|.\n\nSo we can calculate the upper bound on a branch, but it’s still not clear how to calculate the upper bound on a tree. Actually, we calculate an upper confidence bound on each branch on the tree and average it over the relative frequencies of landing in each branch of the tree. It’s best explained by example:\n\nLet’s consider a dataset of 16 examples describing toys (from the Kranf Site). We want to know if the toy is fun or not.\n\n12\n\n[IMAGE_OCR 1]\nl-a\na ‘Fash diiuton this way",
  "383": "Color Max number of players Fun? red 2 yes red 3 yes green 2 yes red 2 yes green 2 yes green 4 yes green 2 yes green 1 yes red 2 yes green 2 yes red 1 yes blue 2 no green 2 yes green 1 yes red 3 yes green 1 yes Think of a split on color.\n\nTo calculate the upper bound on the tree for Option 1, calculate p.25 for each branch, which are respectively .206, .143, and .75. Then the average is: 1 1\nAve of the upper bounds for tree =\n(6 .206 + 9\n16 ·\n· .143 + 1 · .75) = 3.273 × 16\nLet’s compare that to the error rate of Option 2, where we’d replace the tree\nwith a leaf with 6+9+1 = 16 examples in it, where 15 are positive, and 1 is\n1\nnegative. Calculate pα that solves α = P\nz=0 Bin(z, 16, pα), which is .157. The\naverage is: 1 1\nAve of the upper bounds for leaf =\n16 · .157 = 2.512\n16 × .",
  "384": "les in it, where 15 are positive, and 1 is\n1\nnegative. Calculate pα that solves α = P\nz=0 Bin(z, 16, pα), which is .157. The\naverage is: 1 1\nAve of the upper bounds for leaf =\n16 · .157 = 2.512\n16 × . 16\nSay we had to make the decision amongst only Options 1 and 2. Since 2.512 <\n3.273, the upper bound on the error for Option 2 is lower, so we’ll prune the tree\nto a leaf. Look at the data - does it make sense to do this?\n\n13",
  "385": "CART - Classification and Regression Trees (Breiman, Freedman, Olshen, Stone,\n1984)\n\nDoes only binary splits, not multiway splits (less interpretable, but simplifies splitting criteria).\n\nLet’s do classification first, and regression later. For splitting, CART uses the Gini index. The Gini index again is\n\nvariance of Bin(n, p)\np(1 −p) =\n= variance of Bernoulli(p).\nn For pruning, CART uses “minimal cost complexity.”\n\nEach subtree is assigned a cost. The first term in the cost is a misclassification error. The second term is a regularization term. If we choose C to be large, the tree that minimizes the cost will be sparser. If C is small, the tree that minimizes the cost will have better training accuracy.\n\nX\n1[yi=leaf’s class] + C [#leaves in subtree] .",
  "386": "se C to be large, the tree that minimizes the cost will be sparser. If C is small, the tree that minimizes the cost will have better training accuracy.\n\nX\n1[yi=leaf’s class] + C [#leaves in subtree] .\n\nX\n\ncost(subtree) =\n\n̸\n\nves j xi∈\n\nlea\n\nleaf j\n\nWe could create a sequence of nested subtrees by gradually increasing C.\n\nDraw a picture\n\nNow we need to choose C. Here’s one way to do this:\n\n• Step 1: For each C, hold out some data, split, then prune, producing a tree for each C.\n\n• Step 2: see which tree is the best on the holdout data, choose C.\n\n• Step 3: use all data, use chosen C, run split, then prune to produce the final tree.\n\nThere are other ways to do this! You’ll use cross-validation in the homework.\n\nReview C4.5 and CART\n\n14",
  "387": "CART’s Regression Trees\n\nFour-part figure in\n\ncl\n\nud ing partitions, a corresponding tree , and a perspective plot of the predicti on surface .\n\nLearning, Springer, 2009.\n\nCART decides which attributes to split and where to split them. In each leaf, we’re going to assign f(x) to be a constant.\n\nCan you guess what value to assign?\n\nConsider the empirical error, using the least squares loss:\n\nRtrain(f) =\nX (yi i −f(xi))2\n\n15",
  "388": "Break it up by leaves. Call the value of f(x) in leaf j by fj since it’s a constant.\n\nX (y f(x 2 i i))\n\nX\n\nRtrain(f) =\n\nX ves j i −\n\n∈\n\nlea\n\nleaf j\n\n=\n(y\n2 =:\nRtrain i fj) j (fj).\n\nX\n\nX\n\nleaf j −\n\nleaves j i∈\n\nlea\n\nves j\n\nTo choose the value of the fj’s so that they minimize Rtrain j , take the derivative, set it to 0. Let |Sj| be the number of examples in leaf j.\n\nd\n0 =\n˜ df i∈\n\nX ˜ (yi f)2\n˜f=fj\nleaf j −\n\n!\n\nX\n\nyi i −|Sj| ˜f\n\n!\n\n= −2\nX (yi i −˜f)\n\n= −2\n˜f=fj\n\n˜f=fj\n1\nfj =\nX\nyi = y¯\n\n|Sj| Sj,\n\ni∈leaf j\n\nwhere y¯Sj is the sample average of the labels for leaf j’s examples.\n\nSo now we know what value to assign for f in each leaf. How to split? Greedily want attribute A and split point s solving the following.\n\n\n\n\n\nmin (yi\nC1)2 + min\n(yi C2)2 . A, s C1 x\n\nX\n\nX\n\nC2 leaf| (A) − −\n\nmin\n\n\n\n( i leaf for each attribute A ∈{\n|x A)>s\ndo }\n\ni∈{ x ≤s} x\n\na\n\nlinesearch over s\n\nThe first term means that we’ll choose the optimal C1 = y¯{leaf|x(A)≤s .",
  "389": " . A, s C1 x\n\nX\n\nX\n\nC2 leaf| (A) − −\n\nmin\n\n\n\n( i leaf for each attribute A ∈{\n|x A)>s\ndo }\n\ni∈{ x ≤s} x\n\na\n\nlinesearch over s\n\nThe first term means that we’ll choose the optimal C1 = y¯{leaf|x(A)≤s . The second\n}\nterm means we’ll choose C2 = y¯{leaf x(A)>s .\n| }\n\nFor pruning, again CART does minimal cost complexity pruning:\n\ni∈S\n−y¯Sj)2 + C[# leaves in tree]\n\nX\n\nX yi\n\ncost =\n(\n\nves j x\n\nlea\n\nj\n\n16",
  "390": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.",
  "765": "Decision Trees\n\nMIT 15.097 Course Notes Cynthia Rudin\n\nCredit: Russell & Norvig, Mitchell, Kohavi & Quinlan, Carter, Vanden Berghen\n\nWhy trees?\n\n• interpretable/intuitive, popular in medical applications because they mimic\nthe way a doctor thinks\n\n• model ",
  "766": "cause they mimic\nthe way a doctor thinks\n\n• model discrete outcomes nicely\n\n• can be very powerful, can be as complex as you need them\n\n• C4.5 and CART - from “top 10” - decision trees are very popular\n\nSome real examples (from Russell & Norvig, Mitchell)\n",
  "767": "e real examples (from Russell & Norvig, Mitchell)\n\n• BP’s GasOIL system for separating gas and oil on offshore platforms - decision trees replaced a hand-designed rules system with 2500 rules. C4.5-based\nsystem outperformed human experts and saved BP milli",
  "768": "stem outperformed human experts and saved BP millions. (1986)\n\n• learning to fly a Cessna on a flight simulator by watching human experts fly the simulator (1992)\n\n• can also learn to play tennis, analyze C-section risk, etc.\n\nHow to build a decision tree:",
  "769": "-section risk, etc.\n\nHow to build a decision tree:\n\n• Start at the top of the tree.\n\n• Grow it by “splitting” attributes one by one. To determine which attribute to split, look at “node impurity.”\n\n• Assign leaf nodes the majority vote in the leaf.\n\n• When",
  "770": " leaf nodes the majority vote in the leaf.\n\n• When we get to the bottom, prune the tree to prevent overfitting\n\nWhy is this a good way to build a tree?\n\n1",
  "771": "I have to warn you that C4.5 and CART are not elegant by any means that I can define elegant. But the resulting trees can be very elegant. Plus there are 2 of the top 10 algorithms in data mining that are decision tree algorithms! So it’s worth it for us t",
  "772": "ecision tree algorithms! So it’s worth it for us to know what’s under the hood... even though, well... let’s just say it ain’t pretty.\n\nExample: Will the customer wait for a table? (from Russell & Norvig)\n\nHere are the attributes:\n\nList of ten attributes t",
  "773": "Here are the attributes:\n\nList of ten attributes that contribute to whether the customer will wait for a table.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are the examples:\n\nTab le of twe lv e ex am ples , t he att ribu\n\ntes fo r ea ch , an d pred i ct",
  "774": "s , t he att ribu\n\ntes fo r ea ch , an d pred i ctio n of\n\nwhe the r th e cu st om er w i ll wai t.\n\nA Modern Approach, Prentice Hall, 2009.\n\nHere are two options for the first feature to split at the top of the tree. Which one should we choose? Which one ",
  "775": "f the tree. Which one should we choose? Which one gives me the most information?\n\n2",
  "776": "Two examples of de cision trees. Opti on\n\nfir\n\non B is\n\nA i s sp lit\n\nsplit first by Ty pe.\n\nwh ile\n\nO pti\n\nst by\n\nP atr\n\non s,\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWhat we need is a formula to compute “information.” Before we",
  "777": "d is a formula to compute “information.” Before we do that, here’s another example. Let’s say we pick one of them (Patrons). Maybe then we’ll pick Hungry next, because it has a lot of “information”:\n\nA decision tree sp lit first by Patron s,\n\ngry\n\nthe n by",
  "778": "sion tree sp lit first by Patron s,\n\ngry\n\nthe n by Hun\n\n.\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nWe’ll build up to the derivation of C4.5. Origins: Hunt 1962, ID3 of Quinlan 1979 (600 lines of Pascal), C4 (Quinlan 1987). C4.5 is",
  "779": " (600 lines of Pascal), C4 (Quinlan 1987). C4.5 is 9000 lines of C (Quinlan 1993). We start with some basic information theory.\n\n3",
  "780": "Information Theory (from slides of Tom Carter, June 2011)\n\n“Information” from observing the occurrence of an event\n:= #bits needed to encode the probability of the event p = −log2 p.\n\nE.g., a coin flip from a fair coin contains 1 bit of information. If the",
  "781": " a fair coin contains 1 bit of information. If the event has probability 1, we get no information from the occurrence of the event.\n\nWhere did this definition of information come from? Turns out it’s pretty cool. We want to define I so that it obeys all th",
  "782": " cool. We want to define I so that it obeys all these things:\n\n• I(p) ≥0, I(1) = 0; the information of any event is non-negative, no information from events with prob 1\n\n• I(p1 · p2) = I(p1) + I(p2); the information from two independent events\nshould be th",
  "783": "formation from two independent events\nshould be the sum of their informations\n\n• I(p) is continuous, slight changes in probability correspond to slight changes in information\n\nTogether these lead to:\n\nI(p2) = 2I(p) or generally I(pn) = nI(p),\n\nthis means t",
  "784": " = 2I(p) or generally I(pn) = nI(p),\n\nthis means that\n\n1\nI(p) = I\n\n(p1/m)m\n= mI\n\np1/m\nso\nI(p) = I\n\np1/m\nm\n\nand more generally,\n\nI\n\npn/m\nn\n=\nI(p). m\nThis is true for any fraction n/m, which includes rationals, so just define it for\nall positive reals:\nI(pa)",
  "785": "s, so just define it for\nall positive reals:\nI(pa) = aI(p).\n\nThe functions that do this are I(p) = −logb(p) for some b. Choose b = 2 for\n“bits.”\n\n4",
  "786": "Flipping a fair coin gives −log2(1/2) = 1 bit of information if it comes up either\nheads or tails.\n\nA biased coin landing on heads with p = .99 gives −log2(.99) = .0145 bits of\ninformation.\n\nA biased coin landing on heads with p = .01 gives −log2(.01) = 6.",
  "787": "anding on heads with p = .01 gives −log2(.01) = 6.643 bits of\ninformation.\n\nNow, if we had lots of events, what’s the mean information of those events? Assume the events v1, ..., vJ occur with probabilities p1, ..., pJ, where [p1, ..., pJ] is a discrete pr",
  "788": " p1, ..., pJ, where [p1, ..., pJ] is a discrete probability distribution.\n\nX J\nEp [p1,...,pJ]I(p) =\npjI(p ∼\nj) =\n\nj=1\n− X\npj log2 pj =: H(p)\n\nj\n\nwhere p is the vector [p1, ..., pJ]. H(p) is called the entropy of discrete distribution p.\n\nSo if there are on",
  "789": "py of discrete distribution p.\n\nSo if there are only 2 events (binary), with probabilities p = [p, 1 −p],\n\nH(p) = −p log2(p) −(1 −p) log2(1 −p).\n\nIf the probabilities were [1/2, 1/2],\n\n1 1\nH(p) = −2\nlog2\n= 1\n(Yes, we knew that.) 2 2\n\nOr if the probabilitie",
  "790": "1\n(Yes, we knew that.) 2 2\n\nOr if the probabilities were [0.99, 0.01],\n\nH(p) = 0.08 bits.\n\nAs one of the probabilities in the vector p goes to 1, H(p) →0, which is what we want.\n\nBack to C4.5, which uses Information Gain as the splitting criteria.\n\n5",
  "791": "Back to C4.5 (source material: Russell & Norvig, Mitchell, Quinlan) We consider a “test” split on attribute A at a branch.\n\nIn S we have #pos positives and #neg negatives. For each branch j, we have #posj positives and #negj negatives.\n\nThe training probab",
  "792": "ositives and #negj negatives.\n\nThe training probabilities\n\nin branch j are:\n\n#posj #negj ,\n#posj + #negj #posj + #negj\n\n.\n\nThe Information Gain is calculated like this:\n\nGain(S, A) = expected reduction in entropy due to branching on attribute A\n= original\n",
  "793": "ntropy due to branching on attribute A\n= original\n\nentropy −entropy after branching #pos #neg\n= H\n,\n#pos + #neg #pos + #neg\n\nJ\n\n−\nX #posj + #negj\n#posj #negj H , .\n#pos + #neg\n\n#posj + #negj #posj + #negj\nj=1\n\nBack to the example with\n\nthe restauran\n\nts.\n\n",
  "794": "=1\n\nBack to the example with\n\nthe restauran\n\nts.\n\n1 1 2 4 6 2 4\nGain(S, Patrons) = H\n, −\nH([0, 1]) +\nH([1, 0]) +\nH , 2 2 12 12 12\n\n6 6\n\n≈0.541 bits.\n\n2 1 1 2 1 1\nGain(S, Type) = 1 −\nH ,\n+\nH , 12 2 2 12 2 2 4\n\n2 2 4 2 2 ,\n+\nH , 0 bits. 4 4\n\n≈\n\n+\nH 12\n\n12\n\n4",
  "795": "4\n\n2 2 4 2 2 ,\n+\nH , 0 bits. 4 4\n\n≈\n\n+\nH 12\n\n12\n\n4 4\n\n6",
  "796": "Actually Patrons has the highest gain among the attributes, and is chosen to be the root of the tree. In general, we want to choose the feature A that maximizes Gain(S, A).\n\nOne problem with Gain is that it likes to partition too much, and favors numerous ",
  "797": " likes to partition too much, and favors numerous splits: e.g., if each branch contains 1 example:\n\nThen,\n\nH\n\n#posj #negj ,\n= 0 for all j,\n#posj + #negj #posj + #negj\nso all those negative terms would be zero and we’d\n\nchoose that attribute over all the ot",
  "798": "ro and we’d\n\nchoose that attribute over all the others.\n\nAn alternative to Gain is the Gain Ratio. We want to have a large Gain, but also we want small partitions. We’ll choose our attribute according to:\n\nGain(S, A) ←want large SplitInfo(S, A) ←want small",
  "799": "Gain(S, A) ←want large SplitInfo(S, A) ←want small\n\nwhere SplitInfo(S, A) comes from the partition:\n\nJ Sj Sj\nSplitInfo(S, A) =\nX | | − log |S|\n\n| |\n\nj=1\n|S|\n\nwhere |Sj| is the number of examples in branch j. We want each term in the S sum to be large. That",
  "800": ". We want each term in the S sum to be large. That means we want | j| to be large, meaning that we want |S| lots of examples in each branch.\n\nKeep splitting until:\n\n7\n\n[IMAGE_OCR 1] S$ taining data going to this branch\nA=[0,0,0,0,0\nA=[1,1,1,1,1]\nA=[00901]|",
  "801": " this branch\nA=[0,0,0,0,0\nA=[1,1,1,1,1]\nA=[00901]|\n#pos=1 I ] #pos=0\n#neg=0 A=[0,00,1,1] #neg=1\na #pos=1\nmer #neg=0",
  "802": "• no more examples left (no point trying to split)\n\n• all examples have the same class\n\n• no more attributes to split\n\nFor the restaurant example, we get this:\n\nA decisi\n\nee s plit by\n\n, Type,\n\non tr\n\nPa tro\n\nand Fr\n\ni/Sat.\n\nun gry\n\nns, H\n\nArtificial Intel",
  "803": "o\n\nand Fr\n\ni/Sat.\n\nun gry\n\nns, H\n\nArtificial Intelligence: A Modern Approach, Prentice Hall, 2009.\n\nA wrinkle: actually, it turns out that the class labels for the data were themselves generated from a tree. So to get the label for an example, they fed it ",
  "804": ". So to get the label for an example, they fed it into a tree, and got the label from the leaf. That tree is here:\n\n8",
  "805": "A decisi\n\non tree for t\n\nant exampl e, usin\n\nhe restaur\n\ng most of th e attrib\n\nutes.\n\nA Modern Approach, Prentice Hall, 2009.\n\nBut the one we found is simpler! Does that mean our algorithm isn’t doing a good job?\n\nThere are possibilities to replace H([p, ",
  "806": "od job?\n\nThere are possibilities to replace H([p, 1 −p]), it is not the only thing we can use! One example is the Gini index 2p(1 −p) used by CART. Another example is the value 1 −max(p, 1 −p).1\n\n1If an event has prob p of success, this value is the propor",
  "807": "nt has prob p of success, this value is the proportion of time you guess incorrectly if you classify the\nevent to happen when p > 1/2 (and classify the event not to happen when p ≤1/2).\n\n9",
  "808": "Gin\n\nf p bet\n\nclassifica\n\ny,\n\nrop\n\ntion error for values o\n\nent\n\nof\n\nGra ph\n\ni i nde x, and mi s\n\nween 0 and 1\n\n.\n\nLearning, Springer, 2009.\n\nC4.5 uses information gain for splitting, and CART uses the Gini index. (CART only has binary splits.)\n\nPruning Le",
  "809": " index. (CART only has binary splits.)\n\nPruning Let’s start with C4.5’s pruning. C4.5 recursively makes choices as to whether to prune on an attribute:\n\n• Option 1: leaving the tree as is\n\n• Option 2: replace that part of the tree with a leaf corresponding",
  "810": "ce that part of the tree with a leaf corresponding to the most frequent label in the data S going to that part of the tree.\n\n• Option 3: replace that part of the tree with one of its subtrees, corresponding to the most common branch in the split\n\nDemo To f",
  "811": " to the most common branch in the split\n\nDemo To figure out which decision to make, C4.5 computes upper bounds on the probability of error for each option. I’ll show you how to do that shortly.\n\n• Prob of error for Option 1 ≤UpperBound1\n\n• Prob of error fo",
  "812": "rror for Option 1 ≤UpperBound1\n\n• Prob of error for Option 2 ≤UpperBound2\n\n10",
  "813": "• Prob of error for Option 3 ≤UpperBound3\n\nC4.5 chooses the option that has the lowest of these three upper bounds. This ensures that (w.h.p.) the error rate is fairly low.\n\nE.g., which has the smallest upper bound:\n\n• 1 incorrect out of 3\n\n• 5 incorrect o",
  "814": "er bound:\n\n• 1 incorrect out of 3\n\n• 5 incorrect out of 17, or\n\n• 9 incorrect out of 32?\n\nFor each option, we count the number correct and the number incorrect. We need upper confidence intervals on the proportion that are incorrect. To calculate the upper",
  "815": "portion that are incorrect. To calculate the upper bounds, calculate confidence intervals on proportions.\n\nThe abstract problem is: say you flip a coin N times, with M heads. (Here N is the number of examples in the leaf, M is the number incorrectly classi",
  "816": "es in the leaf, M is the number incorrectly classified.) What is an upper bound for the probability p of heads for the coin?\n\nThink visually about the binomial distribution, where we have N coin flips, and how it changes as p changes:\n\np=0.5\np=0.1\np=0.9\n\nW",
  "817": "how it changes as p changes:\n\np=0.5\np=0.1\np=0.9\n\nWe want the upper bound to be as large as possible (largest possible p, it’s an upper bound), but still there needs to be a probability α to get as few errors as we got. In other words, we want:\n\nPM ( ∼Bin(N",
  "818": "s as we got. In other words, we want:\n\nPM ( ∼Bin(N,preasonable upper bound) M or fewer errors) ≥α\n\n11\n\n[IMAGE_OCR 1] ae O a4 aż m 0.03 0.05 004 002 % 5 10 15 20 25\n\n[IMAGE_OCR 2] 0.35 03 025 02 as a 05 % 5 10 15 20\n\n[IMAGE_OCR 3] 0.35 03 025 02 as a 05 % 5",
  "819": "10 15 20\n\n[IMAGE_OCR 3] 0.35 03 025 02 as a 05 % 5 10 15 20",
  "820": "which means we want to choose our upper bound, call it pα), so that it’s the largest possible value of preasonable upper bound that still satisfies that inequality.\n\nThat is,\n\nPM∼Bin(N,pα)(M or fewer errors) ≈α\n\nX M Bin(z, N, pα)\n\nz=0\n≈α\n\nX M N\npz (1 −p )N",
  "821": ") ≈α\n\nX M Bin(z, N, pα)\n\nz=0\n≈α\n\nX M N\npz (1 −p )N−z ≈α for M > 0\n(for M = 0 it’s (1 −p )N\nα z α α\nz=0\n≈α)\n\nWe can calculate this numerically without a problem. So now if you give me α\nM and N, I can give you pα. C4.5 uses α = .25 by default. M for a given",
  "822": "ou pα. C4.5 uses α = .25 by default. M for a given branch\nis how many misclassified examples are in the branch. N for a given branch is just the number of examples in the branch, |Sj|.\n\nSo we can calculate the upper bound on a branch, but it’s still not cl",
  "823": "the upper bound on a branch, but it’s still not clear how to calculate the upper bound on a tree. Actually, we calculate an upper confidence bound on each branch on the tree and average it over the relative frequencies of landing in each branch of the tree",
  "824": " frequencies of landing in each branch of the tree. It’s best explained by example:\n\nLet’s consider a dataset of 16 examples describing toys (from the Kranf Site). We want to know if the toy is fun or not.\n\n12\n\n[IMAGE_OCR 1]\nl-a\na ‘Fash diiuton this way",
  "825": "Color Max number of players Fun? red 2 yes red 3 yes green 2 yes red 2 yes green 2 yes green 4 yes green 2 yes green 1 yes red 2 yes green 2 yes red 1 yes blue 2 no green 2 yes green 1 yes red 3 yes green 1 yes Think of a split on color.\n\nTo calculate the ",
  "826": " yes Think of a split on color.\n\nTo calculate the upper bound on the tree for Option 1, calculate p.25 for each branch, which are respectively .206, .143, and .75. Then the average is: 1 1\nAve of the upper bounds for tree =\n(6 .206 + 9\n16 ·\n· .143 + 1 · .7",
  "827": "bounds for tree =\n(6 .206 + 9\n16 ·\n· .143 + 1 · .75) = 3.273 × 16\nLet’s compare that to the error rate of Option 2, where we’d replace the tree\nwith a leaf with 6+9+1 = 16 examples in it, where 15 are positive, and 1 is\n1\nnegative. Calculate pα that solves",
  "828": "ive, and 1 is\n1\nnegative. Calculate pα that solves α = P\nz=0 Bin(z, 16, pα), which is .157. The\naverage is: 1 1\nAve of the upper bounds for leaf =\n16 · .157 = 2.512\n16 × . 16\nSay we had to make the decision amongst only Options 1 and 2. Since 2.512 <\n3.273",
  "829": " amongst only Options 1 and 2. Since 2.512 <\n3.273, the upper bound on the error for Option 2 is lower, so we’ll prune the tree\nto a leaf. Look at the data - does it make sense to do this?\n\n13",
  "830": "CART - Classification and Regression Trees (Breiman, Freedman, Olshen, Stone,\n1984)\n\nDoes only binary splits, not multiway splits (less interpretable, but simplifies splitting criteria).\n\nLet’s do classification first, and regression later. For splitting, ",
  "831": "ation first, and regression later. For splitting, CART uses the Gini index. The Gini index again is\n\nvariance of Bin(n, p)\np(1 −p) =\n= variance of Bernoulli(p).\nn For pruning, CART uses “minimal cost complexity.”\n\nEach subtree is assigned a cost. The first",
  "832": "xity.”\n\nEach subtree is assigned a cost. The first term in the cost is a misclassification error. The second term is a regularization term. If we choose C to be large, the tree that minimizes the cost will be sparser. If C is small, the tree that minimizes",
  "833": "be sparser. If C is small, the tree that minimizes the cost will have better training accuracy.\n\nX\n1[yi=leaf’s class] + C [#leaves in subtree] .\n\nX\n\ncost(subtree) =\n\n̸\n\nves j xi∈\n\nlea\n\nleaf j\n\nWe could create a sequence of nested subtrees by gradually incr",
  "834": "te a sequence of nested subtrees by gradually increasing C.\n\nDraw a picture\n\nNow we need to choose C. Here’s one way to do this:\n\n• Step 1: For each C, hold out some data, split, then prune, producing a tree for each C.\n\n• Step 2: see which tree is the bes",
  "835": "e for each C.\n\n• Step 2: see which tree is the best on the holdout data, choose C.\n\n• Step 3: use all data, use chosen C, run split, then prune to produce the final tree.\n\nThere are other ways to do this! You’ll use cross-validation in the homework.\n\nRevie",
  "836": "ou’ll use cross-validation in the homework.\n\nReview C4.5 and CART\n\n14",
  "837": "CART’s Regression Trees\n\nFour-part figure in\n\ncl\n\nud ing partitions, a corresponding tree , and a perspective plot of the predicti on surface .\n\nLearning, Springer, 2009.\n\nCART decides which attributes to split and where to split them. In each leaf, we’re ",
  "838": "plit and where to split them. In each leaf, we’re going to assign f(x) to be a constant.\n\nCan you guess what value to assign?\n\nConsider the empirical error, using the least squares loss:\n\nRtrain(f) =\nX (yi i −f(xi))2\n\n15",
  "839": "Break it up by leaves. Call the value of f(x) in leaf j by fj since it’s a constant.\n\nX (y f(x 2 i i))\n\nX\n\nRtrain(f) =\n\nX ves j i −\n\n∈\n\nlea\n\nleaf j\n\n=\n(y\n2 =:\nRtrain i fj) j (fj).\n\nX\n\nX\n\nleaf j −\n\nleaves j i∈\n\nlea\n\nves j\n\nTo choose the value of the fj’s so",
  "840": "i∈\n\nlea\n\nves j\n\nTo choose the value of the fj’s so that they minimize Rtrain j , take the derivative, set it to 0. Let |Sj| be the number of examples in leaf j.\n\nd\n0 =\n˜ df i∈\n\nX ˜ (yi f)2\n˜f=fj\nleaf j −\n\n!\n\nX\n\nyi i −|Sj| ˜f\n\n!\n\n= −2\nX (yi i −˜f)\n\n= −2\n˜f=",
  "841": "\n\nX\n\nyi i −|Sj| ˜f\n\n!\n\n= −2\nX (yi i −˜f)\n\n= −2\n˜f=fj\n\n˜f=fj\n1\nfj =\nX\nyi = y¯\n\n|Sj| Sj,\n\ni∈leaf j\n\nwhere y¯Sj is the sample average of the labels for leaf j’s examples.\n\nSo now we know what value to assign for f in each leaf. How to split? Greedily want att",
  "842": "or f in each leaf. How to split? Greedily want attribute A and split point s solving the following.\n\n\n\n\n\nmin (yi\nC1)2 + min\n(yi C2)2 . A, s C1 x\n\nX\n\nX\n\nC2 leaf| (A) − −\n\nmin\n\n\n\n( i leaf for each attribute A ∈{\n|x A)>s\ndo }\n\ni∈{ x ≤s} x\n\na\n\nlinesearch o",
  "843": "te A ∈{\n|x A)>s\ndo }\n\ni∈{ x ≤s} x\n\na\n\nlinesearch over s\n\nThe first term means that we’ll choose the optimal C1 = y¯{leaf|x(A)≤s . The second\n}\nterm means we’ll choose C2 = y¯{leaf x(A)>s .\n| }\n\nFor pruning, again CART does minimal cost complexity pruning:\n",
  "844": " again CART does minimal cost complexity pruning:\n\ni∈S\n−y¯Sj)2 + C[# leaves in tree]\n\nX\n\nX yi\n\ncost =\n(\n\nves j x\n\nlea\n\nj\n\n16",
  "845": "http://ocw.mit.edu\n\n15.097 Prediction: Machine Learning and Statistics\n\nSpring 2012\n\nFor information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms."
}