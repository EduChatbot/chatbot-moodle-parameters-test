[
    {
      "question": "What is the logistic function and how did it originate in the 19th century?",
      "ground_truth": "The logistic function is a mathematical model with values between 0 and 1, defined as P(t) = e^(α+βt) / (1 + e^(α+βt)) [1]. It originated in the 19th century when Adolphe Quetelet, studying population growth and chemical reactions, realized that the existing exponential growth model produced impossible values [2]. He asked his pupil Pierre-François Verhulst to develop a more realistic model; Verhulst introduced a quadratic function to resist growth as it approached a saturation limit [1, 2]. Although Verhulst's work was initially forgotten, the name 'logistic' was revived by Yule in 1925 [3]."
    },
    {
      "question": "In logistic regression, why do we model the log odds ratio as a linear combination of features?",
      "ground_truth": "We model the log odds ratio as a linear combination of features because a standard linear combination (λ^T x) can take any real value [4]. However, probabilities are restricted to values between 0 and 1 [4]. The odds ratio transforms probabilities into positive real numbers, and the subsequent log transformation (the logit function) turns those values into any real numbers [4]. This allows the model to map the unrestricted values of a linear feature combination onto a valid probability scale [4, 5]."
    },
    {
      "question": "How does maximum likelihood estimation work for logistic regression?",
      "ground_truth":"Maximum likelihood estimation (MLE) for logistic regression involves choosing parameters λ that maximize the likelihood of the observed data given the model [5]. Because the data points are assumed to be independent and identically distributed (iid), the likelihood L(λ) is the product of the probabilities for each individual example [5]. For convenience, the log-likelihood is maximized instead, which is equivalent to minimizing the negative log-likelihood [5, 6]. This results in the minimization of the convex 'logistic loss' function, expressed as the sum of log(1 + e^(-y_i * λ^T * x_i)), which can be solved using gradient descent [6, 7]."
    },
    {
      "question": "What are the key advantages of decision trees mentioned in the course notes?",
      "ground_truth": "According to the course notes, decision trees offer several key advantages: 1) They are interpretable and intuitive, often used in medical applications because they mimic human diagnostic thinking; 2) They model discrete outcomes effectively; 3) They are powerful and flexible, capable of being as complex as a problem requires; and 4) They are highly popular, with algorithms like C4.5 and CART being ranked among the 'top 10' in data mining [1]."
    },
    {
      "question": "Explain how Information Gain is calculated for decision tree splitting and what the downward closure property means.",
      "ground_truth": "Information Gain is the expected reduction in entropy caused by partitioning a dataset S according to an attribute A. It is calculated by taking the original entropy of the set and subtracting the sum of the entropies of the resulting branches, weighted by the proportion of examples that fall into each branch [2]. The downward closure property (primarily discussed in the context of the Apriori algorithm) states that if an itemset is frequent (meeting a support threshold θ), then all of its subsets must also be frequent because the support of a subset is always greater than or equal to the support of the larger set [3, 4]."
    },
    {
      "question": "What is the Gain Ratio and why is it used instead of Information Gain alone?",
      "ground_truth":"The Gain Ratio is an alternative to Information Gain used to prevent the algorithm from favoring attributes with a large number of distinct values [5]. Standard Information Gain tends to maximize when a split creates many small partitions (even down to one example per branch), which results in zero entropy but poor generalization [6]. The Gain Ratio adjusts for this by incorporating 'SplitInfo,' which penalizes splits that partition the data into many small pieces, thereby favoring attributes that produce larger, more meaningful branches [5, 7]."
    },
    {
      "question": "How does C4.5 perform pruning and what are the three options considered at each node?",
      "ground_truth": "C4.5 performs pruning recursively by calculating upper bounds on the probability of error for three different scenarios and choosing the option with the lowest upper bound [8-10]. The three options considered at each node are: 1) Leaving the tree as is; 2) Replacing the sub-tree with a single leaf node representing the most frequent label in that branch; and 3) Replacing the sub-tree with its most common branch (one of its sub-trees) [8]."
    },
    {
      "question": "Compare the splitting criteria used by C4.5 and CART decision tree algorithms.",
      "ground_truth": "C4.5 and CART utilize different mathematical measures for node impurity. C4.5 primarily uses Information Gain, which measures the expected reduction in entropy after a split [1, 2]. It also employs the Gain Ratio to penalize attributes that create too many partitions [3]. In contrast, CART uses the Gini Index for classification, which is defined as 2p(1-p) for binary cases [4, 5]. Additionally, CART is limited to binary splits, whereas C4.5 can perform multiway splits [5]."
    },
    {
      "question": "What is the key assumption made by Naïve Bayes and why does it help with high-dimensional problems?",
      "ground_truth": "The key assumption of Naïve Bayes is that all features (dimensions) are conditionally independent given the class label [6, 7]. This means that knowing the value of one feature does not affect the belief about another feature if the class is already known [6]. This assumption is particularly helpful in high-dimensional problems because it mitigates the 'curse of dimensionality,' allowing the model to estimate probabilities in a 50,000-dimensional space (like a spam filter) without requiring an astronomical amount of training data [6, 8]."
    },
    {
      "question": "Explain why Laplace smoothing is used in Naïve Bayes and how it works.",
      "ground_truth": "Laplace smoothing is used to prevent the 'zero-frequency problem' in high-dimensional data, where a specific feature value might not appear with a certain class in the training set, leading to an empirical probability of zero [9]. Since Naïve Bayes multiplies probabilities together, a single zero would zero out the entire calculation [9]. It works by adding 'hallucinated examples'—specifically adding 1 to the numerator of the count and K (the number of distinct values for that feature) to the denominator—pulling the probability estimates toward 1/K and ensuring they remain positive [10]."
    },
    {
      "question": "What is the difference between generative and discriminative models in classification?",
      "ground_truth": "Generative models, such as Naïve Bayes, attempt to model the underlying distribution of the data by estimating the joint probability P(X,Y); specifically, they estimate P(X|Y) and P(Y) and then use Bayes' rule to find the posterior P(Y|X) [11]. Discriminative models, such as C4.5, CART, SVM, and Logistic Regression, skip modeling how the data is generated and instead directly estimate the conditional probability P(Y|X) or determine the decision boundary between classes [8]."
    },
    {
      "question": "How does K-Nearest Neighbors classification work and what are its main advantages and disadvantages?",
      "ground_truth":"K-Nearest Neighbors (K-NN) classifies a test example $x$ by taking a majority vote of its $k$ closest training points [1]. It does not explicitly compute decision boundaries; instead, the boundaries form a subset of the Voronoi diagram of the training data [2]. For regression, the output is the (weighted) average of the values of the $K$ neighbors [2]. **Advantages** include its simplicity, power, and 'lazy' nature, meaning no explicit training is required and new examples can be added easily [3, 4]. **Disadvantages** include being computationally expensive and slow ($O(md)$), as it must compute distances to all $m$ training examples for every new point [4]. It is also sensitive to attribute scaling and the choice of $k$; a $k$ that is too small models noise, while a $k$ that is too large includes neighbors from other classes [3]."
    },
    {
      "question": "Describe the K-Means clustering algorithm and explain why the cost monotonically decreases during iterations.",
      "ground_truth": "The K-Means algorithm aims to minimize the cost defined as $\\sum_{i} \\min_{k} \\|x_i - z_k\\|^2$ by iteratively performing two steps: 1) assigning each data point $x_i$ to its closest cluster center $z_k$ and 2) re-calculating each center $z_k$ as the mean of its assigned points [5, 6]. The cost monotonically decreases because each step is guaranteed not to increase it [6]. In the first step, assigning points to their nearest representative ensures the cluster assignment is better or equal: $\\text{cost}(C^{(t+1)}, z^{(t)}) \\leq \\text{cost}(C^{(t)}, z^{(t)})$ [6, 7]. In the second step, re-centering at the mean minimizes the squared distance within each cluster, as proven by Lemma 1, ensuring $\\text{cost}(C^{(t+1)}, z^{(t+1)}) \\leq \\text{cost}(C^{(t+1)}, z^{(t)})$ [7]."
    },
    {
      "question": "What is the bias-variance tradeoff and how does it relate to model complexity?",
      "ground_truth":"The bias-variance tradeoff is the necessity to balance two sources of error to minimize the Mean Squared Error (MSE): the variance term, which controls 'wiggliness' and sensitivity to training data, and the bias term, which measures how far the average model prediction is from the true conditional mean [8, 9]. It relates directly to model complexity: simpler models (e.g., low-degree polynomials) typically have high bias but low variance, while more complex models (e.g., high-degree polynomials) have low bias but high variance [8, 9]. Overfitting occurs when a model is too complex and fits the data perfectly, resulting in high variance and poor generalization [9, 10]."
    },
    {
      "question": "Compare the loss functions used in different machine learning algorithms as shown in the regularized learning expression.",
      "ground_truth": "Different machine learning algorithms can be categorized by the specific loss function $R(f(x_i), y_i)$ they minimize within the omnipresent regularized learning expression $\\sum_i R(f(x_i), y_i) + CR_{reg}(f)$ [11]. **Logistic Regression** uses 'logistic loss' defined as $\\log_2(1 + e^{-y_if(x_i)})$ [12]. **Support Vector Machines (SVM)** utilize 'hinge loss' defined as $\\max(0, 1 - y_if(x_i))$ [12]. **AdaBoost** employs 'exponential loss' defined as $e^{-y_if(x_i)}$ [12]. Other standard losses include 'least squares loss' $(f(x_i) - y_i)^2$ and 'misclassification error' $1[y_if(x_i) \\le 0]$, though the latter is computationally difficult to minimize directly [12]."
    },
    {
      "question": "What is the Apriori algorithm and how does it use the downward closure property to efficiently find frequent itemsets?",
      "ground_truth": "The **Apriori algorithm** is a breadth-first search method used to find all frequent itemsets—subsets of items that appear in a database with a frequency greater than or equal to a minimum support threshold $\\theta$ [1-3]. It relies on the **downward closure property**, which states that if an itemset is frequent, then all of its subsets must also be frequent [1]. The algorithm uses this property for efficiency by generating candidate $k$-itemsets ($C_k$) only from known frequent $(k-1)$-itemsets ($L_{k-1}$) [2, 3]. During the **prune step**, any candidate itemset is discarded if any of its $(k-1)$-subsets are not in $L_{k-1}$, significantly reducing the number of itemsets that need to be counted in the database [4]."
    },
    {
      "question": "Explain how CART performs regression tree splitting and what value is assigned to each leaf.",
      "ground_truth": "In **CART (Classification and Regression Trees)**, regression is performed using recursive binary splitting to partition the feature space into regions [5, 6]. To determine a split, the algorithm greedily searches for the attribute $A$ and split point $s$ that minimize the **least squares loss** [6]. The value assigned to each leaf $j$ is the **sample average** of the labels of the examples falling into that leaf: $f_j = \\frac{1}{|S_j|} \\sum_{i \\in leaf_j} y_i$ [6, 7]. This constant value is chosen because the derivative of the empirical error with respect to the leaf value is zero at the mean, thus minimizing the squared error for that partition [6, 7]."
    },
    {
      "question": "What is overfitting and how does learning theory address it through the concepts of training error and test error?",
      "ground_truth": "**Overfitting** occurs when a model is too complex and begins to model noise or 'memorize' specific training examples rather than learning the underlying pattern, resulting in a failure to generalize to new data [8, 9]. **Learning theory** addresses this by analyzing the relationship between **training error** ($R_{train}$), which is the error measured on the input data, and **test error** ($R_{test}$), which is the expected error on new data from the same distribution [8, 10, 11]. Theory suggests that $R_{test}$ is likely to be small if $R_{train}$ is small, the number of training examples $m$ is large, and the model is 'simple' [8]. **Structural Risk Minimization (SRM)** specifically quantifies 'simple models' and uses regularization to strike a balance between bias and variance to prevent overfitting [12-14]."
    },
    {
      "question": "How does information theory define 'information' and what is entropy?",
      "ground_truth": "**Information** is defined as the number of bits required to encode the probability $p$ of an event's occurrence, calculated as $I(p) = -\\log_2 p$ [15, 16]. This definition ensures that information is non-negative and that the information from independent events is additive [17]. **Entropy ($H$)** is the **mean information** of a discrete probability distribution $p = [p_1, ..., p_J]$ [18]. It is calculated using the formula $H(p) = -\\sum_{j=1}^J p_j \\log_2 p_j$ [18]. Entropy serves as a measure of uncertainty; if the probability of one event approaches 1, the entropy approaches 0, indicating no uncertainty and no information gain from the outcome [19]."
    },
    {
      "question": "Describe the fundamental difference between supervised and unsupervised learning, providing examples from the course materials.",
      "ground_truth":"The fundamental difference lies in the presence of a **ground truth**. **Supervised learning** utilizes 'instances with labels' $(x_i, y_i)$ where the goal is to learn a function that predicts the label $y$ for a new input $x$ [10, 20, 21]. Examples include **classification** (e.g., handwriting recognition or spam filtering) and **regression** (e.g., predicting stock prices or income) [20, 22]. In contrast, **unsupervised learning** deals with data without ground truth labels to find inherent patterns or structures [21, 23]. Examples include **clustering** (e.g., K-means for segmenting consumers or gene families) and **rule mining** (e.g., using Apriori to find market basket correlations like 'Diapers $\\rightarrow$ Beer') [21, 23]."
    },
    {
      "question": "What does the conditional expectation f*(x) = E[y|x] minimize and why is this important in regression?",
      "ground_truth": "The conditional expectation $f^*(x) = E[y|x]$ minimizes the expected squared error, or least squares loss, defined as $E_{x,y\\sim D}[(y - f(x))^2]$ [1, 2]. This is fundamentally important in regression because $f^*(x)$ represents the 'best possible' predictor for a given distribution [2, 3]. It also serves as the center point for the bias-variance decomposition, where the bias term is defined as the difference between a model's average prediction and this true conditional mean $f^*(x)$ (also denoted as $\\bar{y}$) [4, 5]."
      },
    {
      "question": "Compare how C4.5 and CART handle pruning decisions differently.",
      "ground_truth": "C4.5 and CART use distinct methodologies for pruning to prevent overfitting. C4.5 employs a recursive strategy that compares three options at each node: keeping the subtree, replacing it with a leaf node (using the majority class), or replacing it with its most frequent branch [6]. It makes this choice by calculating a 'reasonable' upper bound on the probability of error for each option using a binomial distribution confidence interval (typically with $\\alpha = 0.25$) and selecting the option with the lowest bound [7, 8]. In contrast, CART uses 'minimal cost complexity' pruning, which assigns a cost to a subtree based on its misclassification error plus a regularization penalty: $\\text{cost(subtree)} = \\sum 1[y_i \\neq \\text{class}] + C \\times [\\text{# leaves}]$ [9]. CART generates a sequence of nested subtrees by varying the regularization parameter $C$ and typically uses cross-validation to select the optimal tree [9, 10]."
    },
    {
      "question": "How would you implement and evaluate a machine learning model using the R programming language based on the course materials?",
      "ground_truth": "To implement a model in R, you first load the necessary library (e.g., `e1071` for SVM or Naive Bayes, `rpart` for CART) and use the algorithm's specific function, such as `glm()` for logistic regression or `rpart()` for decision trees, typically using a formula like `y ~ x1 + x2` [11-13]. Data is generally handled as a data frame [14, 15]. To evaluate the model, you use the `predict()` function on a separate test dataset to generate estimated values [15, 16]. These predictions are then compared to the actual labels (the ground truth) using a loss function, such as misclassification error for classification or least squares for regression, to calculate the test error ($R_{test}$) [17, 18]."
    },
    {
      "question": "Explain how Lemma 1 in the clustering notes helps justify why K-Means assigns cluster centers to the mean of their assigned points.",
      "ground_truth": "Lemma 1 provides the mathematical justification for the re-centering step in K-Means by stating that for any set $C$ and any representative $z$, the cost is: $\\text{cost}(C, z) = \\text{cost}(C, \\text{mean}(C)) + |C| \\cdot \\|z - \\text{mean}(C)\\|_2^2$ [19, 20]. Because the second term $|C| \\cdot \\|z - \\text{mean}(C)\\|_2^2$ is a squared norm multiplied by the number of points, it is always non-negative and only reaches its minimum value of zero when $z = \\text{mean}(C)$ [19, 20]. This proves that the mean is the unique point that minimizes the sum of squared distances for a cluster, ensuring that the total cost of the K-Means algorithm monotonically decreases during the re-centering step [21, 22]."
    },
    {
      "question": "How do the origins of logistic regression relate to population modeling, and how was this later applied to classification problems?",
      "ground_truth": "Logistic regression originated in the 19th century through the work of Adolphe Quetelet and Pierre-François Verhulst, who sought to improve upon exponential growth models for populations and chemical reactions [23]. They introduced the 'logistic' function to model growth that resists further increase as it approaches a saturation limit $\\Omega$, resulting in a proportion $P(t) = \\frac{e^{\\alpha+\\beta t}}{1+e^{\\alpha+\\beta t}}$ that stays between 0 and 1 [24]. This concept was later applied to classification by modeling the 'log odds ratio' of a class label as a linear combination of features: $\\ln\\left(\\frac{P(Y=1|x,\\lambda)}{P(Y=0|x,\\lambda)}\\right) = \\lambda^T x$ [25]. This allows the model to take any real-valued linear combination of inputs and map it to a valid probability between 0 and 1, which can then be optimized using maximum likelihood estimation [25, 26]."
    },
    {
      "question": "What are the different node impurity measures mentioned for decision trees and how do they compare?",
      "ground_truth": "The sources mention three primary node impurity measures for classification: entropy, the Gini index, and misclassification error [1]. Entropy is defined as $H(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$ and is used by the C4.5 algorithm to calculate Information Gain [2, 3]. The Gini index is defined as $2p(1-p)$ and is the standard measure used by the CART algorithm [1, 4]. Misclassification error is defined as $1 - \\max(p, 1-p)$, representing the proportion of time one would guess incorrectly if classifying based on the majority [1]. Visually, all three measures are concave functions that peak at $p=0.5$ (maximum impurity) and reach 0 at $p=0$ and $p=1$ (perfectly pure nodes) [5]."
    },
    {
      "question": "How does the choice of K affect K-Nearest Neighbors performance and how would you select an appropriate value?",
      "ground_truth": "The choice of $K$ significantly impacts the model's ability to generalize. If $K$ is too small, the algorithm becomes overly sensitive to noise in the training data, leading to a model that fits the noise rather than the underlying pattern [6]. Conversely, if $K$ is too large, the neighborhood becomes too broad and includes points from other classes, which blurs the decision boundaries [6]. According to the sources, an appropriate value for $K$ can be determined by evaluating a range of different $K$ values to find the one that performs best [7]."
    },
    {
      "question": "Describe the complete workflow of the Apriori algorithm from market basket data to actionable association rules.",
      "ground_truth": "The workflow begins with market basket data to find frequent itemsets where the support is $\\ge \\theta$ [8, 9]. First, the algorithm identifies frequent 1-itemsets ($L_1$) [10]. It then iteratively generates candidate $k$-itemsets ($C_k$) from frequent $(k-1)$-itemsets by joining itemsets that share the first $k-2$ items [11]. Using the downward closure property, candidates are pruned if any of their $(k-1)$-subsets are not frequent [8, 11]. The database is scanned to verify the support of the remaining candidates to form $L_k$ [10]. Finally, actionable association rules $a \rightarrow b$ are generated from these frequent itemsets by calculating confidence, $P(b|a)$, and keeping rules that meet a minimum confidence threshold [8, 9]."
    },
    {
      "question": "Explain the relationship between structural risk minimization, regularization, and the bias-variance tradeoff across the course materials.",
      "ground_truth": "Structural Risk Minimization (SRM) provides a framework for generalization by balancing training error with model complexity to avoid overfitting [12, 13]. This is practically implemented through the 'Regularized Learning Expression', $\\sum_i R(f(x_i), y_i) + C R_{reg}(f)$, where the second term penalizes model complexity [14]. This framework directly manages the bias-variance tradeoff: simpler models (high $C$ or fewer parameters) have high bias but low variance, while complex models (low $C$) have low bias but high variance [15, 16]. SRM and regularization aim to inject enough 'structure' (bias) into the model to reduce 'wiggliness' (variance), thereby minimizing the overall Mean Squared Error [15, 16]."
    },
    {
      "question": "How does hierarchical clustering address the limitations of K-Means clustering?",
      "ground_truth": "A major limitation of K-Means clustering is that cluster membership can change arbitrarily and inconsistently as the value of $K$ changes [17]. Hierarchical clustering addresses this by creating a nested hierarchy of clusters [17]. In this approach, clusters at one level are created by merging clusters from the level below, starting from the lowest level where each example is its own cluster up to the highest level where all data is in a single cluster [17, 18]. This structure ensures that the relationships between clusters remain stable across different levels of the hierarchy [17]."
    }]